{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f418a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/opt/slh/icecube/')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "#os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6efa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "from icecube.fastai_fix import *\n",
    "from tqdm.notebook import tqdm\n",
    "from icecube.data_train_v3 import RandomChunkSampler,LenMatchBatchSampler,IceCubeCache, DeviceDataLoader\n",
    "from icecube.loss import loss, loss_vms\n",
    "from icecube.models import EncoderWithDirectionReconstructionV22\n",
    "from fastxtend.vision.all import EMACallback\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b613edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTION = 'total'\n",
    "OUT = 'V22'\n",
    "PATH = '../data/'\n",
    "\n",
    "NUM_WORKERS = 24\n",
    "SEED = 2023\n",
    "bs = 1024 - 256\n",
    "L = 192\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "os.makedirs(OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835aa86f-8905-4cdd-a439-2973c73d32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WrapperAdamW(param_groups,**kwargs):\n",
    "    return OptimWrapper(param_groups,torch.optim.AdamW)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_matching_weights(model, weights_path):\n",
    "    \"\"\"\n",
    "    Load model weights from a given path if they match, otherwise skip.\n",
    "    Prints the number of matched and unmatched weights.\n",
    "\n",
    "    :param model: The PyTorch model for which weights should be loaded.\n",
    "    :param weights_path: The path to the saved weights file (.pth or .pt).\n",
    "    \"\"\"\n",
    "    # Load the saved state dictionary\n",
    "    saved_state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # Get the model's state dictionary\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    # Create a new state dictionary to store matching weights\n",
    "    matching_state_dict = {}\n",
    "\n",
    "    # Initialize counters for matched and unmatched weights\n",
    "    matched_weights = 0\n",
    "    unmatched_weights = 0\n",
    "\n",
    "    # Iterate through the saved state dictionary\n",
    "    for name, saved_weight in saved_state_dict.items():\n",
    "        # Check if the name exists in the model's state dictionary and the shapes match\n",
    "        if name in model_state_dict and model_state_dict[name].shape == saved_weight.shape:\n",
    "            # If it matches, add it to the matching state dictionary\n",
    "            matching_state_dict[name] = saved_weight\n",
    "            matched_weights += 1\n",
    "        else:\n",
    "            print(f\"Skipping weight: {name} - Shape mismatch or not found in model\")\n",
    "            unmatched_weights += 1\n",
    "\n",
    "    # Update the model's state dictionary with the matching state dictionary\n",
    "    model_state_dict.update(matching_state_dict)\n",
    "\n",
    "    # Load the updated state dictionary into the model\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    print(f\"Matched weights: {matched_weights}\")\n",
    "    print(f\"Unmatched weights: {unmatched_weights}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = OUT\n",
    "\n",
    "ds_train = IceCubeCache(PATH, mode='train', L=L, selection=SELECTION,reduce_size=0.125)\n",
    "ds_train_len = IceCubeCache(PATH, mode='train', L=L, reduce_size=0.125, selection=SELECTION, mask_only=True)\n",
    "sampler_train = RandomChunkSampler(ds_train_len, chunks=ds_train.chunks)\n",
    "len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=bs, drop_last=True)\n",
    "dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "            batch_sampler=len_sampler_train, num_workers=NUM_WORKERS, persistent_workers=True))\n",
    "\n",
    "ds_val = IceCubeCache(PATH, mode='eval', L=L, selection=SELECTION)\n",
    "ds_val_len = IceCubeCache(PATH, mode='eval', L=L, selection=SELECTION, mask_only=True)\n",
    "sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=bs, drop_last=False)\n",
    "dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, batch_sampler=len_sampler_val,\n",
    "            num_workers=0))\n",
    "\n",
    "\n",
    "data = DataLoaders(dl_train,dl_val)\n",
    "model = EncoderWithDirectionReconstructionV22(dim=384, dim_base=128, depth=8, head_size=32)\n",
    "#load_matching_weights(model, '/opt/slh/icecube/hb_training_loop/V20FT3/models/model_7.pth')\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "learn = Learner(data,\n",
    "                model,  \n",
    "                path = OUT, \n",
    "                loss_func=loss_vms,\n",
    "                cbs=[GradientClip(3.0),\n",
    "                     CSVLogger(),\n",
    "                     SaveModelCallback(monitor='loss',comp=np.less,every_epoch=True),\n",
    "                     GradientAccumulation(n_acc=4096//bs)],\n",
    "                     metrics=[loss], \n",
    "                     opt_func=partial(WrapperAdamW,eps=1e-7)).to_fp16()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb30a33-80e3-4d26-bacc-819e96e73cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      62.50% [5/8 14:48:03&lt;8:52:50]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.433667</td>\n",
       "      <td>1.432677</td>\n",
       "      <td>1.001155</td>\n",
       "      <td>2:57:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.391384</td>\n",
       "      <td>1.422149</td>\n",
       "      <td>0.995766</td>\n",
       "      <td>2:57:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.356437</td>\n",
       "      <td>1.399272</td>\n",
       "      <td>0.990360</td>\n",
       "      <td>2:57:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.341106</td>\n",
       "      <td>1.382831</td>\n",
       "      <td>0.986583</td>\n",
       "      <td>2:57:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.341254</td>\n",
       "      <td>1.366049</td>\n",
       "      <td>0.982490</td>\n",
       "      <td>2:57:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='11314' class='' max='21314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      53.08% [11314/21314 1:27:17&lt;1:17:09 1.3398]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/slh/icecube/icecube/data_train_v3.py:264: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  L = max(1,L // 16)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/nccl.py:51: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(8, lr_max=5e-4, wd=0.05, pct_start=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea055ef5-5287-4eb9-bcf2-e82a3399b1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be403c-678a-4cc9-93ed-10ed1d40d803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567cddd-6f23-447f-8177-e6be1cf0c7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
