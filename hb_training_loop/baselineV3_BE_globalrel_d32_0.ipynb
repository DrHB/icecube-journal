{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/opt/slh/icecube/')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "#os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd6834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "from icecube.fastai_fix import *\n",
    "from tqdm.notebook import tqdm\n",
    "from icecube.data_train_v3 import RandomChunkSampler,LenMatchBatchSampler,IceCubeCache, DeviceDataLoader\n",
    "from icecube.loss import loss, loss_vms\n",
    "from icecube.models import EncoderWithDirectionReconstructionV22\n",
    "from fastxtend.vision.all import EMACallback\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = 'init'\n",
    "PATH = '../data/'\n",
    "\n",
    "NUM_WORKERS = 12\n",
    "SEED = 2023\n",
    "bs = 128\n",
    "L = 192\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "def WrapperAdamW(param_groups,**kwargs):\n",
    "    return OptimWrapper(param_groups,torch.optim.AdamW)\n",
    "\n",
    "def get_save_fname():\n",
    "    return 'model'+''.join(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement \n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "#BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if self.gamma_1 is None:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class Attention_rel(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
    "            proj_drop=0.0, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.proj_q = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.proj_k = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.proj_v = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, q, k, v, rel_pos_bias=None, key_padding_mask=None):\n",
    "        #rel_pos_bias: B L L C/h\n",
    "        #key_padding_mask - float with -inf\n",
    "        B, N, C = q.shape\n",
    "        #qkv_bias = None\n",
    "        #if self.q_bias is not None:\n",
    "        #    qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        #qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        #qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        #q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        \n",
    "        q = F.linear(input=q, weight=self.proj_q.weight, bias=self.q_bias)\n",
    "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        k = F.linear(input=k, weight=self.proj_k.weight, bias=None)\n",
    "        k = k.reshape(B, k.shape[1], self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        v = F.linear(input=v, weight=self.proj_v.weight, bias=self.v_bias)\n",
    "        v = v.reshape(B, v.shape[1], self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        if rel_pos_bias is not None:\n",
    "            bias = torch.einsum('bhic,bijc->bhij', q, rel_pos_bias)\n",
    "            attn = attn + bias\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dtype == torch.float32 or key_padding_mask.dtype == torch.float16, \\\n",
    "                'incorrect mask dtype'\n",
    "            bias = torch.min(key_padding_mask[:,None,:], key_padding_mask[:,:,None])\n",
    "            bias[torch.max(key_padding_mask[:,None,:], key_padding_mask[:,:,None]) < 0] = 0\n",
    "            #print(bias.shape,bias.min(),bias.max())\n",
    "            attn = attn + bias.unsqueeze(1)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2)\n",
    "        if rel_pos_bias is not None:\n",
    "            x = x + torch.einsum('bhij,bijc->bihc', attn, rel_pos_bias)\n",
    "        x = x.reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "#BEiTv2 block\n",
    "class Block_rel(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention_rel(dim, num_heads, attn_drop=attn_drop, qkv_bias=qkv_bias)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None, rel_pos_bias=None, kv=None):\n",
    "        if self.gamma_1 is None:\n",
    "            xn = self.norm1(x)\n",
    "            kv = xn if kv is None else self.norm1(kv)\n",
    "            x = x + self.drop_path(self.attn(xn, kv, kv,\n",
    "                            rel_pos_bias=rel_pos_bias,\n",
    "                            key_padding_mask=key_padding_mask))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            xn = self.norm1(x)\n",
    "            kv = xn if kv is None else self.norm1(kv)\n",
    "            x = x + self.drop_path(self.gamma_1 * self.drop_path(self.attn(xn, kv, kv,\n",
    "                            rel_pos_bias=rel_pos_bias,\n",
    "                            key_padding_mask=key_padding_mask)))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, dim_base=128, dim=384):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim_base)\n",
    "        self.aux_emb = nn.Embedding(2,dim_base//2)\n",
    "        self.emb2 = SinusoidalPosEmb(dim=dim_base//2)\n",
    "        self.proj = nn.Sequential(nn.Linear(6*dim_base,6*dim_base),nn.LayerNorm(6*dim_base),\n",
    "                                  nn.GELU(),nn.Linear(6*dim_base,dim))\n",
    "        \n",
    "    def forward(self, x, Lmax=None):\n",
    "        pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "        charge = x['charge'] if Lmax is None else x['charge'][:,:Lmax]\n",
    "        time = x['time'] if Lmax is None else x['time'][:,:Lmax]\n",
    "        auxiliary = x['auxiliary'] if Lmax is None else x['auxiliary'][:,:Lmax]\n",
    "        qe = x['qe'] if Lmax is None else x['qe'][:,:Lmax]\n",
    "        ice_properties = x['ice_properties'] if Lmax is None else x['ice_properties'][:,:Lmax]\n",
    "        length = torch.log10(x['L0'].to(dtype=pos.dtype))\n",
    "        \n",
    "        x = torch.cat([self.emb(4096*pos).flatten(-2), self.emb(1024*charge),\n",
    "                       self.emb(4096*time),self.aux_emb(auxiliary),\n",
    "                       self.emb2(length).unsqueeze(1).expand(-1,pos.shape[1],-1)\n",
    "                      ],-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class Rel_ds(nn.Module):\n",
    "    def __init__(self, dim=32):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim)\n",
    "        self.proj = nn.Linear(dim,dim)\n",
    "        \n",
    "    def forward(self, x, Lmax=None):\n",
    "        pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "        time = x['time'] if Lmax is None else x['time'][:,:Lmax]\n",
    "        ds2 = (pos[:,:,None] - pos[:,None,:]).pow(2).sum(-1) - \\\n",
    "                ((time[:,:,None] - time[:,None,:])*(3e4/500*3e-1)).pow(2)\n",
    "        d = torch.sign(ds2)*torch.sqrt(torch.abs(ds2))\n",
    "        emb = self.emb(1024*d.clip(-4,4))\n",
    "        rel_attn = self.proj(emb)\n",
    "        return rel_attn,emb\n",
    "\n",
    "def get_nbs(x, Lmax=None, K=8):\n",
    "    pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "    mask = x['mask'][:,:Lmax]\n",
    "    B = pos.shape[0]\n",
    "    \n",
    "    d = -torch.cdist(pos, pos, p=2) \n",
    "    d -= 100*(~torch.min(mask[:,None,:],mask[:,:,None]))\n",
    "    d -= 200*torch.eye(Lmax, dtype=pos.dtype, device=pos.device).unsqueeze(0)\n",
    "    nbs = d.topk(K-1,dim=-1)[1]\n",
    "    nbs = torch.cat([\n",
    "            torch.arange(Lmax, dtype=nbs.dtype, device=nbs.device).unsqueeze(0).unsqueeze(-1).expand(B,-1,-1),\n",
    "            nbs],-1)\n",
    "    return nbs\n",
    "    \n",
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, dim=192, num_heads=192//64, mlp_ratio=4, drop_path=0, init_values=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.proj_rel_bias = nn.Linear(dim//num_heads,dim//num_heads)\n",
    "        self.block = Block_rel(dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                               drop_path=drop_path, init_values=init_values)\n",
    "        \n",
    "    def forward(self, x, nbs, key_padding_mask=None, rel_pos_bias=None):\n",
    "        B,Lmax,C = x.shape\n",
    "        mask = key_padding_mask if not (key_padding_mask is None) \\\n",
    "            else torch.ones(B, Lmax, dtype=torch.bool, device=x.deice)\n",
    "        \n",
    "        m = torch.gather(mask.unsqueeze(1).expand(-1,Lmax,-1), 2, nbs)\n",
    "        attn_mask = torch.zeros(m.shape, device=m.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        attn_mask = attn_mask[mask]\n",
    "        \n",
    "        if rel_pos_bias is not None:\n",
    "            rel_pos_bias = torch.gather(rel_pos_bias, 2, \n",
    "                                        nbs.unsqueeze(-1).expand(-1,-1,-1,rel_pos_bias.shape[-1]))\n",
    "            rel_pos_bias = rel_pos_bias[mask]\n",
    "            rel_pos_bias = self.proj_rel_bias(rel_pos_bias).unsqueeze(1)\n",
    "            \n",
    "        xl = torch.gather(x.unsqueeze(1).expand(-1,Lmax,-1,-1), 2, nbs.unsqueeze(-1).expand(-1,-1,-1,C))\n",
    "        xl = xl[mask]\n",
    "        # modify only the node (0th element)\n",
    "        #print(xl[:,:1].shape,rel_pos_bias.shape,attn_mask[:,:1].shape,xl.shape)\n",
    "        xl = self.block(xl[:,:1], rel_pos_bias=rel_pos_bias, key_padding_mask=attn_mask[:,:1], kv=xl)\n",
    "        x = torch.zeros(x.shape, device=x.device, dtype=xl.dtype)\n",
    "        x[mask] = xl.squeeze(1)\n",
    "        return x\n",
    "\n",
    "class DeepIceModel(nn.Module):\n",
    "    def __init__(self, dim=384, dim_base=128, depth=8, use_checkpoint=False, head_size=64, **kwargs):\n",
    "        super().__init__()\n",
    "        self.extractor = Extractor(dim_base,dim)\n",
    "        self.rel_pos = Rel_ds(head_size)\n",
    "        self.sandwich = nn.ModuleList([ \n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "        ])\n",
    "        self.cls_token = nn.Linear(dim,1,bias=False)\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            Block(\n",
    "                dim=dim, num_heads=dim//head_size, mlp_ratio=4, drop_path=0.0*(i/(depth-1)), init_values=1,)\n",
    "            for i in range(depth)])\n",
    "        self.proj_out = nn.Linear(dim,3)\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.apply(self._init_weights)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "        self.apply(_init_weights)\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        x = self.extractor(x0, Lmax)\n",
    "        rel_pos_bias, rel_enc = self.rel_pos(x0, Lmax)\n",
    "        #nbs = get_nbs(x0, Lmax)\n",
    "        mask = mask[:,:Lmax]\n",
    "        B,_ = mask.shape\n",
    "        attn_mask = torch.zeros(mask.shape, device=mask.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        \n",
    "        for blk in self.sandwich:\n",
    "            if isinstance(blk,LocalBlock):\n",
    "                x = blk(x,nbs,mask,rel_enc)\n",
    "            else:\n",
    "                x = blk(x,attn_mask,rel_pos_bias)\n",
    "                rel_pos_bias = None\n",
    "        \n",
    "        mask = torch.cat([torch.ones(B,1,dtype=mask.dtype, device=mask.device),mask],1)\n",
    "        attn_mask = torch.zeros(mask.shape, device=mask.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(B,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, None, attn_mask)\n",
    "            else: x = blk(x, None, attn_mask)\n",
    "                \n",
    "        x = self.proj_out(x[:,0]) #cls token\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9464d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4G+12G\n",
    "fname = 'baselineV3_BE_globalrel_d32_0'\n",
    "\n",
    "ds_train = IceCubeCache(PATH, mode='train', L=L, reduce_size=0.125)\n",
    "ds_train_len = IceCubeCache(PATH, mode='train', L=L, reduce_size=0.125, mask_only=True)\n",
    "sampler_train = RandomChunkSampler(ds_train_len, chunks=ds_train.chunks)\n",
    "len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=bs, drop_last=True)\n",
    "dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "            batch_sampler=len_sampler_train, num_workers=16, persistent_workers=True))\n",
    "\n",
    "ds_val = IceCubeCache(PATH, mode='eval', L=L)\n",
    "ds_val_len = IceCubeCache(PATH, mode='eval', L=L, mask_only=True)\n",
    "sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=bs, drop_last=False)\n",
    "dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, batch_sampler=len_sampler_val,\n",
    "            num_workers=0))\n",
    "\n",
    "data = DataLoaders(dl_train,dl_val)\n",
    "model = DeepIceModel(dim=1280, dim_base=256, depth=24, head_size=64)  \n",
    "#model.load_state_dict(torch.load(os.path.join(OUT,f'{fname}_0.pth')))\n",
    "model = nn.DataParallel(model) #don't forget to add \"module\" when save\n",
    "#model = model.cuda()\n",
    "#model = torch.compile(model)#, mode=\"reduce-overhead\")\n",
    "learn = Learner(data, model, loss_func=loss_vms,cbs=[GradientClip(3.0),\n",
    "            SaveModelCallback(monitor='loss',comp=np.less,at_end=True,fname=get_save_fname()),\n",
    "            GradientAccumulation(n_acc=4096//bs)],\n",
    "            metrics=[loss], opt_func=partial(WrapperAdamW,eps=1e-7)).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39aa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(8, lr_max=1e-4, wd=0.05, pct_start=0.01)\n",
    "torch.save(learn.model.state_dict(),os.path.join(OUT,f'{fname}_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7da72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 \t1.458575 \t1.529845 \t1.012041 \t2:20:54\n",
    "1 \t1.446206 \t1.495052 \t1.007132 \t2:21:32\n",
    "2 \t1.448871 \t1.444523 \t1.003260 \t2:20:31\n",
    "3 \t1.435299 \t1.439746 \t0.994916 \t2:21:49\n",
    "4 \t1.380146 \t1.422482 \t0.990330 \t2:22:31\n",
    "5 \t1.394013 \t1.400562 \t0.983808 \t2:22:16\n",
    "6 \t1.381656 \t1.366582 \t0.981285 \t2:22:22\n",
    "7 \t1.357780 \t1.361542 \t0.977381 \t2:22:06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f626e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd443f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93f4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
