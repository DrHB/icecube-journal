{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc17aeb5-65b2-4ae7-a7eb-63b05a8651f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn, einsum\n",
    "import torch\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import rearrange, repeat\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn.pool import knn_graph\n",
    "from graphnet.models.gnn.gnn import GNN\n",
    "from graphnet.models.utils import calculate_xyzt_homophily\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_min, scatter_sum\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n",
    "from torch import Tensor, LongTensor\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_geometric.nn import EdgeConv\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, SiLU, Sequential\n",
    "from graphnet.utilities.config import save_model_config\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "GLOBAL_POOLINGS = {\n",
    "    \"min\": scatter_min,\n",
    "    \"max\": scatter_max,\n",
    "    \"sum\": scatter_sum,\n",
    "    \"mean\": scatter_mean,\n",
    "}\n",
    "\n",
    "class DynEdgeConv(EdgeConv):\n",
    "    \"\"\"Dynamical edge convolution layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn: Callable,\n",
    "        aggr: str = \"max\",\n",
    "        nb_neighbors: int = 8,\n",
    "        features_subset: Optional[Union[Sequence[int], slice]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdgeConv`.\n",
    "        Args:\n",
    "            nn: The MLP/torch.Module to be used within the `EdgeConv`.\n",
    "            aggr: Aggregation method to be used with `EdgeConv`.\n",
    "            nb_neighbors: Number of neighbours to be clustered after the\n",
    "                `EdgeConv` operation.\n",
    "            features_subset: Subset of features in `Data.x` that should be used\n",
    "                when dynamically performing the new graph clustering after the\n",
    "                `EdgeConv` operation. Defaults to all features.\n",
    "            **kwargs: Additional features to be passed to `EdgeConv`.\n",
    "        \"\"\"\n",
    "        # Check(s)\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(None)  # Use all features\n",
    "        assert isinstance(features_subset, (list, slice))\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nn=nn, aggr=aggr, **kwargs)\n",
    "\n",
    "        # Additional member variables\n",
    "        self.nb_neighbors = nb_neighbors\n",
    "        self.features_subset = features_subset\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, edge_index: Adj, batch: Optional[Tensor] = None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Standard EdgeConv forward pass\n",
    "        x = super().forward(x, edge_index)\n",
    "        dev = x.device\n",
    "\n",
    "        # Recompute adjacency\n",
    "        edge_index = knn_graph(\n",
    "            x=x[:, self.features_subset],\n",
    "            k=self.nb_neighbors,\n",
    "            batch=batch,\n",
    "        ).to(dev)\n",
    "\n",
    "        return x, edge_index\n",
    "    \n",
    "    \n",
    "class DynEdgeFEXTRACTRO(GNN):\n",
    "    \"\"\"DynEdge (dynamical edge convolutional) model.\"\"\"\n",
    "\n",
    "    @save_model_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_inputs: int,\n",
    "        *,\n",
    "        nb_neighbours: int = 8,\n",
    "        features_subset: Optional[Union[List[int], slice]] = None,\n",
    "        dynedge_layer_sizes: Optional[List[Tuple[int, ...]]] = None,\n",
    "        post_processing_layer_sizes: Optional[List[int]] = None,\n",
    "        readout_layer_sizes: Optional[List[int]] = None,\n",
    "        global_pooling_schemes: Optional[Union[str, List[str]]] = None,\n",
    "        add_global_variables_after_pooling: bool = False,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdge`.\n",
    "        Args:\n",
    "            nb_inputs: Number of input features on each node.\n",
    "            nb_neighbours: Number of neighbours to used in the k-nearest\n",
    "                neighbour clustering which is performed after each (dynamical)\n",
    "                edge convolution.\n",
    "            features_subset: The subset of latent features on each node that\n",
    "                are used as metric dimensions when performing the k-nearest\n",
    "                neighbours clustering. Defaults to [0,1,2].\n",
    "            dynedge_layer_sizes: The layer sizes, or latent feature dimenions,\n",
    "                used in the `DynEdgeConv` layer. Each entry in\n",
    "                `dynedge_layer_sizes` corresponds to a single `DynEdgeConv`\n",
    "                layer; the integers in the corresponding tuple corresponds to\n",
    "                the layer sizes in the multi-layer perceptron (MLP) that is\n",
    "                applied within each `DynEdgeConv` layer. That is, a list of\n",
    "                size-two tuples means that all `DynEdgeConv` layers contain a\n",
    "                two-layer MLP.\n",
    "                Defaults to [(128, 256), (336, 256), (336, 256), (336, 256)].\n",
    "            post_processing_layer_sizes: Hidden layer sizes in the MLP\n",
    "                following the skip-concatenation of the outputs of each\n",
    "                `DynEdgeConv` layer. Defaults to [336, 256].\n",
    "            readout_layer_sizes: Hidden layer sizes in the MLP following the\n",
    "                post-processing _and_ optional global pooling. As this is the\n",
    "                last layer(s) in the model, the last layer in the read-out\n",
    "                yields the output of the `DynEdge` model. Defaults to [128,].\n",
    "            global_pooling_schemes: The list global pooling schemes to use.\n",
    "                Options are: \"min\", \"max\", \"mean\", and \"sum\".\n",
    "            add_global_variables_after_pooling: Whether to add global variables\n",
    "                after global pooling. The alternative is to  added (distribute)\n",
    "                them to the individual nodes before any convolutional\n",
    "                operations.\n",
    "        \"\"\"\n",
    "        # Latent feature subset for computing nearest neighbours in DynEdge.\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(0, 3)\n",
    "\n",
    "        # DynEdge layer sizes\n",
    "        if dynedge_layer_sizes is None:\n",
    "            dynedge_layer_sizes = [\n",
    "                (\n",
    "                    128,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        assert isinstance(dynedge_layer_sizes, list)\n",
    "        assert len(dynedge_layer_sizes)\n",
    "        assert all(isinstance(sizes, tuple) for sizes in dynedge_layer_sizes)\n",
    "        assert all(len(sizes) > 0 for sizes in dynedge_layer_sizes)\n",
    "        assert all(all(size > 0 for size in sizes) for sizes in dynedge_layer_sizes)\n",
    "\n",
    "        self._dynedge_layer_sizes = dynedge_layer_sizes\n",
    "\n",
    "        # Post-processing layer sizes\n",
    "        if post_processing_layer_sizes is None:\n",
    "            post_processing_layer_sizes = [\n",
    "                336,\n",
    "                256,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(post_processing_layer_sizes, list)\n",
    "        assert len(post_processing_layer_sizes)\n",
    "        assert all(size > 0 for size in post_processing_layer_sizes)\n",
    "\n",
    "        self._post_processing_layer_sizes = post_processing_layer_sizes\n",
    "\n",
    "        # Read-out layer sizes\n",
    "        if readout_layer_sizes is None:\n",
    "            readout_layer_sizes = [\n",
    "                128,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(readout_layer_sizes, list)\n",
    "        assert len(readout_layer_sizes)\n",
    "        assert all(size > 0 for size in readout_layer_sizes)\n",
    "\n",
    "        self._readout_layer_sizes = readout_layer_sizes\n",
    "\n",
    "        # Global pooling scheme(s)\n",
    "        if isinstance(global_pooling_schemes, str):\n",
    "            global_pooling_schemes = [global_pooling_schemes]\n",
    "\n",
    "        if isinstance(global_pooling_schemes, list):\n",
    "            for pooling_scheme in global_pooling_schemes:\n",
    "                assert (\n",
    "                    pooling_scheme in GLOBAL_POOLINGS\n",
    "                ), f\"Global pooling scheme {pooling_scheme} not supported.\"\n",
    "        else:\n",
    "            assert global_pooling_schemes is None\n",
    "\n",
    "        self._global_pooling_schemes = global_pooling_schemes\n",
    "\n",
    "        if add_global_variables_after_pooling:\n",
    "            assert self._global_pooling_schemes, (\n",
    "                \"No global pooling schemes were request, so cannot add global\"\n",
    "                \" variables after pooling.\"\n",
    "            )\n",
    "        self._add_global_variables_after_pooling = add_global_variables_after_pooling\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nb_inputs, self._readout_layer_sizes[-1])\n",
    "\n",
    "        # Remaining member variables()\n",
    "        self._activation = torch.nn.GELU()\n",
    "        self._nb_inputs = nb_inputs\n",
    "        self._nb_global_variables = 5 + nb_inputs\n",
    "        self._nb_neighbours = nb_neighbours\n",
    "        self._features_subset = features_subset\n",
    "\n",
    "        self._construct_layers()\n",
    "\n",
    "    def _construct_layers(self) -> None:\n",
    "        \"\"\"Construct layers (torch.nn.Modules).\"\"\"\n",
    "        # Convolutional operations\n",
    "        nb_input_features = self._nb_inputs\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            nb_input_features += self._nb_global_variables\n",
    "\n",
    "        self._conv_layers = torch.nn.ModuleList()\n",
    "        nb_latent_features = nb_input_features\n",
    "        for sizes in self._dynedge_layer_sizes:\n",
    "            layers = []\n",
    "            layer_sizes = [nb_latent_features] + list(sizes)\n",
    "            for ix, (nb_in, nb_out) in enumerate(\n",
    "                zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "            ):\n",
    "                if ix == 0:\n",
    "                    nb_in *= 2\n",
    "                layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "                layers.append(nn.LayerNorm(nb_out))\n",
    "                layers.append(self._activation)\n",
    "\n",
    "            conv_layer = DynEdgeConv(\n",
    "                torch.nn.Sequential(*layers),\n",
    "                aggr=\"add\",\n",
    "                nb_neighbors=self._nb_neighbours,\n",
    "                features_subset=self._features_subset,\n",
    "            )\n",
    "            self._conv_layers.append(conv_layer)\n",
    "\n",
    "            nb_latent_features = nb_out\n",
    "\n",
    "        # Post-processing operations\n",
    "        nb_latent_features = (\n",
    "            sum(sizes[-1] for sizes in self._dynedge_layer_sizes) + nb_input_features\n",
    "        )\n",
    "\n",
    "        post_processing_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._post_processing_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            post_processing_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            post_processing_layers.append(nn.LayerNorm(nb_out))\n",
    "            post_processing_layers.append(self._activation)\n",
    "\n",
    "        self._post_processing = torch.nn.Sequential(*post_processing_layers)\n",
    "\n",
    "        # Read-out operations\n",
    "        nb_poolings = (\n",
    "            len(self._global_pooling_schemes) if self._global_pooling_schemes else 1\n",
    "        )\n",
    "        nb_latent_features = nb_out * nb_poolings\n",
    "        if self._add_global_variables_after_pooling:\n",
    "            nb_latent_features += self._nb_global_variables\n",
    "\n",
    "        readout_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._readout_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            readout_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            readout_layers.append(nn.LayerNorm(nb_out))\n",
    "            readout_layers.append(self._activation)\n",
    "\n",
    "        self._readout = torch.nn.Sequential(*readout_layers)\n",
    "\n",
    "    def _global_pooling(self, x: Tensor, batch: LongTensor) -> Tensor:\n",
    "        \"\"\"Perform global pooling.\"\"\"\n",
    "        assert self._global_pooling_schemes\n",
    "        pooled = []\n",
    "        for pooling_scheme in self._global_pooling_schemes:\n",
    "            pooling_fn = GLOBAL_POOLINGS[pooling_scheme]\n",
    "            pooled_x = pooling_fn(x, index=batch, dim=0)\n",
    "            if isinstance(pooled_x, tuple) and len(pooled_x) == 2:\n",
    "                # `scatter_{min,max}`, which return also an argument, vs.\n",
    "                # `scatter_{mean,sum}`\n",
    "                pooled_x, _ = pooled_x\n",
    "            pooled.append(pooled_x)\n",
    "\n",
    "        return torch.cat(pooled, dim=1)\n",
    "\n",
    "    def _calculate_global_variables(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: LongTensor,\n",
    "        batch: LongTensor,\n",
    "        *additional_attributes: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Calculate global variables.\"\"\"\n",
    "        # Calculate homophily (scalar variables)\n",
    "        h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n",
    "\n",
    "        # Calculate mean features\n",
    "        global_means = scatter_mean(x, batch, dim=0)\n",
    "\n",
    "        # Add global variables\n",
    "        global_variables = torch.cat(\n",
    "            [\n",
    "                global_means,\n",
    "                h_x,\n",
    "                h_y,\n",
    "                h_z,\n",
    "                h_t,\n",
    "            ]\n",
    "            + [attr.unsqueeze(dim=1) for attr in additional_attributes],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return global_variables\n",
    "\n",
    "    def forward(self, x, edge_index, batch, n_pulses) -> Tensor:\n",
    "        \"\"\"Apply learnable forward pass.\"\"\"\n",
    "        # Convenience variables\n",
    "        global_variables = self._calculate_global_variables(\n",
    "            x,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            torch.log10(n_pulses),\n",
    "        )\n",
    "\n",
    "        # Distribute global variables out to each node\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            distribute = (\n",
    "                batch.unsqueeze(dim=1) == torch.unique(batch).unsqueeze(dim=0)\n",
    "            ).type(torch.float)\n",
    "\n",
    "            global_variables_distributed = torch.sum(\n",
    "                distribute.unsqueeze(dim=2) * global_variables.unsqueeze(dim=0),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            x = torch.cat((x, global_variables_distributed), dim=1)\n",
    "\n",
    "        # DynEdge-convolutions\n",
    "        skip_connections = [x]\n",
    "        for conv_layer in self._conv_layers:\n",
    "            x, edge_index = conv_layer(x, edge_index, batch)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        # Skip-cat\n",
    "        x = torch.cat(skip_connections, dim=1)\n",
    "        x = self._post_processing(x)\n",
    "        return x, edge_index, batch\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def batched_index_select(values, indices):\n",
    "    last_dim = values.shape[-1]\n",
    "    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x + self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class LAttentionV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        and_self_attend = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.and_self_attend = and_self_attend\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        context,\n",
    "        mask = None\n",
    "    ):\n",
    "        h, scale = self.heads, self.scale\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = -torch.finfo(dots.dtype).max\n",
    "            mask = rearrange(mask, 'b n -> b 1 1 n')\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        attn = dots.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LocalLatentsAttent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        num_latents = 64,\n",
    "        latent_self_attend = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "        self.attn1 = LAttentionV2(dim, heads, and_self_attend = latent_self_attend)\n",
    "        self.attn2 = LAttentionV2(dim, heads)\n",
    "\n",
    "    def forward(self, x, latents = None, mask = None):\n",
    "        b, *_ = x.shape\n",
    "\n",
    "        latents = self.latents\n",
    "\n",
    "        if latents.ndim == 2:\n",
    "            latents = repeat(latents, 'n d -> b n d', b = b)\n",
    "\n",
    "        latents = self.attn1(latents, x, mask = mask)\n",
    "        out     = self.attn2(x, latents)\n",
    "\n",
    "        return out, latents\n",
    "    \n",
    "class LocalAttenV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        num_latents = 64,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = PreNorm(dim, LocalLatentsAttent(\n",
    "                dim = dim,\n",
    "                heads = heads,\n",
    "                num_latents = num_latents\n",
    "            )) \n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                global_attn,\n",
    "                Residual(PreNorm(dim, FeedForward(\n",
    "                    dim = dim,\n",
    "                    dropout = ff_dropout\n",
    "                )))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        for attn, ff in self.layers:\n",
    "            out, _ = attn(x, mask = mask)\n",
    "            x = x + out\n",
    "            x = ff(x)\n",
    "        return x\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "    \n",
    "class LocalGlobalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_neighbors_cutoff = None,\n",
    "        attn_dropout = 0.1,\n",
    "        ff_dropout=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_neighbors_cutoff = num_neighbors_cutoff\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = PreNorm(dim, LocalLatentsAttent(\n",
    "                dim = dim,\n",
    "                heads = heads,\n",
    "                num_latents = 64\n",
    "            )) \n",
    "        \n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, NMatrixAttention(\n",
    "                    dim = dim,\n",
    "                    dim_head = dim_head,\n",
    "                    heads = heads,\n",
    "                    dropout = attn_dropout\n",
    "                ))),\n",
    "                global_attn,\n",
    "                Residual(PreNorm(dim, FeedForward(\n",
    "                    dim = dim,\n",
    "                    dropout = ff_dropout\n",
    "                )))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, adjacency_mat, mask = None):\n",
    "        device, n = x.device, x.shape[1]\n",
    "\n",
    "        diag = torch.eye(adjacency_mat.shape[-1], device = device).bool()\n",
    "        adjacency_mat |= diag\n",
    "        if exists(mask):\n",
    "            adjacency_mat &= (mask[:, :, None] * mask[:, None, :])\n",
    "\n",
    "        adj_mat = adjacency_mat.float()\n",
    "        max_neighbors = int(adj_mat.sum(dim = -1).max())\n",
    "\n",
    "        if exists(self.num_neighbors_cutoff) and max_neighbors > self.num_neighbors_cutoff:\n",
    "            noise = torch.empty((n, n), device = device).uniform_(-0.01, 0.01)\n",
    "            adj_mat = adj_mat + noise\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = self.num_neighbors_cutoff)\n",
    "            adj_mask = (adj_mask > 0.5).float()\n",
    "        else:\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = max_neighbors)\n",
    "        \n",
    "        for attn, locla_attn, ff in self.layers:\n",
    "            x = attn(\n",
    "                x,\n",
    "                adj_kv_indices = adj_kv_indices,\n",
    "                mask = adj_mask\n",
    "            )\n",
    "            out, _ = locla_attn(x, mask = mask)\n",
    "            x = x + out\n",
    "            x = ff(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "class ExtractorV0(nn.Module):\n",
    "    def __init__(self, dim_base=128, dim=384, proj = True):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim_base)\n",
    "        self.emb2 = SinusoidalPosEmb(dim=dim_base//2)\n",
    "        self.aux_emb = nn.Embedding(2,dim_base//2)\n",
    "        self.qe_emb = nn.Embedding(2,dim_base//2)\n",
    "        self.proj = nn.Linear(dim_base*7,dim) if proj else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, Lmax=None):\n",
    "        pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "        charge = x['charge'] if Lmax is None else x['charge'][:,:Lmax]\n",
    "        time = x['time'] if Lmax is None else x['time'][:,:Lmax]\n",
    "        auxiliary = x['aux'] if Lmax is None else x['auxiliary'][:,:Lmax]\n",
    "        qe = x['qe'] if Lmax is None else x['qe'][:,:Lmax]\n",
    "        ice_properties = x['ice_properties'] if Lmax is None else x['ice_properties'][:,:Lmax]\n",
    "        \n",
    "        x = torch.cat([self.emb(100*pos).flatten(-2), self.emb(40*charge),\n",
    "                       self.emb(100*time),self.aux_emb(auxiliary),self.qe_emb(qe),\n",
    "                       self.emb2(50*ice_properties).flatten(-2)],-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BeDropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(BeDropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class BeMLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement \n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "#BEiTv2 Beblock\n",
    "class BeBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = BeDropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = BeMLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if self.gamma_1 is None:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.gamma_1 * self.drop_path(self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0]))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BeDeepIceModel(nn.Module):\n",
    "    def __init__(self, dim=384, depth=12, out_class = 3, use_checkpoint=False, drop_b= 0., div_factor=64, attn_drop_b = 0., drop_path = 0.,  **kwargs):\n",
    "        super().__init__()\n",
    "        self.Beblocks = nn.ModuleList([ \n",
    "            BeBlock(\n",
    "                dim=dim, num_heads=dim//div_factor, mlp_ratio=4, drop_path=drop_path, init_values=1, attn_drop=attn_drop_b, drop=drop_b)\n",
    "            for i in range(depth)])\n",
    "        #self.Beblocks = nn.ModuleList([ \n",
    "        #    nn.TransformerEncoderLayer(dim,dim//64,dim*4,dropout=0,\n",
    "        #        activation=nn.GELU(), batch_first=True, norm_first=True)\n",
    "        #    for i in range(depth)])\n",
    "        self.out_class = out_class\n",
    "        self.proj_out = nn.Linear(dim,out_class) if out_class == 3 else nn.Identity()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.Beblocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "        self.apply(_init_weights)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        attn_mask = torch.zeros(mask.shape, device=mask.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        for blk in self.Beblocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, None, attn_mask)\n",
    "            else: x = blk(x, None, attn_mask)\n",
    "        if self.out_class == 3:\n",
    "            x = self.proj_out(x[:,0]) #cls token\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EncoderWithDirectionReconstructionV18(nn.Module):\n",
    "    def __init__(self, dim_out=256, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out//2, dim_base=32)\n",
    "        self.encoder = BeDeepIceModel(dim_out , drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.local_root= DynEdgeFEXTRACTRO(9, \n",
    "                                           post_processing_layer_sizes = [336, dim_out//2], \n",
    "                                           dynedge_layer_sizes = [(128, 256), (336, 256), (336, 256), (336, 256)])\n",
    "        self.global_root =  LocalAttenV2(dim = dim_out//2, depth =4)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        graph_featutre = torch.concat([batch[\"pos\"][mask] , \n",
    "                             batch['time'][mask].view(-1, 1),\n",
    "                             batch['auxiliary'][mask].view(-1, 1),\n",
    "                             batch['qe'][mask].view(-1, 1),\n",
    "                             batch['charge'][mask].view(-1, 1),\n",
    "                             batch[\"ice_properties\"][mask], \n",
    "                              ], dim=1)\n",
    "        bs = mask.shape[0] # int\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = graph_featutre[:,:3], k=8, batch=batch_index).to(mask.device)\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        \n",
    "        graph_featutre, _, _ = self.local_root(graph_featutre, edge_index, batch_index, mask.sum(-1))\n",
    "        graph_featutre, mask = to_dense_batch(graph_featutre, batch_index)\n",
    "        global_featutre = self.global_root(x, mask)\n",
    "        x = torch.cat([global_featutre, graph_featutre],2)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class NMatrixAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 4,\n",
    "        dropout = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        #null_k and null_v parameters serve as learnable \"null\" key and value vectors.\n",
    "        #provids a default key and value for each attention head \n",
    "        #when there is no connection between two nodes or when adjacency information is missing.\n",
    "        #By including these null keys and values, the attention mechanism can learn to assign a\n",
    "        #ppropriate importance to the null entries in the adjacency matrix, effectively allowing the model to learn \n",
    "        #how to handle situations where neighborhood information is incomplete or scarce.\n",
    "        self.null_k = nn.Parameter(torch.randn(heads, dim_head))\n",
    "        self.null_v = nn.Parameter(torch.randn(heads, dim_head))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        adj_kv_indices,\n",
    "        mask\n",
    "    ):\n",
    "        b, n, d, h = *x.shape, self.heads\n",
    "        flat_indices = repeat(adj_kv_indices, 'b n a -> (b h) (n a)', h = h)\n",
    "        #splits the input tensor into query q, key k, and value v tensors using the to_qkv linear laye\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        #rearranges q, k, and v tensors to have separate head dimensions.\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        #batched_index_select to select the corresponding k and v tensors based on the adjacency indices\n",
    "        k, v = map(lambda t: rearrange(t, 'b h n d -> (b h) n d'), (k, v))\n",
    "        k = batched_index_select(k, flat_indices)\n",
    "        v = batched_index_select(v, flat_indices)\n",
    "        k, v = map(lambda t: rearrange(t, '(b h) (n a) d -> b h n a d', h = h, n = n), (k, v))\n",
    "\n",
    "        nk, nv = map(lambda t: rearrange(t, 'h d -> () h () () d').expand(b, -1, n, 1, -1), (self.null_k, self.null_v))\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "        mask = F.pad(mask, (1, 0), value = 1)\n",
    "        #calculate the similarity scores between queries and keys, scales them, and applies the mask.\n",
    "        sim = einsum('b h n d, b h n a d -> b h n a', q, k) * self.scale\n",
    "\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "        mask = rearrange(mask.bool(), 'b n a -> b () n a')\n",
    "        sim.masked_fill_(~mask.bool(), mask_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h n a, b h n a d -> b h n d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class LocalAttenNetwok(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_neighbors_cutoff = None,\n",
    "        attn_dropout = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_neighbors_cutoff = num_neighbors_cutoff\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = None\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, NMatrixAttention(\n",
    "                    dim = dim,\n",
    "                    dim_head = dim_head,\n",
    "                    heads = heads,\n",
    "                    dropout = attn_dropout\n",
    "                ))),\n",
    "                global_attn,\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, adjacency_mat, mask = None):\n",
    "        device, n = x.device, x.shape[1]\n",
    "\n",
    "        diag = torch.eye(adjacency_mat.shape[-1], device = device).bool()\n",
    "        adjacency_mat |= diag\n",
    "        if exists(mask):\n",
    "            adjacency_mat &= (mask[:, :, None] * mask[:, None, :])\n",
    "\n",
    "        adj_mat = adjacency_mat.float()\n",
    "        max_neighbors = int(adj_mat.sum(dim = -1).max())\n",
    "\n",
    "        if exists(self.num_neighbors_cutoff) and max_neighbors > self.num_neighbors_cutoff:\n",
    "            noise = torch.empty((n, n), device = device).uniform_(-0.01, 0.01)\n",
    "            adj_mat = adj_mat + noise\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = self.num_neighbors_cutoff)\n",
    "            adj_mask = (adj_mask > 0.5).float()\n",
    "        else:\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = max_neighbors)\n",
    "        for attn, _ in self.layers:\n",
    "            x = attn(\n",
    "                x,\n",
    "                adj_kv_indices = adj_kv_indices,\n",
    "                mask = adj_mask\n",
    "            )\n",
    "\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV11_V2_LOCAL_GLOBAL(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = LocalGlobalAttention(dim = dim_out, depth = 3, num_neighbors_cutoff = 24)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = pos, k=8, batch=batch_index).to(mask.device)\n",
    "        adj_matrix = to_dense_adj(edge_index, batch_index).int()\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, adj_matrix, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "class EncoderWithDirectionReconstructionV11(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = LocalAttenNetwok(dim = dim_out, depth = 3, num_neighbors_cutoff = 24)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = pos, k=8, batch=batch_index).to(mask.device)\n",
    "        adj_matrix = to_dense_adj(edge_index, batch_index).int()\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, adj_matrix, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2663b7-965e-4f1b-b95f-8f4e05e61b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = EncoderWithDirectionReconstructionV18()\n",
    "md.load_state_dict(torch.load('/opt/slh/icecube/hb_training_loop/V18FT/models/model_6.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae356784-de06-4466-b51a-c657b1cdf739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderWithDirectionReconstructionV11(\n",
       "  (fe): ExtractorV0(\n",
       "    (emb): SinusoidalPosEmb()\n",
       "    (emb2): SinusoidalPosEmb()\n",
       "    (aux_emb): Embedding(2, 48)\n",
       "    (qe_emb): Embedding(2, 48)\n",
       "    (proj): Linear(in_features=672, out_features=320, bias=True)\n",
       "  )\n",
       "  (encoder): BeDeepIceModel(\n",
       "    (Beblocks): ModuleList(\n",
       "      (0): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BeBlock(\n",
       "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BeMLP(\n",
       "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          (act): GELU()\n",
       "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proj_out): Linear(in_features=320, out_features=3, bias=True)\n",
       "  )\n",
       "  (cls_token): Linear(in_features=320, out_features=1, bias=False)\n",
       "  (loacl_attn): LocalAttenNetwok(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): NMatrixAttention(\n",
       "              (to_qkv): Linear(in_features=320, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): NMatrixAttention(\n",
       "              (to_qkv): Linear(in_features=320, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Residual(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): NMatrixAttention(\n",
       "              (to_qkv): Linear(in_features=320, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): None\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EncoderWithDirectionReconstructionV11_V2_LOCAL_GLOBAL()\n",
    "EncoderWithDirectionReconstructionV11()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040b2691-f7cf-4758-8343-85bddf058869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ee77c5-318c-435f-bfbf-2cbe30a52148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polars.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd941a7f-7c8f-4960-a257-6191c89a2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c7bb57-3cc2-4496-a9cc-3207c4e9e486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyarrow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2567864e-bc12-4037-bf8e-631ef3e4f820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
