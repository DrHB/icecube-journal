{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp modelsgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/archive/software/graphnet/src')\n",
    "\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss\n",
    "from graphnet.training.labels import Direction\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SchNet, global_add_pool, global_mean_pool\n",
    "import torch_scatter\n",
    "from torch_scatter import scatter\n",
    "from torch.nn import Linear, ReLU, SiLU, Sequential\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.loader import DataLoader as gDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/opt/slh/icecube')\n",
    "from icecube.graphdataset import GraphDasetV0\n",
    "from datasets import  load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EGNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"E(n) Equivariant GNN Layer\n",
    "        Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim + 1, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\psi_x` for computing messages `\\overrightarrow{m}_ij`\n",
    "        self.mlp_pos = Sequential(\n",
    "            Linear(emb_dim, emb_dim), self.norm(emb_dim), self.activation, Linear(emb_dim, 1)\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, pos_i, pos_j):\n",
    "        # Compute messages\n",
    "        pos_diff = pos_i - pos_j\n",
    "        dists = torch.norm(pos_diff, dim=-1).unsqueeze(1)\n",
    "        msg = torch.cat([h_i, h_j, dists], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        # Scale magnitude of displacement vector\n",
    "        pos_diff = pos_diff * self.mlp_pos(msg)  # torch.clamp(updates, min=-100, max=100)\n",
    "        return msg, pos_diff\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        msgs, pos_diffs = inputs\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(msgs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        pos_aggr = scatter(pos_diffs, index, dim=self.node_dim, reduce=\"mean\")\n",
    "        return msg_aggr, pos_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, pos):\n",
    "        msg_aggr, pos_aggr = aggr_out\n",
    "        upd_out = self.mlp_upd(torch.cat([h, msg_aggr], dim=-1))\n",
    "        upd_pos = pos + pos_aggr\n",
    "        return upd_out, upd_pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\"\n",
    "    \n",
    "    \n",
    "class EGNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=9,\n",
    "        out_dim=4,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ds = GraphDasetV0(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = gDataLoader(ds, batch_size=64, shuffle=True)\n",
    "x = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = EGNNModel().eval()\n",
    "with torch.no_grad():\n",
    "    out = md(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3024, -0.2626, -0.9163],\n",
       "        [ 0.5685, -0.7906, -0.2276],\n",
       "        [-0.5923,  0.4075,  0.6950],\n",
       "        [ 0.9434, -0.0632,  0.3256],\n",
       "        [-0.3884,  0.4468,  0.8059],\n",
       "        [ 0.9331,  0.3209, -0.1622],\n",
       "        [ 0.2757,  0.1921,  0.9419],\n",
       "        [-0.2261,  0.7435, -0.6293],\n",
       "        [-0.1910, -0.5903,  0.7843],\n",
       "        [ 0.4528, -0.7029,  0.5486],\n",
       "        [ 0.3039, -0.9135,  0.2705],\n",
       "        [ 0.5533, -0.2467,  0.7956],\n",
       "        [ 0.0754, -0.3414,  0.9369],\n",
       "        [-0.4229,  0.8572,  0.2937],\n",
       "        [ 0.8806,  0.1395, -0.4529],\n",
       "        [-0.1885,  0.7806,  0.5959],\n",
       "        [-0.1094, -0.1582,  0.9813],\n",
       "        [ 0.9775, -0.0162,  0.2105],\n",
       "        [ 0.9373, -0.3399,  0.0770],\n",
       "        [-0.2167, -0.2301, -0.9487],\n",
       "        [-0.9903, -0.1197,  0.0710],\n",
       "        [-0.5924,  0.6438,  0.4843],\n",
       "        [-0.0430,  0.8164, -0.5759],\n",
       "        [-0.6117, -0.2344,  0.7556],\n",
       "        [-0.3599, -0.5484, -0.7548],\n",
       "        [-0.9938, -0.1021,  0.0447],\n",
       "        [-0.6852, -0.7262, -0.0564],\n",
       "        [-0.1665, -0.5610, -0.8109],\n",
       "        [-0.3599, -0.5188,  0.7754],\n",
       "        [ 0.3571,  0.8752,  0.3264],\n",
       "        [-0.2222, -0.4936,  0.8408],\n",
       "        [-0.6772, -0.7303, -0.0900],\n",
       "        [-0.0662, -0.8334,  0.5488],\n",
       "        [-0.1241, -0.1340, -0.9832],\n",
       "        [ 0.1238,  0.9761,  0.1784],\n",
       "        [-0.0657, -0.9575, -0.2809],\n",
       "        [-0.2678, -0.3732, -0.8883],\n",
       "        [-0.2588, -0.8703,  0.4190],\n",
       "        [-0.8599,  0.0407,  0.5088],\n",
       "        [-0.1266, -0.2623, -0.9567],\n",
       "        [ 0.2896,  0.9193, -0.2666],\n",
       "        [-0.9350,  0.3075, -0.1768],\n",
       "        [ 0.1508, -0.7375,  0.6583],\n",
       "        [-0.0237, -0.9589, -0.2828],\n",
       "        [-0.6314, -0.5815, -0.5130],\n",
       "        [ 0.6379,  0.5536, -0.5354],\n",
       "        [-0.4344,  0.8981, -0.0687],\n",
       "        [-0.5013,  0.8492,  0.1658],\n",
       "        [ 0.8761,  0.0849, -0.4745],\n",
       "        [-0.3364,  0.1972,  0.9209],\n",
       "        [-0.9879,  0.1497, -0.0408],\n",
       "        [-0.8106,  0.1995, -0.5505],\n",
       "        [ 0.3819, -0.9226,  0.0545],\n",
       "        [-0.6762, -0.1183, -0.7272],\n",
       "        [ 0.8663, -0.4501,  0.2167],\n",
       "        [-0.7893, -0.6059,  0.0991],\n",
       "        [-0.9884, -0.1513, -0.0146],\n",
       "        [ 0.5910, -0.7952,  0.1356],\n",
       "        [-0.0105,  0.5658, -0.8245],\n",
       "        [ 0.2049, -0.9713,  0.1209],\n",
       "        [-0.3378, -0.2993, -0.8924],\n",
       "        [-0.2981, -0.8979, -0.3241],\n",
       "        [ 0.2370, -0.8438, -0.4815],\n",
       "        [-0.6408, -0.4947, -0.5870]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.y.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
