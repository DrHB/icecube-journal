{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datasets import  load_from_disk\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "import seaborn as sns\n",
    "class CFG:\n",
    "    CACHE_PATH = Path('../data/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = list((CFG.CACHE_PATH/'batch_3').glob('*.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# function that loads the data from the pth file and return the data and the label as pd.DataFrame\n",
    "def load_data(\n",
    "    fn: Path,\n",
    "    columns_event: str = [\"time\", \"charge\", \"auxiliary\", \"x\", \"y\", \"z\"],\n",
    "    columns_label: str = [\"azimuth\", \"zenith\"],\n",
    "    keep_auxiliary_event: bool = False,\n",
    "):\n",
    "    data = torch.load(fn)\n",
    "    event = pd.DataFrame.from_records(data[\"event\"])[columns_event]\n",
    "    if keep_auxiliary_event:\n",
    "        event = event.query(\"auxiliary == True\")\n",
    "    label = pd.DataFrame.from_records(data[\"target\"])[columns_label]\n",
    "    return event.astype(np.float32), label\n",
    "\n",
    "\n",
    "class IceCubeCasheDatasetV0(Dataset):\n",
    "    def __init__(self, fns, max_events=100):\n",
    "        self.fns = fns\n",
    "        self.max_events = max_events\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        event, label = load_data(fn)\n",
    "\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = torch.tensor(event.values)\n",
    "        mask = torch.ones(len(event), dtype=torch.bool)\n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "\n",
    "        return {\"event\": event, \"mask\": mask, \"label\": label}\n",
    "\n",
    "\n",
    "class IceCubeCasheDatasetV1(Dataset):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): Same as IceCubeCasheDatasetV0 but with the option to keep the auxiliary events\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fns, max_events=100, keep_auxiliary_event: bool = True):\n",
    "        self.fns = fns\n",
    "        self.max_events = max_events\n",
    "        self.keep_auxiliary_event = keep_auxiliary_event\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        event, label = load_data(fn, keep_auxiliary_event=self.keep_auxiliary_event)\n",
    "\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = torch.tensor(event.values)\n",
    "        mask = torch.ones(len(event), dtype=torch.bool)\n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "\n",
    "        return {\"event\": event, \"mask\": mask, \"label\": label}\n",
    "\n",
    "\n",
    "# collate_fn that pads the event and mask to the max length in the batch using pythorch pad_sequence\n",
    "class HuggingFaceDatasetV0(Dataset):\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = event.values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "# function to normalize input between 1 and 0\n",
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV1(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        # feature engineering\n",
    "        event[\"w1\"] = event[\"charge\"] * event[\"time\"]\n",
    "        event[\"w0\"] = event[\"charge\"] - event[\"w1\"]\n",
    "\n",
    "        event[\"wx0\"] = event.x * event.w0\n",
    "        event[\"wy0\"] = event.y * event.w0\n",
    "        event[\"wz0\"] = event.z * event.w0\n",
    "        event[\"wx1\"] = event.x * event.w1\n",
    "        event[\"wy1\"] = event.y * event.w1\n",
    "        event[\"wz1\"] = event.z * event.w1\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"w1\",\n",
    "                \"w0\",\n",
    "                \"wx0\",\n",
    "                \"wy0\",\n",
    "                \"wz0\",\n",
    "                \"wx1\",\n",
    "                \"wy1\",\n",
    "                \"wz1\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV2(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV3(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "class HuggingFaceDatasetV4(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=160):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "def event_filtering_v1(batch, max_pulse_count=128, t_valid_length=6199.700247193777):\n",
    "    col = batch.columns\n",
    "    t_peak = batch[\"time\"][batch[\"charge\"].argmax()]\n",
    "    t_valid_min = t_peak - t_valid_length\n",
    "    t_valid_max = t_peak + t_valid_length\n",
    "    t_valid = (batch[\"time\"] > t_valid_min) * (batch[\"time\"] < t_valid_max)\n",
    "    batch[\"rank\"] = 2 * (1 - batch[\"auxiliary\"]) + (t_valid)\n",
    "    batch = batch.sort_values(by=[\"rank\", \"charge\"])\n",
    "    # pick-up from backward\n",
    "    batch = batch[-max_pulse_count:]\n",
    "        # resort by time\n",
    "    batch = batch.sort_values(by=\"time\")\n",
    "    return batch[col]\n",
    "        \n",
    "\n",
    "class HuggingFaceDatasetV5(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        \n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = IceCubeCasheDatasetV1(fns)\n",
    "#dl = DataLoader(ds, batch_size=64, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "# for x in dl:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ds = HuggingFaceDatasetV5(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ice_transparency(\n",
    "    data_path=\"/opt/slh/icecube/data/ice_transparency.txt\", datum=1950\n",
    "):\n",
    "    # Data from page 31 of https://arxiv.org/pdf/1301.5361.pdf\n",
    "    # Datum is from footnote 8 of page 29\n",
    "    df = pd.read_csv(data_path, delim_whitespace=True)\n",
    "    df[\"z\"] = df[\"depth\"] - datum\n",
    "    df[\"z_norm\"] = df[\"z\"] / 500\n",
    "    df[[\"scattering_len_norm\", \"absorption_len_norm\"]] = RobustScaler().fit_transform(\n",
    "        df[[\"scattering_len\", \"absorption_len\"]]\n",
    "    )\n",
    "\n",
    "    # These are both roughly equivalent after scaling\n",
    "    f_scattering = interp1d(df[\"z_norm\"], df[\"scattering_len_norm\"])\n",
    "    f_absorption = interp1d(df[\"z_norm\"], df[\"absorption_len_norm\"])\n",
    "    return f_scattering, f_absorption\n",
    "\n",
    "def prepare_sensors():\n",
    "    sensors = pd.read_csv('/opt/slh/icecube/data/sensor_geometry.csv').astype(\n",
    "        {\n",
    "            \"sensor_id\": np.int16,\n",
    "            \"x\": np.float32,\n",
    "            \"y\": np.float32,\n",
    "            \"z\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    sensors[\"string\"] = 0\n",
    "    sensors[\"qe\"] = 1\n",
    "\n",
    "    for i in range(len(sensors) // 60):\n",
    "        start, end = i * 60, (i * 60) + 60\n",
    "        sensors.loc[start:end, \"string\"] = i\n",
    "\n",
    "        # High Quantum Efficiency in the lower 50 DOMs - https://arxiv.org/pdf/2209.03042.pdf (Figure 1)\n",
    "        if i in range(78, 86):\n",
    "            start_veto, end_veto = i * 60, (i * 60) + 10\n",
    "            start_core, end_core = end_veto + 1, (i * 60) + 60\n",
    "            sensors.loc[start_core:end_core, \"qe\"] = 1.35\n",
    "\n",
    "    # https://github.com/graphnet-team/graphnet/blob/b2bad25528652587ab0cdb7cf2335ee254cfa2db/src/graphnet/models/detector/icecube.py#L33-L41\n",
    "    # Assume that \"rde\" (relative dom efficiency) is equivalent to QE\n",
    "    sensors[\"x\"] /= 500\n",
    "    sensors[\"y\"] /= 500\n",
    "    sensors[\"z\"] /= 500\n",
    "    sensors[\"qe\"] -= 1.25\n",
    "    sensors[\"qe\"] /= 0.25\n",
    "\n",
    "    return sensors.set_index(\"sensor_id\")[['qe']]\n",
    "\n",
    "\n",
    "def convert_to_3d(azimuth, zenith):\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    x = np.cos(azimuth) * np.sin(zenith)\n",
    "    y = np.sin(azimuth) * np.sin(zenith)\n",
    "    z = np.cos(zenith)\n",
    "    return np.array([x, y, z], dtype=np.float32)\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV6(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV7(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV8(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=148):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV9(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.1339, -0.0466,  0.5000, -0.0865, -0.5350,  0.1833, -1.0000, -0.9956])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = HuggingFaceDatasetV8(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "ds[678]['event'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ds[0]['event'][0]\n",
    "x = ds[0]['event'][3]\n",
    "y = ds[0]['event'][4]\n",
    "z = ds[0]['event'][5]\n",
    "chage = ds[0]['event'][1]\n",
    "magnitude = torch.sqrt(x ** 2 + y ** 2 + z ** 2)\n",
    "distance = magnitude / 3 * 10 ** 8\n",
    "time_of_flight = time + distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]['event'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# pytorch function that takes [n, x, y, z] tensor and calculates the distance between each point and returns [n x n] matrix using torch.cdist\n",
    "def get_distance_matrix(xyz):\n",
    "    return torch.cdist(xyz, xyz)\n",
    "\n",
    "\n",
    "def get_distance_matrix_for_indices(dm, indices):\n",
    "    return dm[indices][:, indices]\n",
    "\n",
    "\n",
    "def get_distance_matrix_from_csv(\n",
    "    path_to_geom=\"/opt/slh/icecube/data/sensor_geometry.csv\",\n",
    "):\n",
    "    geom = pd.read_csv(path_to_geom)[[\"x\", \"y\", \"z\"]]\n",
    "    geom = torch.tensor(geom.values, dtype=torch.float32)\n",
    "    geom = get_distance_matrix(geom)\n",
    "    # nromalize goematry matrix\n",
    "    geom = geom / geom.max()\n",
    "    return geom\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetGraphV0(Dataset):\n",
    "    def __init__(self, ds, min_adj_distance=0.015, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "        self.mad = min_adj_distance\n",
    "        self.distance_matrix_ = get_distance_matrix_from_csv()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        # getting distance matrix for event\n",
    "        distance_matrix = get_distance_matrix_for_indices(\n",
    "            self.distance_matrix_, event[\"sensor_id\"].values\n",
    "        )\n",
    "\n",
    "        dmx = torch.zeros((self.max_events, self.max_events), dtype=torch.float32)\n",
    "        dmx[: distance_matrix.shape[0], : distance_matrix.shape[1]] = distance_matrix\n",
    "        adjecent_matrix = (dmx < self.mad).type(torch.float32)\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"distance_matrix\": dmx,\n",
    "                \"adjecent_matrix\": adjecent_matrix,\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetGraphV1(Dataset):\n",
    "    def __init__(self, ds, min_adj_distance=0.05, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.mad = min_adj_distance\n",
    "        self.distance_matrix_ = get_distance_matrix_from_csv()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = (event['time'] - 1.0e04) / 3.0e4\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event['x'] /=500\n",
    "        event['y'] /=500\n",
    "        event['z'] /=500\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])/3.0\n",
    "\n",
    "        # getting distance matrix for event\n",
    "        distance_matrix = get_distance_matrix_for_indices(\n",
    "            self.distance_matrix_, event[\"sensor_id\"].values\n",
    "        )\n",
    "\n",
    "        dmx = torch.zeros((self.max_events, self.max_events), dtype=torch.float32)\n",
    "        dmx[: distance_matrix.shape[0], : distance_matrix.shape[1]] = distance_matrix\n",
    "        adjecent_matrix = (dmx < self.mad).type(torch.float32)\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"time\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"distance_matrix\": dmx,\n",
    "                \"adjecent_matrix\": adjecent_matrix,\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = HuggingFaceDatasetGraphV1(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "sns.heatmap(ds[np.random.randint(0, len(ds))]['adjecent_matrix'].numpy())\n",
    "#dl = DataLoader(ds, batch_size=64, shuffle=True, num_workers=4, collate_fn=collate_fn_graphv0)\n",
    "#for x in dl:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def good_luck():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class IceCubeKaggle():\n",
    "#     \"\"\"`Detector` class for Kaggle Competition.\"\"\"\n",
    "\n",
    "#     # Implementing abstract class attribute\n",
    "\n",
    "#     def _forward(self, data: Data) -> Data:\n",
    "#         \"\"\"Ingest data, build graph, and preprocess features.\n",
    "#         Args:\n",
    "#             data: Input graph data.\n",
    "#         Returns:\n",
    "#             Connected and preprocessed graph data.\n",
    "#         \"\"\"\n",
    "#         # Check(s)\n",
    "#         self._validate_features(data)\n",
    "\n",
    "#         # Preprocessing\n",
    "#         data.x[:, 0] /= 500.0  # x\n",
    "#         data.x[:, 1] /= 500.0  # y\n",
    "#         data.x[:, 2] /= 500.0  # z\n",
    "#         data.x[:, 3] = (data.x[:, 3] - 1.0e04) / 3.0e4  # time\n",
    "#         data.x[:, 4] = torch.log10(data.x[:, 4]) / 3.0  # charge\n",
    "\n",
    "#         return data\n",
    "\n",
    "# class Direction(Label):\n",
    "#     \"\"\"Class for producing particle direction/pointing label.\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, azimuth_key: str = \"azimuth\", zenith_key: str = \"zenith\"\n",
    "#     ):\n",
    "#         \"\"\"Construct `Direction`.\"\"\"\n",
    "#         self._azimuth_key = azimuth_key\n",
    "#         self._zenith_key = zenith_key\n",
    "\n",
    "#     def __call__(self, graph: Data) -> torch.tensor:\n",
    "#         \"\"\"Compute label for `graph`.\"\"\"\n",
    "#         x = torch.cos(graph[self._azimuth_key]) * torch.sin(\n",
    "#             graph[self._zenith_key]\n",
    "#         ).reshape(-1, 1)\n",
    "#         y = torch.sin(graph[self._azimuth_key]) * torch.sin(\n",
    "#             graph[self._zenith_key]\n",
    "#         ).reshape(-1, 1)\n",
    "#         z = torch.cos(graph[self._zenith_key]).reshape(-1, 1)\n",
    "#         return torch.cat((x, y, z), dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
