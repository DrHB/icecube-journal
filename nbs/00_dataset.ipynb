{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datasets import  load_from_disk\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "import seaborn as sns\n",
    "class CFG:\n",
    "    CACHE_PATH = Path('../data/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = list((CFG.CACHE_PATH/'batch_3').glob('*.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# function that loads the data from the pth file and return the data and the label as pd.DataFrame\n",
    "def load_data(\n",
    "    fn: Path,\n",
    "    columns_event: str = [\"time\", \"charge\", \"auxiliary\", \"x\", \"y\", \"z\"],\n",
    "    columns_label: str = [\"azimuth\", \"zenith\"],\n",
    "    keep_auxiliary_event: bool = False,\n",
    "):\n",
    "    data = torch.load(fn)\n",
    "    event = pd.DataFrame.from_records(data[\"event\"])[columns_event]\n",
    "    if keep_auxiliary_event:\n",
    "        event = event.query(\"auxiliary == True\")\n",
    "    label = pd.DataFrame.from_records(data[\"target\"])[columns_label]\n",
    "    return event.astype(np.float32), label\n",
    "\n",
    "\n",
    "class IceCubeCasheDatasetV0(Dataset):\n",
    "    def __init__(self, fns, max_events=100):\n",
    "        self.fns = fns\n",
    "        self.max_events = max_events\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        event, label = load_data(fn)\n",
    "\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = torch.tensor(event.values)\n",
    "        mask = torch.ones(len(event), dtype=torch.bool)\n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "\n",
    "        return {\"event\": event, \"mask\": mask, \"label\": label}\n",
    "\n",
    "\n",
    "class IceCubeCasheDatasetV1(Dataset):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        Dataset (_type_): Same as IceCubeCasheDatasetV0 but with the option to keep the auxiliary events\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fns, max_events=100, keep_auxiliary_event: bool = True):\n",
    "        self.fns = fns\n",
    "        self.max_events = max_events\n",
    "        self.keep_auxiliary_event = keep_auxiliary_event\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.fns[idx]\n",
    "        event, label = load_data(fn, keep_auxiliary_event=self.keep_auxiliary_event)\n",
    "\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = torch.tensor(event.values)\n",
    "        mask = torch.ones(len(event), dtype=torch.bool)\n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "\n",
    "        return {\"event\": event, \"mask\": mask, \"label\": label}\n",
    "\n",
    "\n",
    "# collate_fn that pads the event and mask to the max length in the batch using pythorch pad_sequence\n",
    "class HuggingFaceDatasetV0(Dataset):\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = event.values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "# function to normalize input between 1 and 0\n",
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV1(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        # feature engineering\n",
    "        event[\"w1\"] = event[\"charge\"] * event[\"time\"]\n",
    "        event[\"w0\"] = event[\"charge\"] - event[\"w1\"]\n",
    "\n",
    "        event[\"wx0\"] = event.x * event.w0\n",
    "        event[\"wy0\"] = event.y * event.w0\n",
    "        event[\"wz0\"] = event.z * event.w0\n",
    "        event[\"wx1\"] = event.x * event.w1\n",
    "        event[\"wy1\"] = event.y * event.w1\n",
    "        event[\"wz1\"] = event.z * event.w1\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"w1\",\n",
    "                \"w0\",\n",
    "                \"wx0\",\n",
    "                \"wy0\",\n",
    "                \"wz0\",\n",
    "                \"wx1\",\n",
    "                \"wy1\",\n",
    "                \"wz1\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV2(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV3(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "class HuggingFaceDatasetV4(Dataset):\n",
    "    \"\"\"\n",
    "    Same as HuggingFaceDatasetV0 but returns sensor_id as well\n",
    "    in addition it adds + 1 to make the sensor_id start from 1 instead of 0,\n",
    "    0 is ignore index\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=160):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        # this is done in order to shift sensor id from 0 to 1\n",
    "        # since paddding index is 0\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "def event_filtering_v1(batch, max_pulse_count=128, t_valid_length=6199.700247193777):\n",
    "    col = batch.columns\n",
    "    t_peak = batch[\"time\"][batch[\"charge\"].argmax()]\n",
    "    t_valid_min = t_peak - t_valid_length\n",
    "    t_valid_max = t_peak + t_valid_length\n",
    "    t_valid = (batch[\"time\"] > t_valid_min) * (batch[\"time\"] < t_valid_max)\n",
    "    batch[\"rank\"] = 2 * (1 - batch[\"auxiliary\"]) + (t_valid)\n",
    "    batch = batch.sort_values(by=[\"rank\", \"charge\"])\n",
    "    # pick-up from backward\n",
    "    batch = batch[-max_pulse_count:]\n",
    "        # resort by time\n",
    "    batch = batch.sort_values(by=\"time\")\n",
    "    return batch[col]\n",
    "        \n",
    "\n",
    "class HuggingFaceDatasetV5(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "        \n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = IceCubeCasheDatasetV1(fns)\n",
    "#dl = DataLoader(ds, batch_size=64, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "# for x in dl:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ds = HuggingFaceDatasetV5(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ice_transparency(\n",
    "    data_path=\"/opt/slh/icecube/data/ice_transparency.txt\", datum=1950\n",
    "):\n",
    "    # Data from page 31 of https://arxiv.org/pdf/1301.5361.pdf\n",
    "    # Datum is from footnote 8 of page 29\n",
    "    df = pd.read_csv(data_path, delim_whitespace=True)\n",
    "    df[\"z\"] = df[\"depth\"] - datum\n",
    "    df[\"z_norm\"] = df[\"z\"] / 500\n",
    "    df[[\"scattering_len_norm\", \"absorption_len_norm\"]] = RobustScaler().fit_transform(\n",
    "        df[[\"scattering_len\", \"absorption_len\"]]\n",
    "    )\n",
    "\n",
    "    # These are both roughly equivalent after scaling\n",
    "    f_scattering = interp1d(df[\"z_norm\"], df[\"scattering_len_norm\"])\n",
    "    f_absorption = interp1d(df[\"z_norm\"], df[\"absorption_len_norm\"])\n",
    "    return f_scattering, f_absorption\n",
    "\n",
    "def prepare_sensors():\n",
    "    sensors = pd.read_csv('/opt/slh/icecube/data/sensor_geometry.csv').astype(\n",
    "        {\n",
    "            \"sensor_id\": np.int16,\n",
    "            \"x\": np.float32,\n",
    "            \"y\": np.float32,\n",
    "            \"z\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    sensors[\"string\"] = 0\n",
    "    sensors[\"qe\"] = 1\n",
    "\n",
    "    for i in range(len(sensors) // 60):\n",
    "        start, end = i * 60, (i * 60) + 60\n",
    "        sensors.loc[start:end, \"string\"] = i\n",
    "\n",
    "        # High Quantum Efficiency in the lower 50 DOMs - https://arxiv.org/pdf/2209.03042.pdf (Figure 1)\n",
    "        if i in range(78, 86):\n",
    "            start_veto, end_veto = i * 60, (i * 60) + 10\n",
    "            start_core, end_core = end_veto + 1, (i * 60) + 60\n",
    "            sensors.loc[start_core:end_core, \"qe\"] = 1.35\n",
    "\n",
    "    # https://github.com/graphnet-team/graphnet/blob/b2bad25528652587ab0cdb7cf2335ee254cfa2db/src/graphnet/models/detector/icecube.py#L33-L41\n",
    "    # Assume that \"rde\" (relative dom efficiency) is equivalent to QE\n",
    "    sensors[\"x\"] /= 500\n",
    "    sensors[\"y\"] /= 500\n",
    "    sensors[\"z\"] /= 500\n",
    "    sensors[\"qe\"] -= 1.25\n",
    "    sensors[\"qe\"] /= 0.25\n",
    "\n",
    "    return sensors.set_index(\"sensor_id\")[['qe']]\n",
    "\n",
    "\n",
    "def convert_to_3d(azimuth, zenith):\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    x = np.cos(azimuth) * np.sin(zenith)\n",
    "    y = np.sin(azimuth) * np.sin(zenith)\n",
    "    z = np.cos(zenith)\n",
    "    return np.array([x, y, z], dtype=np.float32)\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV6(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV7(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV8(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=148):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetV9(Dataset):\n",
    "    \"\"\"\n",
    "    dataset with event filtering up to 128\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "    \n",
    "class HuggingFaceDatasetV10(Dataset):\n",
    "    \"\"\"\n",
    "    same as V9 but with 148 \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=148):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "    \n",
    "class HuggingFaceDatasetV11(Dataset):\n",
    "    \"\"\"\n",
    "    same as V9 but with added sensoor ids \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=128):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "class HuggingFaceDatasetV12(Dataset):\n",
    "    \"\"\"\n",
    "    same as V9 but with added sensoor ids , same as V11 but with 160 `max_len`\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=160):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "    \n",
    "    \n",
    "class HuggingFaceDatasetV13(Dataset):\n",
    "    \"\"\"\n",
    "    same as V9 but with added sensoor ids , same as V11 but with 196 `max_len`\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds, max_events=196):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        sensor_id = event[\"sensor_id\"].values + 1\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = convert_to_3d(item[\"azimuth\"], item[\"zenith\"])\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"sensor_id\": torch.tensor(sensor_id, dtype=torch.int32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1289, -0.0369,  0.5000,  0.0237,  0.3584, -0.2548, -1.0000,  0.2957],\n",
       "        [-0.1266, -0.0037,  0.5000,  0.0832,  0.0710, -0.5961,  0.4000,  0.2331],\n",
       "        [-0.0982,  0.0233,  0.5000,  1.0105,  0.5158,  0.0253, -1.0000, -0.4893],\n",
       "        [-0.0864, -0.0801,  0.5000,  0.2264, -0.1209, -0.4764,  0.4000, -0.3048],\n",
       "        [-0.0747, -0.0466,  0.5000,  1.0105,  0.5158, -0.1790, -1.0000, -0.4543]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = HuggingFaceDatasetV8(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "ds[100]['event'][:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ds[0]['event'][0]\n",
    "x = ds[0]['event'][3]\n",
    "y = ds[0]['event'][4]\n",
    "z = ds[0]['event'][5]\n",
    "chage = ds[0]['event'][1]\n",
    "magnitude = torch.sqrt(x ** 2 + y ** 2 + z ** 2)\n",
    "distance = magnitude / 3 * 10 ** 8\n",
    "time_of_flight = time + distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['event'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "# pytorch function that takes [n, x, y, z] tensor and calculates the distance between each point and returns [n x n] matrix using torch.cdist\n",
    "def get_distance_matrix(xyz):\n",
    "    return torch.cdist(xyz, xyz)\n",
    "\n",
    "\n",
    "def get_distance_matrix_for_indices(dm, indices):\n",
    "    return dm[indices][:, indices]\n",
    "\n",
    "\n",
    "def get_distance_matrix_from_csv(\n",
    "    path_to_geom=\"/opt/slh/icecube/data/sensor_geometry.csv\",\n",
    "):\n",
    "    geom = pd.read_csv(path_to_geom)[[\"x\", \"y\", \"z\"]]\n",
    "    geom = torch.tensor(geom.values, dtype=torch.float32)\n",
    "    geom = get_distance_matrix(geom)\n",
    "    # nromalize goematry matrix\n",
    "    geom = geom / geom.max()\n",
    "    return geom\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetGraphV0(Dataset):\n",
    "    def __init__(self, ds, min_adj_distance=0.015, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.geom_max = np.array([576.37, 509.5, 524.56])\n",
    "        self.geom_min = np.array([[-570.9, -521.08, -512.82]])\n",
    "        self.mad = min_adj_distance\n",
    "        self.distance_matrix_ = get_distance_matrix_from_csv()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = 1 - normalize(event[\"time\"])\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event[[\"x\", \"y\", \"z\"]] = (event[[\"x\", \"y\", \"z\"]].values - self.geom_min) / (\n",
    "            self.geom_max - self.geom_min\n",
    "        )\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])\n",
    "\n",
    "        # getting distance matrix for event\n",
    "        distance_matrix = get_distance_matrix_for_indices(\n",
    "            self.distance_matrix_, event[\"sensor_id\"].values\n",
    "        )\n",
    "\n",
    "        dmx = torch.zeros((self.max_events, self.max_events), dtype=torch.float32)\n",
    "        dmx[: distance_matrix.shape[0], : distance_matrix.shape[1]] = distance_matrix\n",
    "        adjecent_matrix = (dmx < self.mad).type(torch.float32)\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"distance_matrix\": dmx,\n",
    "                \"adjecent_matrix\": adjecent_matrix,\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "class HuggingFaceDatasetGraphV1(Dataset):\n",
    "    def __init__(self, ds, min_adj_distance=0.05, max_events=100):\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.mad = min_adj_distance\n",
    "        self.distance_matrix_ = get_distance_matrix_from_csv()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"sensor_id\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        if self.max_events:\n",
    "            event = event[: self.max_events]\n",
    "        # in this way the time start at 0 and end at 1\n",
    "        event[\"time\"] = (event['time'] - 1.0e04) / 3.0e4\n",
    "\n",
    "        # normalize the x,y,z coordinates of geomatry\n",
    "        # TO DO add this in to preprocessing\n",
    "        event['x'] /=500\n",
    "        event['y'] /=500\n",
    "        event['z'] /=500\n",
    "\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"])/3.0\n",
    "\n",
    "        # getting distance matrix for event\n",
    "        distance_matrix = get_distance_matrix_for_indices(\n",
    "            self.distance_matrix_, event[\"sensor_id\"].values\n",
    "        )\n",
    "\n",
    "        dmx = torch.zeros((self.max_events, self.max_events), dtype=torch.float32)\n",
    "        dmx[: distance_matrix.shape[0], : distance_matrix.shape[1]] = distance_matrix\n",
    "        adjecent_matrix = (dmx < self.mad).type(torch.float32)\n",
    "\n",
    "        event = event[\n",
    "            [\n",
    "\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"time\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "            ]\n",
    "        ].values\n",
    "        mask = np.ones(len(event), dtype=bool)\n",
    "        label = np.array([item[\"azimuth\"], item[\"zenith\"]], dtype=np.float32)\n",
    "\n",
    "        batch = deepcopy(\n",
    "            {\n",
    "                \"distance_matrix\": dmx,\n",
    "                \"adjecent_matrix\": adjecent_matrix,\n",
    "                \"event\": torch.tensor(event, dtype=torch.float32),\n",
    "                \"mask\": torch.tensor(mask),\n",
    "                \"label\": torch.tensor(label),\n",
    "            }\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = HuggingFaceDatasetGraphV1(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "sns.heatmap(ds[np.random.randint(0, len(ds))]['adjecent_matrix'].numpy())\n",
    "#dl = DataLoader(ds, batch_size=64, shuffle=True, num_workers=4, collate_fn=collate_fn_graphv0)\n",
    "#for x in dl:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def good_luck():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class IceCubeKaggle():\n",
    "#     \"\"\"`Detector` class for Kaggle Competition.\"\"\"\n",
    "\n",
    "#     # Implementing abstract class attribute\n",
    "\n",
    "#     def _forward(self, data: Data) -> Data:\n",
    "#         \"\"\"Ingest data, build graph, and preprocess features.\n",
    "#         Args:\n",
    "#             data: Input graph data.\n",
    "#         Returns:\n",
    "#             Connected and preprocessed graph data.\n",
    "#         \"\"\"\n",
    "#         # Check(s)\n",
    "#         self._validate_features(data)\n",
    "\n",
    "#         # Preprocessing\n",
    "#         data.x[:, 0] /= 500.0  # x\n",
    "#         data.x[:, 1] /= 500.0  # y\n",
    "#         data.x[:, 2] /= 500.0  # z\n",
    "#         data.x[:, 3] = (data.x[:, 3] - 1.0e04) / 3.0e4  # time\n",
    "#         data.x[:, 4] = torch.log10(data.x[:, 4]) / 3.0  # charge\n",
    "\n",
    "#         return data\n",
    "\n",
    "# class Direction(Label):\n",
    "#     \"\"\"Class for producing particle direction/pointing label.\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, azimuth_key: str = \"azimuth\", zenith_key: str = \"zenith\"\n",
    "#     ):\n",
    "#         \"\"\"Construct `Direction`.\"\"\"\n",
    "#         self._azimuth_key = azimuth_key\n",
    "#         self._zenith_key = zenith_key\n",
    "\n",
    "#     def __call__(self, graph: Data) -> torch.tensor:\n",
    "#         \"\"\"Compute label for `graph`.\"\"\"\n",
    "#         x = torch.cos(graph[self._azimuth_key]) * torch.sin(\n",
    "#             graph[self._zenith_key]\n",
    "#         ).reshape(-1, 1)\n",
    "#         y = torch.sin(graph[self._azimuth_key]) * torch.sin(\n",
    "#             graph[self._zenith_key]\n",
    "#         ).reshape(-1, 1)\n",
    "#         z = torch.cos(graph[self._zenith_key]).reshape(-1, 1)\n",
    "#         return torch.cat((x, y, z), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
