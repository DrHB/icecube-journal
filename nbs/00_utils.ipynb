{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from typing import List\n",
    "from accelerate import Accelerator\n",
    "import gc\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "import random\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def label_to_df(label, angle_post_fix=\"\", vec_post_fix=\"\"):\n",
    "    df = pd.DataFrame(label, columns=[\"direction_x\", \"direction_y\", \"direction_z\"])\n",
    "    r = np.sqrt(\n",
    "        df[\"direction_x\" + vec_post_fix] ** 2\n",
    "        + df[\"direction_y\" + vec_post_fix] ** 2\n",
    "        + df[\"direction_z\" + vec_post_fix] ** 2\n",
    "    )\n",
    "    df[\"zenith\" + angle_post_fix] = np.arccos(df[\"direction_z\" + vec_post_fix] / r)\n",
    "    df[\"azimuth\" + angle_post_fix] = np.arctan2(\n",
    "        df[\"direction_y\" + vec_post_fix], df[\"direction_x\" + vec_post_fix]\n",
    "    )  # np.sign(results['true_y'])*np.arccos((results['true_x'])/(np.sqrt(results['true_x']**2 + results['true_y']**2)))\n",
    "    df[\"azimuth\" + angle_post_fix][df[\"azimuth\" + angle_post_fix] < 0] = (\n",
    "        df[\"azimuth\" + angle_post_fix][df[\"azimuth\" + angle_post_fix] < 0] + 2 * np.pi\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_size(df):\n",
    "    return round(df.memory_usage(deep=True).sum() / 1024**3, 2)\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = get_size(df)\n",
    "    print(f\"Memory usage of dataframe is {start_mem} GB\")\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = get_size(df)\n",
    "    print(f\"Memory usage after optimization is: {end_mem} GB\")\n",
    "    print(f\"Decreased by {100 * (start_mem - end_mem) / start_mem}%\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_config_as_dict(config_name):\n",
    "    out = dict()\n",
    "    for att in dir(config_name):\n",
    "        if att.isupper():\n",
    "            out[att] = getattr(config_name, att)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    PATH_DATASET = Path('../data')\n",
    "    PATH_META = PATH_DATASET / \"train_meta.parquet\"\n",
    "    PATH_GEOMETRY = PATH_DATASET / \"sensor_geometry.csv\"\n",
    "    SAVE_PATH = Path('../data/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def save_folder(BATCH: int, CFG, n_jobs: int = 48):\n",
    "    \"\"\"\n",
    "    Save the events in a folder\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"...\")\n",
    "    print(\"Loading data...\")\n",
    "    train_meta = pd.read_parquet(CFG.PATH_META).query(\"batch_id == @BATCH\")\n",
    "    geometry = pd.read_csv(CFG.PATH_GEOMETRY)\n",
    "    train = pd.read_parquet(CFG.PATH_DATASET / \"train\" / f\"batch_{BATCH}.parquet\")\n",
    "\n",
    "    # current save path\n",
    "    CURRENT_SAVE_PATH = CFG.SAVE_PATH / f\"batch_{BATCH}\"\n",
    "    os.makedirs(CURRENT_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "    # function to get the event from the dataframe\n",
    "    def get_event(event_id: int):\n",
    "        one_event = train_meta.query(\"event_id == @event_id\")\n",
    "        event = train.loc[event_id]\n",
    "        event = event.merge(geometry, on=\"sensor_id\")\n",
    "        target = one_event[[\"azimuth\", \"zenith\"]]\n",
    "        return {\"event\": event.to_records(), \"target\": target.to_records()}\n",
    "\n",
    "    # function that saves event as .pth file\n",
    "    def save_event(event_id: int, save_path: Path):\n",
    "        event = get_event(event_id)\n",
    "        torch.save(event, save_path / f\"{event_id}.pth\")\n",
    "\n",
    "    # function that save in parallel all the using joblib\n",
    "    def save_all_events(event_ids: int, save_path: Path, n_jobs: int = 8):\n",
    "        Parallel(n_jobs=n_jobs)(\n",
    "            delayed(save_event)(event_id, save_path) for event_id in tqdm(event_ids)\n",
    "        )\n",
    "\n",
    "    print(\"Saving data for batch\", BATCH)\n",
    "    save_all_events(train_meta.event_id.unique(), CURRENT_SAVE_PATH, n_jobs=n_jobs)\n",
    "\n",
    "def save_pred_as_csv(y_hat: torch.Tensor, y: torch.Tensor, name: Path):\n",
    "    y_hat = y_hat.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    azimuth_gt = y[:, 0]\n",
    "    zenith_gt = y[:, 1]\n",
    "    azimuth_pred = y_hat[:, 0]\n",
    "    zenith_pred = y_hat[:, 1]\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"azimuth_gt\": azimuth_gt,\n",
    "            \"zenith_gt\": zenith_gt,\n",
    "            \"azimuth_pred\": azimuth_pred,\n",
    "            \"zenith_pred\": zenith_pred,\n",
    "        }\n",
    "    )\n",
    "    df.to_csv(f\"{name}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class SaveModel:\n",
    "    def __init__(self, folder, exp_name, best=np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder) / f\"{exp_name}.pth\"\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        if score < self.best:\n",
    "            self.best = score\n",
    "            print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "            torch.save(model.state_dict(), self.folder)\n",
    "\n",
    "\n",
    "class SaveModelMetric:\n",
    "    def __init__(self, folder, exp_name, best=-np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder) / f\"{exp_name}.pth\"\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        if score > self.best:\n",
    "            self.best = score\n",
    "            print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "            torch.save(model.state_dict(), self.folder)\n",
    "\n",
    "\n",
    "class SaveModelEpoch:\n",
    "    def __init__(self, folder, exp_name, best=-np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        self.best = score\n",
    "        print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "        torch.save(model.state_dict(), f\"{self.folder/self.exp_name}_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def fit(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    loss_fn,\n",
    "    opt,\n",
    "    metric,\n",
    "    folder=\"models\",\n",
    "    exp_name=\"exp_00\",\n",
    "    device=None,\n",
    "    sched=None,\n",
    "    save_md=SaveModelEpoch,\n",
    "):\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write([\"epoch\", \"train_loss\", \"valid_loss\", \"val_metric\"], table=True)\n",
    "    model.to(device)  # we have to put our model on gpu\n",
    "    scaler = torch.cuda.amp.GradScaler()  # this for half precision training\n",
    "    save_md = save_md(folder, exp_name)\n",
    "\n",
    "    for i in mb:  # iterating  epoch\n",
    "        trn_loss, val_loss = 0.0, 0.0\n",
    "        trn_n, val_n = len(train_dl.dataset), len(valid_dl.dataset)\n",
    "        model.train()  # set model for training\n",
    "        for batch in progress_bar(train_dl, parent=mb):\n",
    "            # putting batches to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.cuda.amp.autocast():  # half precision\n",
    "                out = model(batch[\"event\"], mask=batch[\"mask\"])  # forward pass\n",
    "                loss = loss_fn(out, batch[\"label\"])  # calulation loss\n",
    "\n",
    "            trn_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()  # backward\n",
    "            scaler.step(opt)  # optimzers step\n",
    "            scaler.update()  # for half precision\n",
    "            opt.zero_grad()  # zeroing optimizer\n",
    "            if sched is not None:\n",
    "                sched.step()  # scuedular step\n",
    "\n",
    "        trn_loss /= mb.child.total\n",
    "\n",
    "        # putting model in eval mode\n",
    "        model.eval()\n",
    "        gt = []\n",
    "        pred = []\n",
    "        # after epooch is done we can run a validation dataloder and see how are doing\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar(valid_dl, parent=mb):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                with torch.cuda.amp.autocast():  # half precision\n",
    "                    out = model(batch[\"event\"], mask=batch[\"mask\"])  # forward pass\n",
    "                    loss = loss_fn(out, batch[\"label\"])  # calulation loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                gt.append(batch[\"label\"].detach())\n",
    "                pred.append(out.detach())\n",
    "        # calculating metric\n",
    "        metric_ = metric(torch.cat(pred), torch.cat(gt))\n",
    "        # saving model if necessary\n",
    "        save_md(metric_, model, i)\n",
    "        val_loss /= mb.child.total\n",
    "        res = pd.DataFrame(\n",
    "            {\n",
    "                \"epoch\": [i],\n",
    "                \"train_loss\": [trn_loss],\n",
    "                \"valid_loss\": [val_loss],\n",
    "                \"metric\": [metric_],\n",
    "            }\n",
    "        )\n",
    "        print(res)\n",
    "        res.to_csv(f\"{Path(folder)/exp_name}_{i}.csv\", index=False)\n",
    "        gc.collect()\n",
    "    print(\"Training done\")\n",
    "\n",
    "\n",
    "def fit_shuflle(\n",
    "    epochs,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    opt,\n",
    "    metric,\n",
    "    config,\n",
    "    folder=\"models\",\n",
    "    exp_name=\"exp_00\",\n",
    "    device=None,\n",
    "    sched=None,\n",
    "    save_md=SaveModelEpoch,\n",
    "):\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write([\"epoch\", \"train_loss\", \"valid_loss\", \"val_metric\"], table=True)\n",
    "    model.to(device)  # we have to put our model on gpu\n",
    "    scaler = torch.cuda.amp.GradScaler()  # this for half precision training\n",
    "    save_md = save_md(folder, exp_name)\n",
    "\n",
    "    vld_pth = [\n",
    "        load_from_disk(config.DATA_CACHE_DIR / f\"batch_{i}.parquet\")\n",
    "        for i in range(config.VAL_BATCH_RANGE[0], config.VAL_BATCH_RANGE[1])\n",
    "    ]\n",
    "\n",
    "    vld_pth = concatenate_datasets(vld_pth)\n",
    "\n",
    "    vld_ds = config.VAL_DATASET(vld_pth)\n",
    "\n",
    "    valid_dl = DataLoader(\n",
    "        vld_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=config.PRESISTENT_WORKERS,\n",
    "        collate_fn=config.COLLAT_FN,\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"ice\",\n",
    "        entity=\"kaggle-hi\",\n",
    "        name=config.EXP_NAME,\n",
    "        config=get_config_as_dict(config),\n",
    "    )\n",
    "    wandb.watch(model)\n",
    "\n",
    "    for i in mb:  # iterating  epoch\n",
    "        trn_loss, val_loss = 0.0, 0.0\n",
    "        # shuffling the data before every epoch cheaper than shuffling the dataloader\n",
    "\n",
    "        nums = [i for i in range(config.TRN_BATCH_RANGE[0], config.TRN_BATCH_RANGE[1])]\n",
    "        random.shuffle(nums)\n",
    "        trn_pth = [\n",
    "            load_from_disk(config.DATA_CACHE_DIR / f\"batch_{i}.parquet\") for i in nums\n",
    "        ]\n",
    "\n",
    "        trn_pth = concatenate_datasets(trn_pth)\n",
    "        trn_ds = config.TRN_DATASET(trn_pth)\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            trn_ds,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=config.PRESISTENT_WORKERS,\n",
    "            collate_fn=config.COLLAT_FN,\n",
    "        )\n",
    "\n",
    "        trn_n, val_n = len(train_dl.dataset), len(valid_dl.dataset)\n",
    "        model.train()  # set model for training\n",
    "        for batch in progress_bar(train_dl, parent=mb):\n",
    "            # putting batches to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.cuda.amp.autocast():  # half precision\n",
    "                out = model(batch)  # forward pass\n",
    "                loss = loss_fn(out, batch[\"label\"])  # calulation loss\n",
    "\n",
    "            trn_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()  # backward\n",
    "            scaler.step(opt)  # optimzers step\n",
    "            scaler.update()  # for half precision\n",
    "            opt.zero_grad()  # zeroing optimizer\n",
    "            if sched is not None:\n",
    "                sched.step()  # scuedular step\n",
    "\n",
    "        trn_loss /= mb.child.total\n",
    "\n",
    "        # putting model in eval mode\n",
    "        model.eval()\n",
    "        gt = []\n",
    "        pred = []\n",
    "        # after epooch is done we can run a validation dataloder and see how are doing\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar(valid_dl, parent=mb):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                with torch.cuda.amp.autocast():  # half precision\n",
    "                    out = model(batch)  # forward pass\n",
    "                    loss = loss_fn(out, batch[\"label\"])  # calulation loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                gt.append(batch[\"label\"].detach())\n",
    "                pred.append(out.detach())\n",
    "        # calculating metric\n",
    "        metric_ = metric(torch.cat(pred), torch.cat(gt))\n",
    "        # saving predictions\n",
    "        # save_pred_as_csv(\n",
    "        #    torch.cat(pred), torch.cat(gt), name=f\"{Path(folder)/exp_name}_OOF_{i}.csv\"\n",
    "        # )\n",
    "        # saving model if necessary\n",
    "        save_md(metric_, model, i)\n",
    "\n",
    "        val_loss /= mb.child.total\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": i,\n",
    "                \"train_loss\": trn_loss,\n",
    "                \"valid_loss\": val_loss,\n",
    "                \"metric\": metric_,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        res = pd.DataFrame(\n",
    "            {\n",
    "                \"epoch\": [i],\n",
    "                \"train_loss\": [trn_loss],\n",
    "                \"valid_loss\": [val_loss],\n",
    "                \"metric\": [metric_],\n",
    "            }\n",
    "        )\n",
    "        print(res)\n",
    "        res.to_csv(f\"{Path(folder)/exp_name}_{i}.csv\", index=False)\n",
    "        gc.collect()\n",
    "    print(\"Training done\")\n",
    "\n",
    "\n",
    "def fit_shuflle_fp32(\n",
    "    epochs,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    opt,\n",
    "    metric,\n",
    "    config,\n",
    "    folder=\"models\",\n",
    "    exp_name=\"exp_00\",\n",
    "    device=None,\n",
    "    sched=None,\n",
    "    save_md=SaveModelEpoch,\n",
    "):\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write([\"epoch\", \"train_loss\", \"valid_loss\", \"val_metric\"], table=True)\n",
    "    model.to(device)  # we have to put our model on gpu\n",
    "\n",
    "    vld_pth = [\n",
    "        load_from_disk(config.DATA_CACHE_DIR / f\"batch_{i}.parquet\")\n",
    "        for i in range(config.VAL_BATCH_RANGE[0], config.VAL_BATCH_RANGE[1])\n",
    "    ]\n",
    "\n",
    "    vld_pth = concatenate_datasets(vld_pth)\n",
    "\n",
    "    vld_ds = config.VAL_DATASET(vld_pth)\n",
    "\n",
    "    valid_dl = DataLoader(\n",
    "        vld_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=config.PRESISTENT_WORKERS,\n",
    "        collate_fn=config.COLLAT_FN,\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"ice\",\n",
    "        entity=\"kaggle-hi\",\n",
    "        name=config.EXP_NAME,\n",
    "        config=get_config_as_dict(config),\n",
    "    )\n",
    "    wandb.watch(model)\n",
    "\n",
    "    for i in mb:  # iterating  epoch\n",
    "        trn_loss, val_loss = 0.0, 0.0\n",
    "        # shuffling the data before every epoch cheaper than shuffling the dataloader\n",
    "\n",
    "        nums = [i for i in range(config.TRN_BATCH_RANGE[0], config.TRN_BATCH_RANGE[1])]\n",
    "        random.shuffle(nums)\n",
    "        trn_pth = [\n",
    "            load_from_disk(config.DATA_CACHE_DIR / f\"batch_{i}.parquet\") for i in nums\n",
    "        ]\n",
    "\n",
    "        trn_pth = concatenate_datasets(trn_pth)\n",
    "        trn_ds = config.TRN_DATASET(trn_pth)\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            trn_ds,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=config.NUM_WORKERS,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=config.PRESISTENT_WORKERS,\n",
    "            collate_fn=config.COLLAT_FN,\n",
    "        )\n",
    "\n",
    "        trn_n, val_n = len(train_dl.dataset), len(valid_dl.dataset)\n",
    "        model.train()  # set model for training\n",
    "        for batch in progress_bar(train_dl, parent=mb):\n",
    "            # putting batches to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            opt.zero_grad()  # zeroing optimizer\n",
    "\n",
    "            out = model(batch)  # forward pass\n",
    "            loss = loss_fn(out, batch[\"label\"])  # calulation loss\n",
    "\n",
    "            trn_loss += loss.item()\n",
    "\n",
    "            loss.backward()  # backward\n",
    "            opt.step()  # optimizer step\n",
    "            if sched is not None:\n",
    "                sched.step()  # scuedular step\n",
    "\n",
    "        trn_loss /= mb.child.total\n",
    "\n",
    "        # putting model in eval mode\n",
    "        model.eval()\n",
    "        gt = []\n",
    "        pred = []\n",
    "        # after epooch is done we can run a validation dataloder and see how are doing\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar(valid_dl, parent=mb):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                out = model(batch)  # forward pass\n",
    "                loss = loss_fn(out, batch[\"label\"])  # calulation loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                gt.append(batch[\"label\"].detach())\n",
    "                pred.append(out.detach())\n",
    "        # calculating metric\n",
    "        metric_ = metric(torch.cat(pred), torch.cat(gt))\n",
    "        # saving predictions\n",
    "        # save_pred_as_csv(\n",
    "        #    torch.cat(pred), torch.cat(gt), name=f\"{Path(folder)/exp_name}_OOF_{i}.csv\"\n",
    "        # )\n",
    "        # saving model if necessary\n",
    "        save_md(metric_, model, i)\n",
    "\n",
    "        val_loss /= mb.child.total\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": i,\n",
    "                \"train_loss\": trn_loss,\n",
    "                \"valid_loss\": val_loss,\n",
    "                \"metric\": metric_,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        res = pd.DataFrame(\n",
    "            {\n",
    "                \"epoch\": [i],\n",
    "                \"train_loss\": [trn_loss],\n",
    "                \"valid_loss\": [val_loss],\n",
    "                \"metric\": [metric_],\n",
    "            }\n",
    "        )\n",
    "        print(res)\n",
    "        res.to_csv(f\"{Path(folder)/exp_name}_{i}.csv\", index=False)\n",
    "        gc.collect()\n",
    "    print(\"Training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes prediction and groudtruth and saves as csv\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches = pd.read_parquet(CFG.PATH_META)['batch_id'].unique()\n",
    "# batches[50:100]\n",
    "# for b in batches[50:100]:\n",
    "#     save_folder(b, CFG, n_jobs=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#function that compares events from parquet and pth files\n",
    "def compare_events(event_id: int, CFG, BATCH: int):\n",
    "    train_meta = pd.read_parquet(CFG.PATH_META)\n",
    "    geometry = pd.read_csv(CFG.PATH_GEOMETRY)\n",
    "    train = pd.read_parquet(CFG.PATH_DATASET/'train'/f'batch_{BATCH}.parquet')\n",
    "    one_event = train_meta.query(\"event_id == @event_id\")\n",
    "    event = train.loc[event_id]\n",
    "    event = event.merge(geometry, on=\"sensor_id\")\n",
    "    event_pth = pd.DataFrame.from_records(torch.load(CFG.SAVE_PATH/f'batch_{BATCH}' / f\"{event_id}.pth\")['event']).iloc[:, 1:]\n",
    "    return np.all(event == event_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_batch_paths(\n",
    "    start: int,\n",
    "    end: int,\n",
    "    extension: str = \"*.pth\",\n",
    "    cache_dir: Path = Path(\"../data/cache\"),\n",
    ") -> List[Path]:\n",
    "    \"\"\"Get paths to all files in a range of batches\"\"\"\n",
    "    trn_path = []\n",
    "    for i in range(start, end + 1):\n",
    "        path = (cache_dir / f\"batch_{i}\").glob(extension)\n",
    "        trn_path.extend(list(path))\n",
    "    return trn_path\n",
    "\n",
    "\n",
    "def angular_dist_score(\n",
    "    az_true: torch.Tensor,\n",
    "    zen_true: torch.Tensor,\n",
    "    az_pred: torch.Tensor,\n",
    "    zen_pred: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    sa1 = torch.sin(az_true)\n",
    "    ca1 = torch.cos(az_true)\n",
    "    sz1 = torch.sin(zen_true)\n",
    "    cz1 = torch.cos(zen_true)\n",
    "\n",
    "    sa2 = torch.sin(az_pred)\n",
    "    ca2 = torch.cos(az_pred)\n",
    "    sz2 = torch.sin(zen_pred)\n",
    "    cz2 = torch.cos(zen_pred)\n",
    "\n",
    "    scalar_prod = sz1 * sz2 * (ca1 * ca2 + sa1 * sa2) + cz1 * cz2\n",
    "    scalar_prod = torch.clamp(scalar_prod, -1, 1)\n",
    "    return torch.mean(torch.abs(torch.acos(scalar_prod)))\n",
    "\n",
    "\n",
    "# calculte metric based on angular distance\n",
    "def get_score(y_hat, y):\n",
    "    return (\n",
    "        angular_dist_score(y[:, 0], y[:, 1], y_hat[:, 0], y_hat[:, 1])\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_score_v1(y_hat,y):\n",
    "    azi_pred, zen_pred = y_hat.split(2, 1)\n",
    "    y_hat = torch.stack([azi_pred[:, 0], zen_pred[:, 0]], dim=1)\n",
    "    return (\n",
    "        angular_dist_score(y[:, 0], y[:, 1], y_hat[:, 0], y_hat[:, 1])\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    \n",
    "def get_score_vector(y_hat, y):\n",
    "    y_hat = label_to_df(y_hat.detach().cpu().numpy()[:, :3])\n",
    "    y = label_to_df(y.detach().cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        angular_dist_score(\n",
    "            torch.tensor(y[\"azimuth\"].values, dtype=torch.float32),\n",
    "            torch.tensor(y[\"zenith\"].values, dtype=torch.float32),\n",
    "            torch.tensor(y_hat[\"azimuth\"].values, dtype=torch.float32),\n",
    "            torch.tensor(y_hat[\"zenith\"].values, dtype=torch.float32),\n",
    "        )\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    event = [x[\"event\"] for x in batch]\n",
    "    mask = [x[\"mask\"] for x in batch]\n",
    "    label = [x[\"label\"] for x in batch]\n",
    "\n",
    "    event = torch.nn.utils.rnn.pad_sequence(event, batch_first=True)\n",
    "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True)\n",
    "    batch = {\"event\": event, \"mask\": mask, \"label\": torch.stack(label)}\n",
    "    return batch\n",
    "\n",
    "\n",
    "def collate_fn_v1(batch):\n",
    "\n",
    "    event = [x[\"event\"] for x in batch]\n",
    "    mask = [x[\"mask\"] for x in batch]\n",
    "    label = [x[\"label\"] for x in batch]\n",
    "    sensor_id = [x[\"sensor_id\"] for x in batch]\n",
    "\n",
    "    event = torch.nn.utils.rnn.pad_sequence(event, batch_first=True)\n",
    "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True)\n",
    "    sensor_id = torch.nn.utils.rnn.pad_sequence(sensor_id, batch_first=True)\n",
    "    batch = {\n",
    "        \"event\": event,\n",
    "        \"mask\": mask,\n",
    "        \"label\": torch.stack(label),\n",
    "        \"sensor_id\": sensor_id,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def collate_fn_graphv0(batch):\n",
    "\n",
    "    event = [x[\"event\"] for x in batch]\n",
    "    mask = [x[\"mask\"] for x in batch]\n",
    "    label = [x[\"label\"] for x in batch]\n",
    "    distance_matrix = [x[\"distance_matrix\"] for x in batch]\n",
    "    adjecent_matrix = [x[\"adjecent_matrix\"] for x in batch]\n",
    "\n",
    "    event = torch.nn.utils.rnn.pad_sequence(event, batch_first=True)\n",
    "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True)\n",
    "\n",
    "    batch = {\n",
    "        \"event\": event,\n",
    "        \"mask\": mask,\n",
    "        \"label\": torch.stack(label),\n",
    "        \"distance_matrix\": torch.stack(distance_matrix),\n",
    "        \"adjecent_matrix\": torch.stack(adjecent_matrix),\n",
    "    }\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_all_events(train_meta.event_id.unique(), CURRENT_SAVE_PATH, n_jobs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class IceCubeCasheDataset(Dataset):\n",
    "#     def __init__(self, fns):\n",
    "#         self.fns = fns\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.fns)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         res = pd.DataFrame.from_records(torch.load(self.fns[idx])[\"event\"])[\n",
    "#             [\"x\", \"y\", \"z\", \"charge\"]\n",
    "#         ]\n",
    "#         return res.values\n",
    "\n",
    "\n",
    "# # collate function that pads the data to the same length\n",
    "# def collate_fn(batch):\n",
    "#     batch = [torch.tensor(x) for x in batch]\n",
    "#     batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True)\n",
    "#     return batch\n",
    "\n",
    "\n",
    "# dataset = IceCubeCasheDataset(fns)\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=256,\n",
    "#     shuffle=True,\n",
    "#     num_workers=16,\n",
    "#     persistent_workers=True,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def good_luck():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def generate_matrices_torch(points, threshold_distance):\n",
    "#     # Convert the input points to a PyTorch tensor\n",
    "#     points = torch.tensor(points, dtype=torch.float32)\n",
    "    \n",
    "#     # Calculate the pairwise distances using PyTorch's pairwise_distance function\n",
    "#     dist_matrix = torch.pairwise_distance(points, p=2)\n",
    "    \n",
    "#     # Create the adjacency matrix by thresholding the distance matrix\n",
    "#     adj_matrix = (dist_matrix < threshold_distance).float()\n",
    "    \n",
    "#     return adj_matrix, dist_matrix\n",
    "\n",
    "# points = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "# threshold_distance = 5\n",
    "# adj_matrix, dist_matrix = generate_matrices_torch(points, threshold_distance)\n",
    "\n",
    "# print(\"Adjacency Matrix:\")\n",
    "# print(adj_matrix)\n",
    "\n",
    "# print(\"Distance Matrix:\")\n",
    "# print(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
