{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp graphdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from datasets import  load_from_disk\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from functools import partial\n",
    "\n",
    "from graphnet.models.components.pool import (\n",
    "    sum_pool_and_distribute,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "import seaborn as sns\n",
    "class CFG:\n",
    "    CACHE_PATH = Path('../data/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/slh/icecube/')\n",
    "from icecube.models import SinusoidalPosEmb\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = list((CFG.CACHE_PATH/'batch_3').glob('*.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset as gDataset\n",
    "from torch_geometric.loader import DataLoader as gDataLoader\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "\n",
    "\n",
    "def add_features_to_batch(batch):\n",
    "    '''add features to batch + 2'''\n",
    "    charge = batch.x[:, 4].unsqueeze(-1)\n",
    "    center_of_gravity = sum_pool_and_distribute(\n",
    "                batch.pos * charge, batch.batch\n",
    "            ) / sum_pool_and_distribute(charge, batch.batch)\n",
    "    vector_to_center_of_gravity = center_of_gravity - batch.pos\n",
    "    distance_to_center_of_gravity = torch.norm(\n",
    "                vector_to_center_of_gravity, p=2, dim=1\n",
    "            )\n",
    "    unit_vector_to_center_of_gravity = vector_to_center_of_gravity / (\n",
    "                distance_to_center_of_gravity.unsqueeze(dim=1) + 1e-3\n",
    "            )\n",
    "    cos_angle_wrt_center_of_gravity = (\n",
    "                batch.pos * unit_vector_to_center_of_gravity\n",
    "            ).sum(dim=1)\n",
    "    \n",
    "    out = torch.cat(\n",
    "            (\n",
    "                batch.x,\n",
    "                distance_to_center_of_gravity.unsqueeze(dim=1),\n",
    "                cos_angle_wrt_center_of_gravity.unsqueeze(dim=1),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "# function to normalize input between 1 and 0\n",
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "#| export\n",
    "def ice_transparency(\n",
    "    data_path=\"/opt/slh/icecube/data/ice_transparency.txt\", datum=1950\n",
    "):\n",
    "    # Data from page 31 of https://arxiv.org/pdf/1301.5361.pdf\n",
    "    # Datum is from footnote 8 of page 29\n",
    "    df = pd.read_csv(data_path, delim_whitespace=True)\n",
    "    df[\"z\"] = df[\"depth\"] - datum\n",
    "    df[\"z_norm\"] = df[\"z\"] / 500\n",
    "    df[[\"scattering_len_norm\", \"absorption_len_norm\"]] = RobustScaler().fit_transform(\n",
    "        df[[\"scattering_len\", \"absorption_len\"]]\n",
    "    )\n",
    "\n",
    "    # These are both roughly equivalent after scaling\n",
    "    f_scattering = interp1d(df[\"z_norm\"], df[\"scattering_len_norm\"])\n",
    "    f_absorption = interp1d(df[\"z_norm\"], df[\"absorption_len_norm\"])\n",
    "    return f_scattering, f_absorption\n",
    "\n",
    "def prepare_sensors():\n",
    "    sensors = pd.read_csv('/opt/slh/icecube/data/sensor_geometry.csv').astype(\n",
    "        {\n",
    "            \"sensor_id\": np.int16,\n",
    "            \"x\": np.float32,\n",
    "            \"y\": np.float32,\n",
    "            \"z\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    sensors[\"string\"] = 0\n",
    "    sensors[\"qe\"] = 1\n",
    "\n",
    "    for i in range(len(sensors) // 60):\n",
    "        start, end = i * 60, (i * 60) + 60\n",
    "        sensors.loc[start:end, \"string\"] = i\n",
    "\n",
    "        # High Quantum Efficiency in the lower 50 DOMs - https://arxiv.org/pdf/2209.03042.pdf (Figure 1)\n",
    "        if i in range(78, 86):\n",
    "            start_veto, end_veto = i * 60, (i * 60) + 10\n",
    "            start_core, end_core = end_veto + 1, (i * 60) + 60\n",
    "            sensors.loc[start_core:end_core, \"qe\"] = 1.35\n",
    "\n",
    "    # https://github.com/graphnet-team/graphnet/blob/b2bad25528652587ab0cdb7cf2335ee254cfa2db/src/graphnet/models/detector/icecube.py#L33-L41\n",
    "    # Assume that \"rde\" (relative dom efficiency) is equivalent to QE\n",
    "    sensors[\"x\"] /= 500\n",
    "    sensors[\"y\"] /= 500\n",
    "    sensors[\"z\"] /= 500\n",
    "    sensors[\"qe\"] -= 1.25\n",
    "    sensors[\"qe\"] /= 0.25\n",
    "\n",
    "    return sensors.set_index(\"sensor_id\")[['qe']]\n",
    "\n",
    "\n",
    "def convert_to_3d(azimuth, zenith):\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    x = np.cos(azimuth) * np.sin(zenith)\n",
    "    y = np.sin(azimuth) * np.sin(zenith)\n",
    "    z = np.cos(zenith)\n",
    "    return np.array([x, y, z], dtype=np.float32)\n",
    "\n",
    "def event_filtering_v1(batch, max_pulse_count=128, t_valid_length=6199.700247193777):\n",
    "    col = batch.columns\n",
    "    t_peak = batch[\"time\"][batch[\"charge\"].argmax()]\n",
    "    t_valid_min = t_peak - t_valid_length\n",
    "    t_valid_max = t_peak + t_valid_length\n",
    "    t_valid = (batch[\"time\"] > t_valid_min) * (batch[\"time\"] < t_valid_max)\n",
    "    batch[\"rank\"] = 2 * (1 - batch[\"auxiliary\"]) + (t_valid)\n",
    "    batch = batch.sort_values(by=[\"rank\", \"charge\"])\n",
    "    # pick-up from backward\n",
    "    batch = batch[-max_pulse_count:]\n",
    "        # resort by time\n",
    "    batch = batch.sort_values(by=\"time\")\n",
    "    return batch[col]\n",
    "\n",
    "\n",
    "def event_filtering_v2(batch, max_pulse_count=128, t_valid_length=6199.700247193777):\n",
    "    col = batch.columns\n",
    "    batch = batch.sort_values(by=[\"auxiliary\", \"time\"])\n",
    "    batch = batch[:max_pulse_count]\n",
    "    batch = batch.sort_values(by=\"time\")\n",
    "    return batch[col]\n",
    "\n",
    "\n",
    "def get_distance_matrix(xyz):\n",
    "    return torch.cdist(xyz, xyz)\n",
    "\n",
    "\n",
    "def get_distance_matrix_for_indices(dm, indices):\n",
    "    return dm[indices][:, indices]\n",
    "\n",
    "\n",
    "def get_distance_matrix_from_csv(\n",
    "    path_to_geom=\"/opt/slh/icecube/data/sensor_geometry.csv\",\n",
    "):\n",
    "    geom = pd.read_csv(path_to_geom)[[\"x\", \"y\", \"z\"]]\n",
    "    geom = torch.tensor(geom.values, dtype=torch.float32)\n",
    "    geom = get_distance_matrix(geom)\n",
    "    # nromalize goematry matrix\n",
    "    geom = geom / geom.max()\n",
    "    return geom\n",
    "    \n",
    "\n",
    "class GraphDasetV0(gDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        max_events=196,\n",
    "        transform=None,\n",
    "        pre_transform= KNNGraphBuilder(nb_nearest_neighbours=8, columns=[0, 1, 2, 3]),\n",
    "        pre_filter=None,\n",
    "    ):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def get(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = torch.tensor(event[\n",
    "            [\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values, dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor(convert_to_3d(item[\"azimuth\"], item[\"zenith\"]), dtype=torch.float32)\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])    \n",
    "    \n",
    "        data = Data(x=deepcopy(event), \n",
    "                    pos= event[:, :3],\n",
    "                    n_pulses=torch.tensor(event.shape[0],\n",
    "                                          dtype=torch.int32), \n",
    "                    y = label)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "class GraphDasetV1(gDataset):\n",
    "    \"same as GraphDasetV0 but with eventid\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        max_events=196,\n",
    "        transform=None,\n",
    "        pre_transform= KNNGraphBuilder(nb_nearest_neighbours=9, columns=[0, 1, 2, 3]),\n",
    "        pre_filter=None,\n",
    "    ):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def get(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = torch.tensor(event[\n",
    "            [\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].values, dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor(convert_to_3d(item[\"azimuth\"], item[\"zenith\"]), dtype=torch.float32)\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])    \n",
    "    \n",
    "        data = Data(x=deepcopy(event[:, :-1]), \n",
    "                    pos= event[:, :3],\n",
    "                    n_pulses=torch.tensor(event.shape[0],\n",
    "                                          dtype=torch.int32), \n",
    "                    sensor_id = event[:, -1].int(),\n",
    "                    y = label)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "class GraphDasetV3(gDataset):\n",
    "    'same as V0 but with 128 events'\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        max_events=128,\n",
    "        transform=None,\n",
    "        pre_transform= KNNGraphBuilder(nb_nearest_neighbours=8, columns=[0, 1, 2, 3]),\n",
    "        pre_filter=None,\n",
    "    ):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def get(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v1(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "        event[\"auxiliary\"] -= 0.5\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event = torch.tensor(event[\n",
    "            [\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"qe\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values, dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor(convert_to_3d(item[\"azimuth\"], item[\"zenith\"]), dtype=torch.float32)\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])    \n",
    "    \n",
    "        data = Data(x=deepcopy(event), \n",
    "                    pos= event[:, :3],\n",
    "                    n_pulses=torch.tensor(event.shape[0],\n",
    "                                          dtype=torch.int32), \n",
    "                    y = label)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "class GraphDasetV4(gDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        max_events=196,\n",
    "        transform=None,\n",
    "        pre_transform= KNNGraphBuilder(nb_nearest_neighbours=7, columns=[0, 1, 2, 3, 4]),\n",
    "        pre_filter=None,\n",
    "    ):\n",
    "        super().__init__(transform, pre_transform, pre_filter)\n",
    "        self.ds = ds\n",
    "        self.max_events = max_events\n",
    "        self.f_scattering, self.f_absorption = ice_transparency()\n",
    "        self.sensor_data = prepare_sensors()\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def get(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        event = pd.DataFrame(item)[\n",
    "            [\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"auxiliary\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"sensor_id\"\n",
    "            ]\n",
    "        ].astype(np.float32)\n",
    "        t = (event[\"time\"].values - 1.0e04) / 3.0e4\n",
    "        event[\"time\"] /= event[\"time\"].max()\n",
    "\n",
    "        if event.shape[0] > self.max_events:\n",
    "            event = event_filtering_v2(event, max_pulse_count=self.max_events)\n",
    "\n",
    "        event[[\"x\", \"y\", \"z\"]] /= 500\n",
    "        event[\"charge\"] = np.log10(event[\"charge\"]) / 3.0\n",
    "\n",
    "        event[\"time\"] = t[: self.max_events]\n",
    "        event[\"scattering\"] = self.f_scattering(event[\"z\"].values).reshape(-1)\n",
    "        event[\"absorption\"] = self.f_absorption(event[\"z\"].values).reshape(-1)\n",
    "        event['qe'] = self.sensor_data.loc[event['sensor_id'].values].values.reshape(-1)\n",
    "        event_ = torch.tensor(event[\n",
    "            [\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"z\",\n",
    "                \"time\",\n",
    "                \"charge\",\n",
    "                \"scattering\",\n",
    "                \"absorption\",\n",
    "            ]\n",
    "        ].values, dtype=torch.float32)\n",
    "        \n",
    "        label = torch.tensor(convert_to_3d(item[\"azimuth\"], item[\"zenith\"]), dtype=torch.float32)\n",
    "        #print(item[\"azimuth\"], item[\"zenith\"])    \n",
    "    \n",
    "        data = Data(x=deepcopy(event_), \n",
    "                    n_pulses=torch.tensor(event.shape[0],dtype=torch.int32), \n",
    "                    qe = torch.tensor(event['qe'].values + 1, dtype=torch.long),\n",
    "                    aux= torch.tensor(event['auxiliary'].values, dtype=torch.long),\n",
    "                    y = label)\n",
    "        \n",
    "        return data\n",
    "\n",
    "GraphDasetV5 = partial(GraphDasetV0,\n",
    "        max_events=196,\n",
    "        transform=None,\n",
    "        pre_transform= KNNGraphBuilder(nb_nearest_neighbours=8, columns=[0, 1, 2, 3, 4]),\n",
    "        pre_filter=None)\n",
    "\n",
    "\n",
    "GraphDasetV6 = partial(GraphDasetV0,\n",
    "        max_events=256,\n",
    "        transform=None,\n",
    "        pre_transform= KNNGraphBuilder(nb_nearest_neighbours=8, columns=[0, 1, 2, 3]),\n",
    "        pre_filter=None)\n",
    "\n",
    "\n",
    "\n",
    "GraphDasetV7 = partial(GraphDasetV4,\n",
    "        max_events=196,\n",
    "        transform=None,\n",
    "        pre_transform= KNNGraphBuilder(nb_nearest_neighbours=8, columns=[0, 1, 2, 3]),\n",
    "        pre_filter=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = GraphDasetV7(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[23].aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[23].aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = gDataLoader(ds, batch_size=10, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([727, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.x[:, :4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[655, 656, 657, 658, 659]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(655, 660))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.data import DataLoader as gDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dl = gDataLoader(ds, batch_size=10, shuffle=False)\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index = np.random.randint(0, 200000)\n",
    "index\n",
    "index = 2307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
