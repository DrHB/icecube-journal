{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp modelsgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/archive/software/graphnet/src')\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn\n",
    "from graphnet.models.task.reconstruction import (\n",
    "    DirectionReconstructionWithKappa,\n",
    "    AzimuthReconstructionWithKappa,\n",
    "    ZenithReconstruction,\n",
    ")\n",
    "\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss,  VonMisesFisher2DLoss, EuclideanDistanceLoss\n",
    "from graphnet.models.gnn.gnn import GNN\n",
    "from graphnet.models.utils import calculate_xyzt_homophily\n",
    "from graphnet.utilities.config import save_model_config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import EdgeConv\n",
    "from torch_geometric.nn.pool import knn_graph\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_min, scatter_sum\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n",
    "from torch import Tensor, LongTensor\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch.nn import Linear, ReLU, SiLU, Sequential\n",
    "from torch_scatter import scatter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('/opt/slh/icecube/')\n",
    "from icecube.graphdataset import GraphDasetV0, GraphDasetV1, GraphDasetV3\n",
    "from datasets import  load_from_disk\n",
    "from torch_geometric.loader import DataLoader as gDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "    \n",
    "loss_fn_azi = VonMisesFisher2DLoss()\n",
    "loss_fn_zen = nn.L1Loss()\n",
    "\n",
    "class CombineLossV0(nn.Module):\n",
    "    def __init__(self, loss_fn_azi=loss_fn_azi, loss_fn_zen=loss_fn_zen):\n",
    "        super().__init__()\n",
    "        self.loss_fn_azi = loss_fn_azi\n",
    "        self.loss_fn_zen = loss_fn_zen\n",
    "        \n",
    "    def forward(self, batch, output):\n",
    "        target = batch['label']\n",
    "        azi_pred, zen_pred = output.split(2, 1)\n",
    "        azi_loss = self.loss_fn_azi(azi_pred, target)\n",
    "        zen_loss = self.loss_fn_zen(zen_pred, target[:, -1].unsqueeze(-1))\n",
    "        return azi_loss + zen_loss\n",
    "\n",
    "    \n",
    "class EncoderWithReconstructionLossV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.az = AzimuthReconstructionWithKappa(\n",
    "            hidden_size=128,\n",
    "            loss_function=loss_fn_azi,\n",
    "            target_labels=[\"azimuth\", \"kappa\"],\n",
    "        )\n",
    "        self.zn =  ZenithReconstruction(\n",
    "            hidden_size=128,\n",
    "            loss_function=loss_fn_zen,\n",
    "            target_labels=[\"zenith\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch['event'], batch['mask']\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        az = self.az(x)\n",
    "        zn = self.zn(x)\n",
    "        return torch.concat([az, zn], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderWithReconstructionLossV0().eval()\n",
    "event = torch.rand(10, 100, 8)\n",
    "mask = torch.ones(10, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (10, 100))\n",
    "label = torch.rand(10, 2)\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id, label=label)\n",
    "with torch.no_grad():\n",
    "    y = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class EuclideanDistanceLossG(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-6, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        diff = prediction - target\n",
    "        loss = torch.norm(diff, dim=1) + self.eps\n",
    "        if self.reduction == 'mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "class gVonMisesFisher3DLossEcludeLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = EuclideanDistanceLossG()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.reshape(-1, 3)\n",
    "        return (self.vonmis(y_pred, y_true) + self.cosine(y_pred[:, :3], y_true))/2\n",
    "    \n",
    "    \n",
    "class gVonMisesFisher3DLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.reshape(-1, 3)\n",
    "        return self.vonmis(y_pred, y_true)\n",
    "    \n",
    "class gVonMisesFisher3DLossCosineSimularityLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.reshape(-1, 3)\n",
    "        return (self.vonmis(y_pred, y_true) + (1-self.cosine(y_pred[:, :3], y_true).mean()))/2\n",
    "    \n",
    "    \n",
    "GLOBAL_POOLINGS = {\n",
    "    \"min\": scatter_min,\n",
    "    \"max\": scatter_max,\n",
    "    \"sum\": scatter_sum,\n",
    "    \"mean\": scatter_mean,\n",
    "}\n",
    "\n",
    "class DynEdgeConv(EdgeConv):\n",
    "    \"\"\"Dynamical edge convolution layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn: Callable,\n",
    "        aggr: str = \"max\",\n",
    "        nb_neighbors: int = 8,\n",
    "        features_subset: Optional[Union[Sequence[int], slice]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdgeConv`.\n",
    "        Args:\n",
    "            nn: The MLP/torch.Module to be used within the `EdgeConv`.\n",
    "            aggr: Aggregation method to be used with `EdgeConv`.\n",
    "            nb_neighbors: Number of neighbours to be clustered after the\n",
    "                `EdgeConv` operation.\n",
    "            features_subset: Subset of features in `Data.x` that should be used\n",
    "                when dynamically performing the new graph clustering after the\n",
    "                `EdgeConv` operation. Defaults to all features.\n",
    "            **kwargs: Additional features to be passed to `EdgeConv`.\n",
    "        \"\"\"\n",
    "        # Check(s)\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(None)  # Use all features\n",
    "        assert isinstance(features_subset, (list, slice))\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nn=nn, aggr=aggr, **kwargs)\n",
    "\n",
    "        # Additional member variables\n",
    "        self.nb_neighbors = nb_neighbors\n",
    "        self.features_subset = features_subset\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, edge_index: Adj, batch: Optional[Tensor] = None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Standard EdgeConv forward pass\n",
    "        x = super().forward(x, edge_index)\n",
    "        dev = x.device\n",
    "\n",
    "        # Recompute adjacency\n",
    "        edge_index = knn_graph(\n",
    "            x=x[:, self.features_subset],\n",
    "            k=self.nb_neighbors,\n",
    "            batch=batch,\n",
    "        ).to(dev)\n",
    "\n",
    "        return x, edge_index\n",
    "\n",
    "\n",
    "class DynEdge(GNN):\n",
    "    \"\"\"DynEdge (dynamical edge convolutional) model.\"\"\"\n",
    "\n",
    "    @save_model_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_inputs: int,\n",
    "        *,\n",
    "        nb_neighbours: int = 8,\n",
    "        features_subset: Optional[Union[List[int], slice]] = None,\n",
    "        dynedge_layer_sizes: Optional[List[Tuple[int, ...]]] = None,\n",
    "        post_processing_layer_sizes: Optional[List[int]] = None,\n",
    "        readout_layer_sizes: Optional[List[int]] = None,\n",
    "        global_pooling_schemes: Optional[Union[str, List[str]]] = None,\n",
    "        add_global_variables_after_pooling: bool = False,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdge`.\n",
    "        Args:\n",
    "            nb_inputs: Number of input features on each node.\n",
    "            nb_neighbours: Number of neighbours to used in the k-nearest\n",
    "                neighbour clustering which is performed after each (dynamical)\n",
    "                edge convolution.\n",
    "            features_subset: The subset of latent features on each node that\n",
    "                are used as metric dimensions when performing the k-nearest\n",
    "                neighbours clustering. Defaults to [0,1,2].\n",
    "            dynedge_layer_sizes: The layer sizes, or latent feature dimenions,\n",
    "                used in the `DynEdgeConv` layer. Each entry in\n",
    "                `dynedge_layer_sizes` corresponds to a single `DynEdgeConv`\n",
    "                layer; the integers in the corresponding tuple corresponds to\n",
    "                the layer sizes in the multi-layer perceptron (MLP) that is\n",
    "                applied within each `DynEdgeConv` layer. That is, a list of\n",
    "                size-two tuples means that all `DynEdgeConv` layers contain a\n",
    "                two-layer MLP.\n",
    "                Defaults to [(128, 256), (336, 256), (336, 256), (336, 256)].\n",
    "            post_processing_layer_sizes: Hidden layer sizes in the MLP\n",
    "                following the skip-concatenation of the outputs of each\n",
    "                `DynEdgeConv` layer. Defaults to [336, 256].\n",
    "            readout_layer_sizes: Hidden layer sizes in the MLP following the\n",
    "                post-processing _and_ optional global pooling. As this is the\n",
    "                last layer(s) in the model, the last layer in the read-out\n",
    "                yields the output of the `DynEdge` model. Defaults to [128,].\n",
    "            global_pooling_schemes: The list global pooling schemes to use.\n",
    "                Options are: \"min\", \"max\", \"mean\", and \"sum\".\n",
    "            add_global_variables_after_pooling: Whether to add global variables\n",
    "                after global pooling. The alternative is to  added (distribute)\n",
    "                them to the individual nodes before any convolutional\n",
    "                operations.\n",
    "        \"\"\"\n",
    "        # Latent feature subset for computing nearest neighbours in DynEdge.\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(0, 3)\n",
    "\n",
    "        # DynEdge layer sizes\n",
    "        if dynedge_layer_sizes is None:\n",
    "            dynedge_layer_sizes = [\n",
    "                (\n",
    "                    128,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        assert isinstance(dynedge_layer_sizes, list)\n",
    "        assert len(dynedge_layer_sizes)\n",
    "        assert all(isinstance(sizes, tuple) for sizes in dynedge_layer_sizes)\n",
    "        assert all(len(sizes) > 0 for sizes in dynedge_layer_sizes)\n",
    "        assert all(all(size > 0 for size in sizes) for sizes in dynedge_layer_sizes)\n",
    "\n",
    "        self._dynedge_layer_sizes = dynedge_layer_sizes\n",
    "\n",
    "        # Post-processing layer sizes\n",
    "        if post_processing_layer_sizes is None:\n",
    "            post_processing_layer_sizes = [\n",
    "                336,\n",
    "                256,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(post_processing_layer_sizes, list)\n",
    "        assert len(post_processing_layer_sizes)\n",
    "        assert all(size > 0 for size in post_processing_layer_sizes)\n",
    "\n",
    "        self._post_processing_layer_sizes = post_processing_layer_sizes\n",
    "\n",
    "        # Read-out layer sizes\n",
    "        if readout_layer_sizes is None:\n",
    "            readout_layer_sizes = [\n",
    "                128,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(readout_layer_sizes, list)\n",
    "        assert len(readout_layer_sizes)\n",
    "        assert all(size > 0 for size in readout_layer_sizes)\n",
    "\n",
    "        self._readout_layer_sizes = readout_layer_sizes\n",
    "\n",
    "        # Global pooling scheme(s)\n",
    "        if isinstance(global_pooling_schemes, str):\n",
    "            global_pooling_schemes = [global_pooling_schemes]\n",
    "\n",
    "        if isinstance(global_pooling_schemes, list):\n",
    "            for pooling_scheme in global_pooling_schemes:\n",
    "                assert (\n",
    "                    pooling_scheme in GLOBAL_POOLINGS\n",
    "                ), f\"Global pooling scheme {pooling_scheme} not supported.\"\n",
    "        else:\n",
    "            assert global_pooling_schemes is None\n",
    "\n",
    "        self._global_pooling_schemes = global_pooling_schemes\n",
    "\n",
    "        if add_global_variables_after_pooling:\n",
    "            assert self._global_pooling_schemes, (\n",
    "                \"No global pooling schemes were request, so cannot add global\"\n",
    "                \" variables after pooling.\"\n",
    "            )\n",
    "        self._add_global_variables_after_pooling = add_global_variables_after_pooling\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nb_inputs, self._readout_layer_sizes[-1])\n",
    "\n",
    "        # Remaining member variables()\n",
    "        self._activation = torch.nn.GELU()\n",
    "        self._nb_inputs = nb_inputs\n",
    "        self._nb_global_variables = 5 + nb_inputs\n",
    "        self._nb_neighbours = nb_neighbours\n",
    "        self._features_subset = features_subset\n",
    "\n",
    "        self._construct_layers()\n",
    "\n",
    "    def _construct_layers(self) -> None:\n",
    "        \"\"\"Construct layers (torch.nn.Modules).\"\"\"\n",
    "        # Convolutional operations\n",
    "        nb_input_features = self._nb_inputs\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            nb_input_features += self._nb_global_variables\n",
    "\n",
    "        self._conv_layers = torch.nn.ModuleList()\n",
    "        nb_latent_features = nb_input_features\n",
    "        for sizes in self._dynedge_layer_sizes:\n",
    "            layers = []\n",
    "            layer_sizes = [nb_latent_features] + list(sizes)\n",
    "            for ix, (nb_in, nb_out) in enumerate(\n",
    "                zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "            ):\n",
    "                if ix == 0:\n",
    "                    nb_in *= 2\n",
    "                layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "                layers.append(nn.BatchNorm1d(nb_out))\n",
    "                layers.append(self._activation)\n",
    "\n",
    "            conv_layer = DynEdgeConv(\n",
    "                torch.nn.Sequential(*layers),\n",
    "                aggr=\"add\",\n",
    "                nb_neighbors=self._nb_neighbours,\n",
    "                features_subset=self._features_subset,\n",
    "            )\n",
    "            self._conv_layers.append(conv_layer)\n",
    "\n",
    "            nb_latent_features = nb_out\n",
    "\n",
    "        # Post-processing operations\n",
    "        nb_latent_features = (\n",
    "            sum(sizes[-1] for sizes in self._dynedge_layer_sizes) + nb_input_features\n",
    "        )\n",
    "\n",
    "        post_processing_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._post_processing_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            post_processing_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            post_processing_layers.append(nn.BatchNorm1d(nb_out))\n",
    "            post_processing_layers.append(self._activation)\n",
    "\n",
    "        self._post_processing = torch.nn.Sequential(*post_processing_layers)\n",
    "\n",
    "        # Read-out operations\n",
    "        nb_poolings = (\n",
    "            len(self._global_pooling_schemes) if self._global_pooling_schemes else 1\n",
    "        )\n",
    "        nb_latent_features = nb_out * nb_poolings\n",
    "        if self._add_global_variables_after_pooling:\n",
    "            nb_latent_features += self._nb_global_variables\n",
    "\n",
    "        readout_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._readout_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            readout_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            readout_layers.append(nn.BatchNorm1d(nb_out))\n",
    "            readout_layers.append(self._activation)\n",
    "\n",
    "        self._readout = torch.nn.Sequential(*readout_layers)\n",
    "\n",
    "    def _global_pooling(self, x: Tensor, batch: LongTensor) -> Tensor:\n",
    "        \"\"\"Perform global pooling.\"\"\"\n",
    "        assert self._global_pooling_schemes\n",
    "        pooled = []\n",
    "        for pooling_scheme in self._global_pooling_schemes:\n",
    "            pooling_fn = GLOBAL_POOLINGS[pooling_scheme]\n",
    "            pooled_x = pooling_fn(x, index=batch, dim=0)\n",
    "            if isinstance(pooled_x, tuple) and len(pooled_x) == 2:\n",
    "                # `scatter_{min,max}`, which return also an argument, vs.\n",
    "                # `scatter_{mean,sum}`\n",
    "                pooled_x, _ = pooled_x\n",
    "            pooled.append(pooled_x)\n",
    "\n",
    "        return torch.cat(pooled, dim=1)\n",
    "\n",
    "    def _calculate_global_variables(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: LongTensor,\n",
    "        batch: LongTensor,\n",
    "        *additional_attributes: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Calculate global variables.\"\"\"\n",
    "        # Calculate homophily (scalar variables)\n",
    "        h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n",
    "\n",
    "        # Calculate mean features\n",
    "        global_means = scatter_mean(x, batch, dim=0)\n",
    "\n",
    "        # Add global variables\n",
    "        global_variables = torch.cat(\n",
    "            [\n",
    "                global_means,\n",
    "                h_x,\n",
    "                h_y,\n",
    "                h_z,\n",
    "                h_t,\n",
    "            ]\n",
    "            + [attr.unsqueeze(dim=1) for attr in additional_attributes],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return global_variables\n",
    "\n",
    "    def forward(self, data: Data) -> Tensor:\n",
    "        \"\"\"Apply learnable forward pass.\"\"\"\n",
    "        # Convenience variables\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        global_variables = self._calculate_global_variables(\n",
    "            x,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            torch.log10(data.n_pulses),\n",
    "        )\n",
    "\n",
    "        # Distribute global variables out to each node\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            distribute = (\n",
    "                batch.unsqueeze(dim=1) == torch.unique(batch).unsqueeze(dim=0)\n",
    "            ).type(torch.float)\n",
    "\n",
    "            global_variables_distributed = torch.sum(\n",
    "                distribute.unsqueeze(dim=2) * global_variables.unsqueeze(dim=0),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            x = torch.cat((x, global_variables_distributed), dim=1)\n",
    "\n",
    "        # DynEdge-convolutions\n",
    "        skip_connections = [x]\n",
    "        for conv_layer in self._conv_layers:\n",
    "            x, edge_index = conv_layer(x, edge_index, batch)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        # Skip-cat\n",
    "        x = torch.cat(skip_connections, dim=1)\n",
    "\n",
    "        # Post-processing\n",
    "        x = self._post_processing(x)\n",
    "\n",
    "        # (Optional) Global pooling\n",
    "        if self._global_pooling_schemes:\n",
    "            x = self._global_pooling(x, batch=batch)\n",
    "            if self._add_global_variables_after_pooling:\n",
    "                x = torch.cat(\n",
    "                    [\n",
    "                        x,\n",
    "                        global_variables,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "        # Read-out\n",
    "        x = self._readout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DynEdgeConv(EdgeConv):\n",
    "    \"\"\"Dynamical edge convolution layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn: Callable,\n",
    "        aggr: str = \"max\",\n",
    "        nb_neighbors: int = 8,\n",
    "        features_subset: Optional[Union[Sequence[int], slice]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdgeConv`.\n",
    "        Args:\n",
    "            nn: The MLP/torch.Module to be used within the `EdgeConv`.\n",
    "            aggr: Aggregation method to be used with `EdgeConv`.\n",
    "            nb_neighbors: Number of neighbours to be clustered after the\n",
    "                `EdgeConv` operation.\n",
    "            features_subset: Subset of features in `Data.x` that should be used\n",
    "                when dynamically performing the new graph clustering after the\n",
    "                `EdgeConv` operation. Defaults to all features.\n",
    "            **kwargs: Additional features to be passed to `EdgeConv`.\n",
    "        \"\"\"\n",
    "        # Check(s)\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(None)  # Use all features\n",
    "        assert isinstance(features_subset, (list, slice))\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nn=nn, aggr=aggr, **kwargs)\n",
    "\n",
    "        # Additional member variables\n",
    "        self.nb_neighbors = nb_neighbors\n",
    "        self.features_subset = features_subset\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, edge_index: Adj, batch: Optional[Tensor] = None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Standard EdgeConv forward pass\n",
    "        x = super().forward(x, edge_index)\n",
    "        dev = x.device\n",
    "\n",
    "        # Recompute adjacency\n",
    "        edge_index = knn_graph(\n",
    "            x=x[:, self.features_subset],\n",
    "            k=self.nb_neighbors,\n",
    "            batch=batch,\n",
    "        ).to(dev)\n",
    "\n",
    "        return x, edge_index\n",
    "\n",
    "\n",
    "class DynEdgeFEXTRACTRO(GNN):\n",
    "    \"\"\"DynEdge (dynamical edge convolutional) model.\"\"\"\n",
    "\n",
    "    @save_model_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_inputs: int,\n",
    "        *,\n",
    "        nb_neighbours: int = 8,\n",
    "        features_subset: Optional[Union[List[int], slice]] = None,\n",
    "        dynedge_layer_sizes: Optional[List[Tuple[int, ...]]] = None,\n",
    "        post_processing_layer_sizes: Optional[List[int]] = None,\n",
    "        readout_layer_sizes: Optional[List[int]] = None,\n",
    "        global_pooling_schemes: Optional[Union[str, List[str]]] = None,\n",
    "        add_global_variables_after_pooling: bool = False,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdge`.\n",
    "        Args:\n",
    "            nb_inputs: Number of input features on each node.\n",
    "            nb_neighbours: Number of neighbours to used in the k-nearest\n",
    "                neighbour clustering which is performed after each (dynamical)\n",
    "                edge convolution.\n",
    "            features_subset: The subset of latent features on each node that\n",
    "                are used as metric dimensions when performing the k-nearest\n",
    "                neighbours clustering. Defaults to [0,1,2].\n",
    "            dynedge_layer_sizes: The layer sizes, or latent feature dimenions,\n",
    "                used in the `DynEdgeConv` layer. Each entry in\n",
    "                `dynedge_layer_sizes` corresponds to a single `DynEdgeConv`\n",
    "                layer; the integers in the corresponding tuple corresponds to\n",
    "                the layer sizes in the multi-layer perceptron (MLP) that is\n",
    "                applied within each `DynEdgeConv` layer. That is, a list of\n",
    "                size-two tuples means that all `DynEdgeConv` layers contain a\n",
    "                two-layer MLP.\n",
    "                Defaults to [(128, 256), (336, 256), (336, 256), (336, 256)].\n",
    "            post_processing_layer_sizes: Hidden layer sizes in the MLP\n",
    "                following the skip-concatenation of the outputs of each\n",
    "                `DynEdgeConv` layer. Defaults to [336, 256].\n",
    "            readout_layer_sizes: Hidden layer sizes in the MLP following the\n",
    "                post-processing _and_ optional global pooling. As this is the\n",
    "                last layer(s) in the model, the last layer in the read-out\n",
    "                yields the output of the `DynEdge` model. Defaults to [128,].\n",
    "            global_pooling_schemes: The list global pooling schemes to use.\n",
    "                Options are: \"min\", \"max\", \"mean\", and \"sum\".\n",
    "            add_global_variables_after_pooling: Whether to add global variables\n",
    "                after global pooling. The alternative is to  added (distribute)\n",
    "                them to the individual nodes before any convolutional\n",
    "                operations.\n",
    "        \"\"\"\n",
    "        # Latent feature subset for computing nearest neighbours in DynEdge.\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(0, 3)\n",
    "\n",
    "        # DynEdge layer sizes\n",
    "        if dynedge_layer_sizes is None:\n",
    "            dynedge_layer_sizes = [\n",
    "                (\n",
    "                    128,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        assert isinstance(dynedge_layer_sizes, list)\n",
    "        assert len(dynedge_layer_sizes)\n",
    "        assert all(isinstance(sizes, tuple) for sizes in dynedge_layer_sizes)\n",
    "        assert all(len(sizes) > 0 for sizes in dynedge_layer_sizes)\n",
    "        assert all(all(size > 0 for size in sizes) for sizes in dynedge_layer_sizes)\n",
    "\n",
    "        self._dynedge_layer_sizes = dynedge_layer_sizes\n",
    "\n",
    "        # Post-processing layer sizes\n",
    "        if post_processing_layer_sizes is None:\n",
    "            post_processing_layer_sizes = [\n",
    "                336,\n",
    "                256,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(post_processing_layer_sizes, list)\n",
    "        assert len(post_processing_layer_sizes)\n",
    "        assert all(size > 0 for size in post_processing_layer_sizes)\n",
    "\n",
    "        self._post_processing_layer_sizes = post_processing_layer_sizes\n",
    "\n",
    "        # Read-out layer sizes\n",
    "        if readout_layer_sizes is None:\n",
    "            readout_layer_sizes = [\n",
    "                128,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(readout_layer_sizes, list)\n",
    "        assert len(readout_layer_sizes)\n",
    "        assert all(size > 0 for size in readout_layer_sizes)\n",
    "\n",
    "        self._readout_layer_sizes = readout_layer_sizes\n",
    "\n",
    "        # Global pooling scheme(s)\n",
    "        if isinstance(global_pooling_schemes, str):\n",
    "            global_pooling_schemes = [global_pooling_schemes]\n",
    "\n",
    "        if isinstance(global_pooling_schemes, list):\n",
    "            for pooling_scheme in global_pooling_schemes:\n",
    "                assert (\n",
    "                    pooling_scheme in GLOBAL_POOLINGS\n",
    "                ), f\"Global pooling scheme {pooling_scheme} not supported.\"\n",
    "        else:\n",
    "            assert global_pooling_schemes is None\n",
    "\n",
    "        self._global_pooling_schemes = global_pooling_schemes\n",
    "\n",
    "        if add_global_variables_after_pooling:\n",
    "            assert self._global_pooling_schemes, (\n",
    "                \"No global pooling schemes were request, so cannot add global\"\n",
    "                \" variables after pooling.\"\n",
    "            )\n",
    "        self._add_global_variables_after_pooling = add_global_variables_after_pooling\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nb_inputs, self._readout_layer_sizes[-1])\n",
    "\n",
    "        # Remaining member variables()\n",
    "        self._activation = torch.nn.GELU()\n",
    "        self._nb_inputs = nb_inputs\n",
    "        self._nb_global_variables = 5 + nb_inputs\n",
    "        self._nb_neighbours = nb_neighbours\n",
    "        self._features_subset = features_subset\n",
    "\n",
    "        self._construct_layers()\n",
    "\n",
    "    def _construct_layers(self) -> None:\n",
    "        \"\"\"Construct layers (torch.nn.Modules).\"\"\"\n",
    "        # Convolutional operations\n",
    "        nb_input_features = self._nb_inputs\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            nb_input_features += self._nb_global_variables\n",
    "\n",
    "        self._conv_layers = torch.nn.ModuleList()\n",
    "        nb_latent_features = nb_input_features\n",
    "        for sizes in self._dynedge_layer_sizes:\n",
    "            layers = []\n",
    "            layer_sizes = [nb_latent_features] + list(sizes)\n",
    "            for ix, (nb_in, nb_out) in enumerate(\n",
    "                zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "            ):\n",
    "                if ix == 0:\n",
    "                    nb_in *= 2\n",
    "                layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "                layers.append(nn.BatchNorm1d(nb_out))\n",
    "                layers.append(self._activation)\n",
    "\n",
    "            conv_layer = DynEdgeConv(\n",
    "                torch.nn.Sequential(*layers),\n",
    "                aggr=\"add\",\n",
    "                nb_neighbors=self._nb_neighbours,\n",
    "                features_subset=self._features_subset,\n",
    "            )\n",
    "            self._conv_layers.append(conv_layer)\n",
    "\n",
    "            nb_latent_features = nb_out\n",
    "\n",
    "        # Post-processing operations\n",
    "        nb_latent_features = (\n",
    "            sum(sizes[-1] for sizes in self._dynedge_layer_sizes) + nb_input_features\n",
    "        )\n",
    "\n",
    "        post_processing_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._post_processing_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            post_processing_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            post_processing_layers.append(nn.BatchNorm1d(nb_out))\n",
    "            post_processing_layers.append(self._activation)\n",
    "\n",
    "        self._post_processing = torch.nn.Sequential(*post_processing_layers)\n",
    "\n",
    "        # Read-out operations\n",
    "        nb_poolings = (\n",
    "            len(self._global_pooling_schemes) if self._global_pooling_schemes else 1\n",
    "        )\n",
    "        nb_latent_features = nb_out * nb_poolings\n",
    "        if self._add_global_variables_after_pooling:\n",
    "            nb_latent_features += self._nb_global_variables\n",
    "\n",
    "        readout_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._readout_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            readout_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            readout_layers.append(nn.BatchNorm1d(nb_out))\n",
    "            readout_layers.append(self._activation)\n",
    "\n",
    "        self._readout = torch.nn.Sequential(*readout_layers)\n",
    "\n",
    "    def _global_pooling(self, x: Tensor, batch: LongTensor) -> Tensor:\n",
    "        \"\"\"Perform global pooling.\"\"\"\n",
    "        assert self._global_pooling_schemes\n",
    "        pooled = []\n",
    "        for pooling_scheme in self._global_pooling_schemes:\n",
    "            pooling_fn = GLOBAL_POOLINGS[pooling_scheme]\n",
    "            pooled_x = pooling_fn(x, index=batch, dim=0)\n",
    "            if isinstance(pooled_x, tuple) and len(pooled_x) == 2:\n",
    "                # `scatter_{min,max}`, which return also an argument, vs.\n",
    "                # `scatter_{mean,sum}`\n",
    "                pooled_x, _ = pooled_x\n",
    "            pooled.append(pooled_x)\n",
    "\n",
    "        return torch.cat(pooled, dim=1)\n",
    "\n",
    "    def _calculate_global_variables(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: LongTensor,\n",
    "        batch: LongTensor,\n",
    "        *additional_attributes: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Calculate global variables.\"\"\"\n",
    "        # Calculate homophily (scalar variables)\n",
    "        h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n",
    "\n",
    "        # Calculate mean features\n",
    "        global_means = scatter_mean(x, batch, dim=0)\n",
    "\n",
    "        # Add global variables\n",
    "        global_variables = torch.cat(\n",
    "            [\n",
    "                global_means,\n",
    "                h_x,\n",
    "                h_y,\n",
    "                h_z,\n",
    "                h_t,\n",
    "            ]\n",
    "            + [attr.unsqueeze(dim=1) for attr in additional_attributes],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return global_variables\n",
    "\n",
    "    def forward(self, data: Data) -> Tensor:\n",
    "        \"\"\"Apply learnable forward pass.\"\"\"\n",
    "        # Convenience variables\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        global_variables = self._calculate_global_variables(\n",
    "            x,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            torch.log10(data.n_pulses),\n",
    "        )\n",
    "\n",
    "        # Distribute global variables out to each node\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            distribute = (\n",
    "                batch.unsqueeze(dim=1) == torch.unique(batch).unsqueeze(dim=0)\n",
    "            ).type(torch.float)\n",
    "\n",
    "            global_variables_distributed = torch.sum(\n",
    "                distribute.unsqueeze(dim=2) * global_variables.unsqueeze(dim=0),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            x = torch.cat((x, global_variables_distributed), dim=1)\n",
    "\n",
    "        # DynEdge-convolutions\n",
    "        skip_connections = [x]\n",
    "        for conv_layer in self._conv_layers:\n",
    "            x, edge_index = conv_layer(x, edge_index, batch)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        # Skip-cat\n",
    "        x = torch.cat(skip_connections, dim=1)\n",
    "\n",
    "        return x, edge_index, batch\n",
    "    \n",
    "class DynEdgeV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = DynEdge(\n",
    "            nb_inputs=9,\n",
    "            nb_neighbours=8,\n",
    "            global_pooling_schemes=[\"min\", \"max\", \"mean\", \"sum\"],\n",
    "            features_subset=slice(0, 4),  # NN search using xyzt\n",
    "        )\n",
    "\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=self.encoder.nb_outputs,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.encoder(batch)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class DynEdgeV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = DynEdge(\n",
    "            nb_inputs=9,\n",
    "            nb_neighbours=8,\n",
    "            global_pooling_schemes=[\"min\", \"max\", \"mean\", \"sum\"],\n",
    "            features_subset=slice(0, 3),  # NN search using xyz3\n",
    "        )\n",
    "\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=self.encoder.nb_outputs,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.encoder(batch)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EGNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"E(n) Equivariant GNN Layer\n",
    "        Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim + 1, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\psi_x` for computing messages `\\overrightarrow{m}_ij`\n",
    "        self.mlp_pos = Sequential(\n",
    "            Linear(emb_dim, emb_dim), self.norm(emb_dim), self.activation, Linear(emb_dim, 1)\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, pos_i, pos_j):\n",
    "        # Compute messages\n",
    "        pos_diff = pos_i - pos_j\n",
    "        dists = torch.norm(pos_diff, dim=-1).unsqueeze(1)\n",
    "        msg = torch.cat([h_i, h_j, dists], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        # Scale magnitude of displacement vector\n",
    "        pos_diff = pos_diff * self.mlp_pos(msg)  # torch.clamp(updates, min=-100, max=100)\n",
    "        return msg, pos_diff\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        msgs, pos_diffs = inputs\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(msgs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        pos_aggr = scatter(pos_diffs, index, dim=self.node_dim, reduce=\"mean\")\n",
    "        return msg_aggr, pos_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, pos):\n",
    "        msg_aggr, pos_aggr = aggr_out\n",
    "        upd_out = self.mlp_upd(torch.cat([h, msg_aggr], dim=-1))\n",
    "        upd_pos = pos + pos_aggr\n",
    "        return upd_out, upd_pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\"\n",
    "    \n",
    "class EGNNLayerKNN(MessagePassing):\n",
    "    def __init__(self, emb_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"E(n) Equivariant GNN Layer\n",
    "        Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim + 1, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\psi_x` for computing messages `\\overrightarrow{m}_ij`\n",
    "        self.mlp_pos = Sequential(\n",
    "            Linear(emb_dim, emb_dim), self.norm(emb_dim), self.activation, Linear(emb_dim, 1)\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos)\n",
    "        dev = pos.device\n",
    "\n",
    "        # Recompute adjacency\n",
    "        edge_index = knn_graph(\n",
    "            x=out[0][:, slice(0, 3)],\n",
    "            k=9,\n",
    "            batch=batch,\n",
    "        ).to(dev)\n",
    "\n",
    "        return (*out, edge_index)\n",
    "\n",
    "    def message(self, h_i, h_j, pos_i, pos_j):\n",
    "        # Compute messages\n",
    "        pos_diff = pos_i - pos_j\n",
    "        dists = torch.norm(pos_diff, dim=-1).unsqueeze(1)\n",
    "        msg = torch.cat([h_i, h_j, dists], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        # Scale magnitude of displacement vector\n",
    "        pos_diff = pos_diff * self.mlp_pos(msg)  # torch.clamp(updates, min=-100, max=100)\n",
    "        return msg, pos_diff\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        msgs, pos_diffs = inputs\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(msgs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        pos_aggr = scatter(pos_diffs, index, dim=self.node_dim, reduce=\"mean\")\n",
    "        return msg_aggr, pos_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, pos):\n",
    "        msg_aggr, pos_aggr = aggr_out\n",
    "        upd_out = self.mlp_upd(torch.cat([h, msg_aggr], dim=-1))\n",
    "        upd_pos = pos + pos_aggr\n",
    "        return upd_out, upd_pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\"\n",
    "    \n",
    "    \n",
    "class EGNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=9,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "    \n",
    "class EGNNModelV2(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=6,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.x[:, 3:])  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    \n",
    "class EGNNModelV3(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=9,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        _global_pooling_schemes=[\"sum\", \"mean\", \"max\", \"min\"],\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "\n",
    "\n",
    "\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim * 4,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "        self._global_pooling_schemes = _global_pooling_schemes\n",
    "        \n",
    "        \n",
    "    def _global_pooling(self, x: Tensor, batch: LongTensor) -> Tensor:\n",
    "        \"\"\"Perform global pooling.\"\"\"\n",
    "        assert self._global_pooling_schemes\n",
    "        pooled = []\n",
    "        for pooling_scheme in self._global_pooling_schemes:\n",
    "            pooling_fn = GLOBAL_POOLINGS[pooling_scheme]\n",
    "            pooled_x = pooling_fn(x, index=batch, dim=0)\n",
    "            if isinstance(pooled_x, tuple) and len(pooled_x) == 2:\n",
    "                # `scatter_{min,max}`, which return also an argument, vs.\n",
    "                # `scatter_{mean,sum}`\n",
    "                pooled_x, _ = pooled_x\n",
    "            pooled.append(pooled_x)\n",
    "\n",
    "        return torch.cat(pooled, dim=1)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self._global_pooling(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "\n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "    \n",
    "class EGNNModelV4(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=9,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim + 14, emb_dim)\n",
    "        self._add_global_variables_after_pooling = False\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "        \n",
    "    def _calculate_global_variables(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: LongTensor,\n",
    "        batch: LongTensor,\n",
    "        *additional_attributes: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Calculate global variables.\"\"\"\n",
    "        # Calculate homophily (scalar variables)\n",
    "        h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n",
    "\n",
    "        # Calculate mean features\n",
    "        global_means = scatter_mean(x, batch, dim=0)\n",
    "\n",
    "        # Add global variables\n",
    "        global_variables = torch.cat(\n",
    "            [\n",
    "                global_means,\n",
    "                h_x,\n",
    "                h_y,\n",
    "                h_z,\n",
    "                h_t,\n",
    "            ]\n",
    "            + [attr.unsqueeze(dim=1) for attr in additional_attributes],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return global_variables\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, pos = data.x, data.edge_index, data.batch, data.pos\n",
    "        global_variables = self._calculate_global_variables(\n",
    "            x,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            torch.log10(data.n_pulses),\n",
    "        )\n",
    "        #print(global_variables.shape)\n",
    "       # Distribute global variables out to each node\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            distribute = (\n",
    "                batch.unsqueeze(dim=1) == torch.unique(batch).unsqueeze(dim=0)\n",
    "            ).type(torch.float)\n",
    "\n",
    "            global_variables_distributed = torch.sum(\n",
    "                distribute.unsqueeze(dim=2) * global_variables.unsqueeze(dim=0),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            x = torch.cat((x, global_variables_distributed), dim=1)\n",
    "            \n",
    "        h = self.emb_in(x)  # (n,) -> (n, d)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class EGNNModelV5(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=9,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim + 14, emb_dim)\n",
    "        self._add_global_variables_after_pooling = False\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "        \n",
    "    def _calculate_global_variables(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: LongTensor,\n",
    "        batch: LongTensor,\n",
    "        *additional_attributes: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Calculate global variables.\"\"\"\n",
    "        # Calculate homophily (scalar variables)\n",
    "        h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n",
    "\n",
    "        # Calculate mean features\n",
    "        global_means = scatter_mean(x, batch, dim=0)\n",
    "\n",
    "        # Add global variables\n",
    "        global_variables = torch.cat(\n",
    "            [\n",
    "                global_means,\n",
    "                h_x,\n",
    "                h_y,\n",
    "                h_z,\n",
    "                h_t,\n",
    "            ]\n",
    "            + [attr.unsqueeze(dim=1) for attr in additional_attributes],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return global_variables\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch, pos = data.x, data.edge_index, data.batch, data.pos\n",
    "        global_variables = self._calculate_global_variables(\n",
    "            x,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            torch.log10(data.n_pulses),\n",
    "        )\n",
    "        #print(global_variables.shape)\n",
    "       # Distribute global variables out to each node\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            distribute = (\n",
    "                batch.unsqueeze(dim=1) == torch.unique(batch).unsqueeze(dim=0)\n",
    "            ).type(torch.float)\n",
    "\n",
    "            global_variables_distributed = torch.sum(\n",
    "                distribute.unsqueeze(dim=2) * global_variables.unsqueeze(dim=0),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            x = torch.cat((x, global_variables_distributed), dim=1)\n",
    "            \n",
    "        h = self.emb_in(x)  # (n,) -> (n, d)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "    \n",
    "class EGNNModelV6(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_inputs=9,\n",
    "        num_layers=5,\n",
    "        emb_dim=279,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = DynEdgeFEXTRACTRO(nb_inputs=nb_inputs,\n",
    "            nb_neighbours=8,\n",
    "            global_pooling_schemes=[\"min\", \"max\", \"mean\", \"sum\"],\n",
    "            features_subset=slice(0, 4),\n",
    "            dynedge_layer_sizes=[(128, 256)])\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = data.pos\n",
    "        h, edge_index, batch = self.emb_in(data)  # (n,) -> (n, d)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "    \n",
    "class TokenEmbeddingV2(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "    \n",
    "class EGNNModelV7(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=6,\n",
    "        activation=\"swish\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        added embedding for events\n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim, emb_dim)\n",
    "        self.sensor_id = TokenEmbeddingV2((emb_dim)//4, num_tokens=5161)\n",
    "        emb_dim = emb_dim + (emb_dim)//4\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.x[:, 3:])  # (n,) -> (n, d)\n",
    "        event = self.sensor_id(batch.sensor_id)\n",
    "        h = torch.cat([h, event], dim=1)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "    \n",
    "class EGNNModelV8(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=9,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = nn.Linear(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayerKNN(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.postpool = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "        edge_index = batch.edge_index\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update, edge_index = conv(h, pos, edge_index, batch.batch)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        out = self.postpool(out) \n",
    "        out = self.out(out)  # (batch_size, d) -> (batch_size, 1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "from torch_geometric.utils import to_dense_batch\n",
    "class PoolingWithMask(nn.Module):\n",
    "    def __init__(self, pool_type):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        if self.pool_type == \"mean\":\n",
    "            # Sum the values along the sequence dimension\n",
    "            x = torch.sum(x, dim=1)\n",
    "\n",
    "            # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "            x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "        elif self.pool_type == \"max\":\n",
    "            # Find the maximum value along the sequence dimension\n",
    "            x, _ = torch.max(x, dim=1)\n",
    "        elif self.pool_type == \"min\":\n",
    "            # Find the minimum value along the sequence dimension\n",
    "            x, _ = torch.min(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pool_type. Choose from ['mean', 'max', 'min']\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionX(nn.Module):\n",
    "    def __init__(self, dim_in=151, dim_out=196, max_seq_len=150):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=dim_in,\n",
    "            dim_out=dim_out,\n",
    "            max_seq_len=max_seq_len,\n",
    "            post_emb_norm = True,\n",
    "            attn_layers=Encoder(dim=dim_out,\n",
    "                                depth=6,\n",
    "                                heads=8,\n",
    "                                ff_glu = True,\n",
    "                                rotary_pos_emb = True),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=dim_out * 2,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "class GraphxTransformerV0(torch.nn.Module):\n",
    "    def __init__(\n",
    "        \n",
    "        self,\n",
    "        nb_inputs=9\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = DynEdgeFEXTRACTRO(nb_inputs=nb_inputs,\n",
    "            nb_neighbours=8,\n",
    "            global_pooling_schemes=[\"min\", \"max\", \"mean\", \"sum\"],\n",
    "            features_subset=slice(0, 4),\n",
    "            dynedge_layer_sizes=[(128, 128)])\n",
    "        \n",
    "        self.transformer = EncoderWithDirectionReconstructionX()\n",
    "\n",
    "    def forward(self, data):\n",
    "        pos = data.pos\n",
    "        h, _, batch = self.emb_in(data)  # (n,) -> (n, d)\n",
    "        h, mask = to_dense_batch(h, batch)\n",
    "        h = self.transformer({\"event\": h, \"mask\": mask})\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = EGNNModelV8().eval()\n",
    "ds = GraphDasetV0(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = gDataLoader(ds, batch_size=10, shuffle=False)\n",
    "batch = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.7627,   0.4557,  -0.4590,  46.9633],\n",
       "        [ -0.7868,   0.3754,  -0.4898,  37.1862],\n",
       "        [ -0.8636,   0.4008,  -0.3059,  27.2965],\n",
       "        [ -0.8274,   0.4391,  -0.3500, 109.1340],\n",
       "        [ -0.7678,   0.4068,  -0.4950,  48.6662],\n",
       "        [ -0.7225,   0.4271,  -0.5437,  36.9482],\n",
       "        [ -0.8784,   0.4157,  -0.2357, 160.9553],\n",
       "        [ -0.6128,   0.6038,  -0.5098,  35.3256],\n",
       "        [ -0.7391,   0.4087,  -0.5354,  30.9799],\n",
       "        [ -0.7866,   0.3419,  -0.5141,  29.4356]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = GraphxTransformerV0().eval()\n",
    "ds = GraphDasetV3(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = gDataLoader(ds, batch_size=10, shuffle=False)\n",
    "batch = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"Vanilla Message Passing GNN layer\n",
    "        \n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: (n, d) - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j):\n",
    "        # Compute messages\n",
    "        msg = torch.cat([h_i, h_j], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        return msg\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        return msg_aggr\n",
    "\n",
    "    def update(self, aggr_out, h):\n",
    "        upd_out = self.mlp_upd(torch.cat([h, aggr_out], dim=-1))\n",
    "        return upd_out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\"\n",
    "    \n",
    "    \n",
    "class MPNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=9,\n",
    "        out_dim=4,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"Vanilla Message Passing GNN model\n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.Linear(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(MPNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.pred = DirectionReconstructionWithKappa(\n",
    "            hidden_size=emb_dim,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            # Message passing layer and residual connection\n",
    "            h = h + conv(h, batch.edge_index) if self.residual else conv(h, batch.edge_index)\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MPNNModel()\n",
    "ds = GraphDasetV0(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = gDataLoader(ds, batch_size=10, shuffle=False)\n",
    "batch = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNNModel(\n",
       "  (emb_in): Linear(in_features=9, out_features=128, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0): MPNNLayer(emb_dim=128, aggr=sum)\n",
       "    (1): MPNNLayer(emb_dim=128, aggr=sum)\n",
       "    (2): MPNNLayer(emb_dim=128, aggr=sum)\n",
       "    (3): MPNNLayer(emb_dim=128, aggr=sum)\n",
       "    (4): MPNNLayer(emb_dim=128, aggr=sum)\n",
       "  )\n",
       "  (pred): DirectionReconstructionWithKappa(\n",
       "    (_loss_function): VonMisesFisher3DLoss()\n",
       "    (_affine): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = EGNNModelV3().eval()\n",
    "ds = GraphDasetV0(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = gDataLoader(ds, batch_size=10, shuffle=False)\n",
    "batch = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import e3nn\n",
    "from e3nn import o3\n",
    "from e3nn import nn\n",
    "\n",
    "class BesselBasis(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Klicpera, J.; Gro, J.; Gnnemann, S. Directional Message Passing for Molecular Graphs; ICLR 2020.\n",
    "    Equation (7)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r_max: float, num_basis=8, trainable=False):\n",
    "        super().__init__()\n",
    "\n",
    "        bessel_weights = (\n",
    "            np.pi\n",
    "            / r_max\n",
    "            * torch.linspace(\n",
    "                start=1.0,\n",
    "                end=num_basis,\n",
    "                steps=num_basis,\n",
    "                dtype=torch.get_default_dtype(),\n",
    "            )\n",
    "        )\n",
    "        if trainable:\n",
    "            self.bessel_weights = torch.nn.Parameter(bessel_weights)\n",
    "        else:\n",
    "            self.register_buffer(\"bessel_weights\", bessel_weights)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"r_max\", torch.tensor(r_max, dtype=torch.get_default_dtype())\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"prefactor\",\n",
    "            torch.tensor(np.sqrt(2.0 / r_max), dtype=torch.get_default_dtype()),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor,) -> torch.Tensor:  # [..., 1]\n",
    "        numerator = torch.sin(self.bessel_weights * x)  # [..., num_basis]\n",
    "        return self.prefactor * (numerator / x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(r_max={self.r_max}, num_basis={len(self.bessel_weights)}, \"\n",
    "            f\"trainable={self.bessel_weights.requires_grad})\"\n",
    "        )\n",
    "\n",
    "def irreps2gate(irreps):\n",
    "    irreps_scalars = []\n",
    "    irreps_gated = []\n",
    "    for mul, ir in irreps:\n",
    "        if ir.l == 0 and ir.p == 1:\n",
    "            irreps_scalars.append((mul, ir))\n",
    "        else:\n",
    "            irreps_gated.append((mul, ir))\n",
    "    irreps_scalars = o3.Irreps(irreps_scalars).simplify()\n",
    "    irreps_gated = o3.Irreps(irreps_gated).simplify()\n",
    "    if irreps_gated.dim > 0:\n",
    "        ir = '0e'\n",
    "    else:\n",
    "        ir = None\n",
    "    irreps_gates = o3.Irreps([(mul, ir) for mul, _ in irreps_gated]).simplify()\n",
    "    return irreps_scalars, irreps_gates, irreps_gated\n",
    "\n",
    "\n",
    "class PolynomialCutoff(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Klicpera, J.; Gro, J.; Gnnemann, S. Directional Message Passing for Molecular Graphs; ICLR 2020.\n",
    "    Equation (8)\n",
    "    \"\"\"\n",
    "\n",
    "    p: torch.Tensor\n",
    "    r_max: torch.Tensor\n",
    "\n",
    "    def __init__(self, r_max: float, p=6):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"p\", torch.tensor(p, dtype=torch.get_default_dtype()))\n",
    "        self.register_buffer(\n",
    "            \"r_max\", torch.tensor(r_max, dtype=torch.get_default_dtype())\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # yapf: disable\n",
    "        envelope = (\n",
    "                1.0\n",
    "                - ((self.p + 1.0) * (self.p + 2.0) / 2.0) * torch.pow(x / self.r_max, self.p)\n",
    "                + self.p * (self.p + 2.0) * torch.pow(x / self.r_max, self.p + 1)\n",
    "                - (self.p * (self.p + 1.0) / 2) * torch.pow(x / self.r_max, self.p + 2)\n",
    "        )\n",
    "        # yapf: enable\n",
    "\n",
    "        # noinspection PyUnresolvedReferences\n",
    "        return envelope * (x < self.r_max).type(torch.get_default_dtype())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(p={self.p}, r_max={self.r_max})\"\n",
    "\n",
    "\n",
    "class TensorProductConvLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_irreps,  \n",
    "        out_irreps,\n",
    "        sh_irreps,\n",
    "        edge_feats_dim, \n",
    "        hidden_dim,\n",
    "        aggr=\"add\",\n",
    "        batch_norm=False,\n",
    "        gate=True\n",
    "    ):\n",
    "        \"\"\"Tensor Field Network GNN Layer\n",
    "        \n",
    "        Implements a Tensor Field Network equivariant GNN layer for higher-order tensors, using e3nn.\n",
    "        Implementation adapted from: https://github.com/gcorso/DiffDock/\n",
    "        Paper: Tensor Field Networks, Thomas, Smidt et al.\n",
    "        Args:\n",
    "            in_irreps: (e3nn.o3.Irreps) Input irreps dimensions\n",
    "            out_irreps: (e3nn.o3.Irreps) Output irreps dimensions\n",
    "            sh_irreps: (e3nn.o3.Irreps) Spherical harmonic irreps dimensions\n",
    "            edge_feats_dim: (int) Edge feature dimensions\n",
    "            hidden_dim: (int) Hidden dimension of MLP for computing tensor product weights\n",
    "            aggr: (str) Message passing aggregator\n",
    "            batch_norm: (bool) Whether to apply equivariant batch norm\n",
    "            gate: (bool) Whether to apply gated non-linearity\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_irreps = in_irreps\n",
    "        self.out_irreps = out_irreps\n",
    "        self.sh_irreps = sh_irreps\n",
    "        self.edge_feats_dim = edge_feats_dim\n",
    "        self.aggr = aggr\n",
    "\n",
    "        if gate:\n",
    "            # Optionally apply gated non-linearity\n",
    "            irreps_scalars, irreps_gates, irreps_gated = irreps2gate(o3.Irreps(out_irreps))\n",
    "            act_scalars =  [torch.nn.functional.silu for _, ir in irreps_scalars]\n",
    "            act_gates = [torch.sigmoid for _, ir in irreps_gates]\n",
    "            if irreps_gated.num_irreps == 0:\n",
    "                self.gate = nn.Activation(out_irreps, acts=[torch.nn.functional.silu])\n",
    "            else:\n",
    "                self.gate = nn.Gate(\n",
    "                    irreps_scalars, act_scalars,  # scalar\n",
    "                    irreps_gates, act_gates,  # gates (scalars)\n",
    "                    irreps_gated  # gated tensors\n",
    "                )\n",
    "                # Output irreps for the tensor product must be updated\n",
    "                self.out_irreps = out_irreps = self.gate.irreps_in\n",
    "        else:\n",
    "            self.gate = None\n",
    "\n",
    "        # Tensor product over edges to construct messages\n",
    "        self.tp = o3.FullyConnectedTensorProduct(in_irreps, sh_irreps, out_irreps, shared_weights=False)\n",
    "\n",
    "        # MLP used to compute weights of tensor product\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(edge_feats_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, self.tp.weight_numel)\n",
    "        )\n",
    "\n",
    "        # Optional equivariant batch norm\n",
    "        self.batch_norm = nn.BatchNorm(out_irreps) if batch_norm else None\n",
    "\n",
    "    def forward(self, node_attr, edge_index, edge_attr, edge_feat):\n",
    "        src, dst = edge_index\n",
    "        # Compute messages \n",
    "        tp = self.tp(node_attr[dst], edge_attr, self.fc(edge_feat))\n",
    "        # Aggregate messages\n",
    "        out = scatter(tp, src, dim=0, reduce=self.aggr)\n",
    "        # Optionally apply gated non-linearity and/or batch norm\n",
    "        if self.gate:\n",
    "            out = self.gate(out)\n",
    "        if self.batch_norm:\n",
    "            out = self.batch_norm(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class RadialEmbeddingBlock(torch.nn.Module):\n",
    "    def __init__(self, r_max: float, num_bessel: int, num_polynomial_cutoff: int):\n",
    "        super().__init__()\n",
    "        self.bessel_fn = BesselBasis(r_max=r_max, num_basis=num_bessel)\n",
    "        self.cutoff_fn = PolynomialCutoff(r_max=r_max, p=num_polynomial_cutoff)\n",
    "        self.out_dim = num_bessel\n",
    "\n",
    "    def forward(\n",
    "        self, edge_lengths: torch.Tensor,  # [n_edges, 1]\n",
    "    ):\n",
    "        bessel = self.bessel_fn(edge_lengths)  # [n_edges, n_basis]\n",
    "        cutoff = self.cutoff_fn(edge_lengths)  # [n_edges, 1]\n",
    "        return bessel * cutoff  # [n_edges, n_basis]\n",
    "    \n",
    "class TFNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        r_max=10.0,\n",
    "        num_bessel=8,\n",
    "        num_polynomial_cutoff=5,\n",
    "        max_ell=2,\n",
    "        num_layers=5,\n",
    "        emb_dim=64,\n",
    "        in_dim=9,\n",
    "        out_dim=1,\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True,\n",
    "        scalar_pred=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.r_max = r_max\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.residual = residual\n",
    "        self.scalar_pred = scalar_pred\n",
    "        # Embedding\n",
    "        self.radial_embedding = RadialEmbeddingBlock(\n",
    "            r_max=r_max,\n",
    "            num_bessel=num_bessel,\n",
    "            num_polynomial_cutoff=num_polynomial_cutoff,\n",
    "        )\n",
    "        sh_irreps = o3.Irreps.spherical_harmonics(max_ell)\n",
    "        self.spherical_harmonics = o3.SphericalHarmonics(\n",
    "            sh_irreps, normalize=True, normalization=\"component\"\n",
    "        )\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.Linear(in_dim, emb_dim)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        hidden_irreps = (sh_irreps * emb_dim).sort()[0].simplify()\n",
    "        irrep_seq = [\n",
    "            o3.Irreps(f'{emb_dim}x0e'),\n",
    "            # o3.Irreps(f'{emb_dim}x0e + {emb_dim}x1o + {emb_dim}x2e'),\n",
    "            # o3.Irreps(f'{emb_dim//2}x0e + {emb_dim//2}x0o + {emb_dim//2}x1e + {emb_dim//2}x1o + {emb_dim//2}x2e + {emb_dim//2}x2o'),\n",
    "            hidden_irreps\n",
    "        ]\n",
    "        for i in range(num_layers):\n",
    "            in_irreps = irrep_seq[min(i, len(irrep_seq) - 1)]\n",
    "            out_irreps = irrep_seq[min(i + 1, len(irrep_seq) - 1)]\n",
    "            conv = TensorProductConvLayer(\n",
    "                in_irreps=in_irreps,\n",
    "                out_irreps=out_irreps,\n",
    "                sh_irreps=sh_irreps,\n",
    "                edge_feats_dim=self.radial_embedding.out_dim,\n",
    "                hidden_dim=emb_dim,\n",
    "                gate=True,\n",
    "                aggr=aggr,\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        if self.scalar_pred:\n",
    "            # Predictor MLP\n",
    "            self.pred = torch.nn.Sequential(\n",
    "                torch.nn.Linear(emb_dim, emb_dim),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(emb_dim, out_dim)\n",
    "            )\n",
    "        else:\n",
    "            self.pred = torch.nn.Linear(hidden_irreps.dim, out_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        h = self.emb_in(batch.x)  # (n,) -> (n, d)\n",
    "\n",
    "        # Edge features\n",
    "        vectors = batch.pos[batch.edge_index[0]] - batch.pos[batch.edge_index[1]]  # [n_edges, 3]\n",
    "        lengths = torch.linalg.norm(vectors, dim=-1, keepdim=True)  # [n_edges, 1]\n",
    "        edge_attrs = self.spherical_harmonics(vectors)\n",
    "        edge_feats = self.radial_embedding(lengths)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update = conv(h, batch.edge_index, edge_attrs, edge_feats)\n",
    "\n",
    "            # Update node features\n",
    "            h = h_update + F.pad(h, (0, h_update.shape[-1] - h.shape[-1])) if self.residual else h_update\n",
    "\n",
    "        if self.scalar_pred:\n",
    "            # Select only scalars for prediction\n",
    "            h = h[:,:self.emb_dim]\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TFNModel().eval()\n",
    "ds = GraphDasetV0(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = gDataLoader(ds, batch_size=10, shuffle=False)\n",
    "batch = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
