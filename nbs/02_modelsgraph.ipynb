{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp modelsgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/archive/software/graphnet/src')\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn\n",
    "from graphnet.models.task.reconstruction import (\n",
    "    DirectionReconstructionWithKappa,\n",
    "    AzimuthReconstructionWithKappa,\n",
    "    ZenithReconstruction,\n",
    ")\n",
    "\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss,  VonMisesFisher2DLoss, EuclideanDistanceLoss\n",
    "from graphnet.models.gnn.gnn import GNN\n",
    "from graphnet.models.utils import calculate_xyzt_homophily\n",
    "from graphnet.utilities.config import save_model_config\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import EdgeConv\n",
    "from torch_geometric.nn.pool import knn_graph\n",
    "from torch_geometric.typing import Adj\n",
    "from torch_scatter import scatter_max, scatter_mean, scatter_min, scatter_sum\n",
    "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n",
    "from torch import Tensor, LongTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('/opt/slh/icecube/')\n",
    "from icecube.graphdataset import GraphDasetV0\n",
    "from datasets import  load_from_disk\n",
    "from torch_geometric.loader import DataLoader as gDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "    \n",
    "loss_fn_azi = VonMisesFisher2DLoss()\n",
    "loss_fn_zen = nn.L1Loss()\n",
    "\n",
    "class CombineLossV0(nn.Module):\n",
    "    def __init__(self, loss_fn_azi=loss_fn_azi, loss_fn_zen=loss_fn_zen):\n",
    "        super().__init__()\n",
    "        self.loss_fn_azi = loss_fn_azi\n",
    "        self.loss_fn_zen = loss_fn_zen\n",
    "        \n",
    "    def forward(self, batch, output):\n",
    "        target = batch['label']\n",
    "        azi_pred, zen_pred = output.split(2, 1)\n",
    "        azi_loss = self.loss_fn_azi(azi_pred, target)\n",
    "        zen_loss = self.loss_fn_zen(zen_pred, target[:, -1].unsqueeze(-1))\n",
    "        return azi_loss + zen_loss\n",
    "\n",
    "    \n",
    "class EncoderWithReconstructionLossV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.az = AzimuthReconstructionWithKappa(\n",
    "            hidden_size=128,\n",
    "            loss_function=loss_fn_azi,\n",
    "            target_labels=[\"azimuth\", \"kappa\"],\n",
    "        )\n",
    "        self.zn =  ZenithReconstruction(\n",
    "            hidden_size=128,\n",
    "            loss_function=loss_fn_zen,\n",
    "            target_labels=[\"zenith\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch['event'], batch['mask']\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        az = self.az(x)\n",
    "        zn = self.zn(x)\n",
    "        return torch.concat([az, zn], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderWithReconstructionLossV0().eval()\n",
    "event = torch.rand(10, 100, 8)\n",
    "mask = torch.ones(10, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (10, 100))\n",
    "label = torch.rand(10, 2)\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id, label=label)\n",
    "with torch.no_grad():\n",
    "    y = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class gVonMisesFisher3DLossEcludeLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = EuclideanDistanceLoss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_true = y_true.reshape(-1, 3)\n",
    "        return (self.vonmis(y_pred, y_true) + self.cosine(y_pred[:, :3], y_true))/2\n",
    "    \n",
    "    \n",
    "GLOBAL_POOLINGS = {\n",
    "    \"min\": scatter_min,\n",
    "    \"max\": scatter_max,\n",
    "    \"sum\": scatter_sum,\n",
    "    \"mean\": scatter_mean,\n",
    "}\n",
    "\n",
    "class DynEdgeConv(EdgeConv):\n",
    "    \"\"\"Dynamical edge convolution layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn: Callable,\n",
    "        aggr: str = \"max\",\n",
    "        nb_neighbors: int = 8,\n",
    "        features_subset: Optional[Union[Sequence[int], slice]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdgeConv`.\n",
    "        Args:\n",
    "            nn: The MLP/torch.Module to be used within the `EdgeConv`.\n",
    "            aggr: Aggregation method to be used with `EdgeConv`.\n",
    "            nb_neighbors: Number of neighbours to be clustered after the\n",
    "                `EdgeConv` operation.\n",
    "            features_subset: Subset of features in `Data.x` that should be used\n",
    "                when dynamically performing the new graph clustering after the\n",
    "                `EdgeConv` operation. Defaults to all features.\n",
    "            **kwargs: Additional features to be passed to `EdgeConv`.\n",
    "        \"\"\"\n",
    "        # Check(s)\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(None)  # Use all features\n",
    "        assert isinstance(features_subset, (list, slice))\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nn=nn, aggr=aggr, **kwargs)\n",
    "\n",
    "        # Additional member variables\n",
    "        self.nb_neighbors = nb_neighbors\n",
    "        self.features_subset = features_subset\n",
    "\n",
    "    def forward(\n",
    "        self, x: Tensor, edge_index: Adj, batch: Optional[Tensor] = None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # Standard EdgeConv forward pass\n",
    "        x = super().forward(x, edge_index)\n",
    "        dev = x.device\n",
    "\n",
    "        # Recompute adjacency\n",
    "        edge_index = knn_graph(\n",
    "            x=x[:, self.features_subset],\n",
    "            k=self.nb_neighbors,\n",
    "            batch=batch,\n",
    "        ).to(dev)\n",
    "\n",
    "        return x, edge_index\n",
    "\n",
    "\n",
    "class DynEdge(GNN):\n",
    "    \"\"\"DynEdge (dynamical edge convolutional) model.\"\"\"\n",
    "\n",
    "    @save_model_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_inputs: int,\n",
    "        *,\n",
    "        nb_neighbours: int = 8,\n",
    "        features_subset: Optional[Union[List[int], slice]] = None,\n",
    "        dynedge_layer_sizes: Optional[List[Tuple[int, ...]]] = None,\n",
    "        post_processing_layer_sizes: Optional[List[int]] = None,\n",
    "        readout_layer_sizes: Optional[List[int]] = None,\n",
    "        global_pooling_schemes: Optional[Union[str, List[str]]] = None,\n",
    "        add_global_variables_after_pooling: bool = False,\n",
    "    ):\n",
    "        \"\"\"Construct `DynEdge`.\n",
    "        Args:\n",
    "            nb_inputs: Number of input features on each node.\n",
    "            nb_neighbours: Number of neighbours to used in the k-nearest\n",
    "                neighbour clustering which is performed after each (dynamical)\n",
    "                edge convolution.\n",
    "            features_subset: The subset of latent features on each node that\n",
    "                are used as metric dimensions when performing the k-nearest\n",
    "                neighbours clustering. Defaults to [0,1,2].\n",
    "            dynedge_layer_sizes: The layer sizes, or latent feature dimenions,\n",
    "                used in the `DynEdgeConv` layer. Each entry in\n",
    "                `dynedge_layer_sizes` corresponds to a single `DynEdgeConv`\n",
    "                layer; the integers in the corresponding tuple corresponds to\n",
    "                the layer sizes in the multi-layer perceptron (MLP) that is\n",
    "                applied within each `DynEdgeConv` layer. That is, a list of\n",
    "                size-two tuples means that all `DynEdgeConv` layers contain a\n",
    "                two-layer MLP.\n",
    "                Defaults to [(128, 256), (336, 256), (336, 256), (336, 256)].\n",
    "            post_processing_layer_sizes: Hidden layer sizes in the MLP\n",
    "                following the skip-concatenation of the outputs of each\n",
    "                `DynEdgeConv` layer. Defaults to [336, 256].\n",
    "            readout_layer_sizes: Hidden layer sizes in the MLP following the\n",
    "                post-processing _and_ optional global pooling. As this is the\n",
    "                last layer(s) in the model, the last layer in the read-out\n",
    "                yields the output of the `DynEdge` model. Defaults to [128,].\n",
    "            global_pooling_schemes: The list global pooling schemes to use.\n",
    "                Options are: \"min\", \"max\", \"mean\", and \"sum\".\n",
    "            add_global_variables_after_pooling: Whether to add global variables\n",
    "                after global pooling. The alternative is to  added (distribute)\n",
    "                them to the individual nodes before any convolutional\n",
    "                operations.\n",
    "        \"\"\"\n",
    "        # Latent feature subset for computing nearest neighbours in DynEdge.\n",
    "        if features_subset is None:\n",
    "            features_subset = slice(0, 3)\n",
    "\n",
    "        # DynEdge layer sizes\n",
    "        if dynedge_layer_sizes is None:\n",
    "            dynedge_layer_sizes = [\n",
    "                (\n",
    "                    128,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "                (\n",
    "                    336,\n",
    "                    256,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        assert isinstance(dynedge_layer_sizes, list)\n",
    "        assert len(dynedge_layer_sizes)\n",
    "        assert all(isinstance(sizes, tuple) for sizes in dynedge_layer_sizes)\n",
    "        assert all(len(sizes) > 0 for sizes in dynedge_layer_sizes)\n",
    "        assert all(all(size > 0 for size in sizes) for sizes in dynedge_layer_sizes)\n",
    "\n",
    "        self._dynedge_layer_sizes = dynedge_layer_sizes\n",
    "\n",
    "        # Post-processing layer sizes\n",
    "        if post_processing_layer_sizes is None:\n",
    "            post_processing_layer_sizes = [\n",
    "                336,\n",
    "                256,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(post_processing_layer_sizes, list)\n",
    "        assert len(post_processing_layer_sizes)\n",
    "        assert all(size > 0 for size in post_processing_layer_sizes)\n",
    "\n",
    "        self._post_processing_layer_sizes = post_processing_layer_sizes\n",
    "\n",
    "        # Read-out layer sizes\n",
    "        if readout_layer_sizes is None:\n",
    "            readout_layer_sizes = [\n",
    "                128,\n",
    "            ]\n",
    "\n",
    "        assert isinstance(readout_layer_sizes, list)\n",
    "        assert len(readout_layer_sizes)\n",
    "        assert all(size > 0 for size in readout_layer_sizes)\n",
    "\n",
    "        self._readout_layer_sizes = readout_layer_sizes\n",
    "\n",
    "        # Global pooling scheme(s)\n",
    "        if isinstance(global_pooling_schemes, str):\n",
    "            global_pooling_schemes = [global_pooling_schemes]\n",
    "\n",
    "        if isinstance(global_pooling_schemes, list):\n",
    "            for pooling_scheme in global_pooling_schemes:\n",
    "                assert (\n",
    "                    pooling_scheme in GLOBAL_POOLINGS\n",
    "                ), f\"Global pooling scheme {pooling_scheme} not supported.\"\n",
    "        else:\n",
    "            assert global_pooling_schemes is None\n",
    "\n",
    "        self._global_pooling_schemes = global_pooling_schemes\n",
    "\n",
    "        if add_global_variables_after_pooling:\n",
    "            assert self._global_pooling_schemes, (\n",
    "                \"No global pooling schemes were request, so cannot add global\"\n",
    "                \" variables after pooling.\"\n",
    "            )\n",
    "        self._add_global_variables_after_pooling = add_global_variables_after_pooling\n",
    "\n",
    "        # Base class constructor\n",
    "        super().__init__(nb_inputs, self._readout_layer_sizes[-1])\n",
    "\n",
    "        # Remaining member variables()\n",
    "        self._activation = torch.nn.GELU()\n",
    "        self._nb_inputs = nb_inputs\n",
    "        self._nb_global_variables = 5 + nb_inputs\n",
    "        self._nb_neighbours = nb_neighbours\n",
    "        self._features_subset = features_subset\n",
    "\n",
    "        self._construct_layers()\n",
    "\n",
    "    def _construct_layers(self) -> None:\n",
    "        \"\"\"Construct layers (torch.nn.Modules).\"\"\"\n",
    "        # Convolutional operations\n",
    "        nb_input_features = self._nb_inputs\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            nb_input_features += self._nb_global_variables\n",
    "\n",
    "        self._conv_layers = torch.nn.ModuleList()\n",
    "        nb_latent_features = nb_input_features\n",
    "        for sizes in self._dynedge_layer_sizes:\n",
    "            layers = []\n",
    "            layer_sizes = [nb_latent_features] + list(sizes)\n",
    "            for ix, (nb_in, nb_out) in enumerate(\n",
    "                zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "            ):\n",
    "                if ix == 0:\n",
    "                    nb_in *= 2\n",
    "                layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "                layers.append(nn.BatchNorm1d(nb_out))\n",
    "                layers.append(self._activation)\n",
    "\n",
    "            conv_layer = DynEdgeConv(\n",
    "                torch.nn.Sequential(*layers),\n",
    "                aggr=\"add\",\n",
    "                nb_neighbors=self._nb_neighbours,\n",
    "                features_subset=self._features_subset,\n",
    "            )\n",
    "            self._conv_layers.append(conv_layer)\n",
    "\n",
    "            nb_latent_features = nb_out\n",
    "\n",
    "        # Post-processing operations\n",
    "        nb_latent_features = (\n",
    "            sum(sizes[-1] for sizes in self._dynedge_layer_sizes) + nb_input_features\n",
    "        )\n",
    "\n",
    "        post_processing_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._post_processing_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            post_processing_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            post_processing_layers.append(nn.BatchNorm1d(nb_out))\n",
    "            post_processing_layers.append(self._activation)\n",
    "\n",
    "        self._post_processing = torch.nn.Sequential(*post_processing_layers)\n",
    "\n",
    "        # Read-out operations\n",
    "        nb_poolings = (\n",
    "            len(self._global_pooling_schemes) if self._global_pooling_schemes else 1\n",
    "        )\n",
    "        nb_latent_features = nb_out * nb_poolings\n",
    "        if self._add_global_variables_after_pooling:\n",
    "            nb_latent_features += self._nb_global_variables\n",
    "\n",
    "        readout_layers = []\n",
    "        layer_sizes = [nb_latent_features] + list(self._readout_layer_sizes)\n",
    "        for nb_in, nb_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            readout_layers.append(torch.nn.Linear(nb_in, nb_out))\n",
    "            readout_layers.append(nn.BatchNorm1d(nb_out))\n",
    "            readout_layers.append(self._activation)\n",
    "\n",
    "        self._readout = torch.nn.Sequential(*readout_layers)\n",
    "\n",
    "    def _global_pooling(self, x: Tensor, batch: LongTensor) -> Tensor:\n",
    "        \"\"\"Perform global pooling.\"\"\"\n",
    "        assert self._global_pooling_schemes\n",
    "        pooled = []\n",
    "        for pooling_scheme in self._global_pooling_schemes:\n",
    "            pooling_fn = GLOBAL_POOLINGS[pooling_scheme]\n",
    "            pooled_x = pooling_fn(x, index=batch, dim=0)\n",
    "            if isinstance(pooled_x, tuple) and len(pooled_x) == 2:\n",
    "                # `scatter_{min,max}`, which return also an argument, vs.\n",
    "                # `scatter_{mean,sum}`\n",
    "                pooled_x, _ = pooled_x\n",
    "            pooled.append(pooled_x)\n",
    "\n",
    "        return torch.cat(pooled, dim=1)\n",
    "\n",
    "    def _calculate_global_variables(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: LongTensor,\n",
    "        batch: LongTensor,\n",
    "        *additional_attributes: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Calculate global variables.\"\"\"\n",
    "        # Calculate homophily (scalar variables)\n",
    "        h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n",
    "\n",
    "        # Calculate mean features\n",
    "        global_means = scatter_mean(x, batch, dim=0)\n",
    "\n",
    "        # Add global variables\n",
    "        global_variables = torch.cat(\n",
    "            [\n",
    "                global_means,\n",
    "                h_x,\n",
    "                h_y,\n",
    "                h_z,\n",
    "                h_t,\n",
    "            ]\n",
    "            + [attr.unsqueeze(dim=1) for attr in additional_attributes],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return global_variables\n",
    "\n",
    "    def forward(self, data: Data) -> Tensor:\n",
    "        \"\"\"Apply learnable forward pass.\"\"\"\n",
    "        # Convenience variables\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        global_variables = self._calculate_global_variables(\n",
    "            x,\n",
    "            edge_index,\n",
    "            batch,\n",
    "            torch.log10(data.n_pulses),\n",
    "        )\n",
    "\n",
    "        # Distribute global variables out to each node\n",
    "        if not self._add_global_variables_after_pooling:\n",
    "            distribute = (\n",
    "                batch.unsqueeze(dim=1) == torch.unique(batch).unsqueeze(dim=0)\n",
    "            ).type(torch.float)\n",
    "\n",
    "            global_variables_distributed = torch.sum(\n",
    "                distribute.unsqueeze(dim=2) * global_variables.unsqueeze(dim=0),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            x = torch.cat((x, global_variables_distributed), dim=1)\n",
    "\n",
    "        # DynEdge-convolutions\n",
    "        skip_connections = [x]\n",
    "        for conv_layer in self._conv_layers:\n",
    "            x, edge_index = conv_layer(x, edge_index, batch)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        # Skip-cat\n",
    "        x = torch.cat(skip_connections, dim=1)\n",
    "\n",
    "        # Post-processing\n",
    "        x = self._post_processing(x)\n",
    "\n",
    "        # (Optional) Global pooling\n",
    "        if self._global_pooling_schemes:\n",
    "            x = self._global_pooling(x, batch=batch)\n",
    "            if self._add_global_variables_after_pooling:\n",
    "                x = torch.cat(\n",
    "                    [\n",
    "                        x,\n",
    "                        global_variables,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "        # Read-out\n",
    "        x = self._readout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class DynEdgeV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = DynEdge(\n",
    "            nb_inputs=9,\n",
    "            nb_neighbours=8,\n",
    "            global_pooling_schemes=[\"min\", \"max\", \"mean\", \"sum\"],\n",
    "            features_subset=slice(0, 4),  # NN search using xyzt\n",
    "        )\n",
    "\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=self.encoder.nb_outputs,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.encoder(batch)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py:1536: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = DynEdgeV0().eval()\n",
    "ds = GraphDasetV0(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = gDataLoader(ds, batch_size=10, shuffle=False)\n",
    "batch = next(iter(dl))\n",
    "with torch.no_grad():\n",
    "    out = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2712, -0.8261, -0.4940,  0.9138,  0.4056,  0.0211,  0.2689,  0.6181,\n",
       "        -0.7387,  0.6235, -0.2914,  0.7255,  0.6406,  0.4908,  0.5905,  0.9622,\n",
       "         0.0109,  0.2720, -0.5679, -0.2346, -0.7890,  0.3201, -0.5343,  0.7824,\n",
       "        -0.8785,  0.0498, -0.4751,  0.9721, -0.1260,  0.1978])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
