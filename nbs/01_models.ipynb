{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/archive/software/graphnet/src')\n",
    "sys.path.append('/opt/slh/icecube/')\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "from abc import abstractmethod\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "import scipy\n",
    "import numpy as np\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, AzimuthReconstructionWithKappa, ZenithReconstruction\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss,  VonMisesFisher2DLoss, EuclideanDistanceLoss\n",
    "from graphnet.training.labels import Direction\n",
    "from icecube.modelsgraph import EGNNModeLFEAT, DynEdgeFEXTRACTRO\n",
    "from torch_geometric.nn.pool import knn_graph\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from einops import repeat\n",
    "from torch_geometric.utils import to_dense_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import math\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "\n",
    "class EuclideanDistanceLossG(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-6, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        diff = prediction - target\n",
    "        loss = torch.norm(diff, dim=1) + self.eps\n",
    "        if self.reduction == 'mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class VonMisesFisher3DLossCosineSimularityLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (self.vonmis(y_pred, y_true) + (1-self.cosine(y_pred[:, :3], y_true).mean()))/2\n",
    "    \n",
    "    \n",
    "class VonMisesFisher3DLossEcludeLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = EuclideanDistanceLossG()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (self.vonmis(y_pred, y_true) + self.cosine(y_pred[:, :3], y_true))/2\n",
    "    \n",
    "class VonMisesFisher3DLossEcludeLossCosine(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine =  nn.CosineSimilarity(dim=1, eps=eps)\n",
    "        self.euclud = EuclideanDistanceLossG()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (self.vonmis(y_pred, y_true) + \n",
    "                (self.euclud(y_pred[:, :3], y_true)) +\n",
    "                (1-self.cosine(y_pred[:, :3], y_true).mean()))/3\n",
    "    \n",
    "\n",
    "\n",
    "class VonMisesFisher2DLossL1Loss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher2DLoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        vm = self.vonmis(y_pred[:, :2], y_true)\n",
    "        l1 = self.l1(y_pred[:, 2], y_true[:, -1])\n",
    "        return (vm + l1)/2\n",
    "        \n",
    "    \n",
    "    \n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "class SigmoidRange(nn.Module):\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * (self.high - self.low) + self.low\n",
    "\n",
    "\n",
    "class Adjustoutput(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.az = SigmoidRange(6.436839548775502e-08, 6.2891)\n",
    "        self.zn = SigmoidRange(8.631674577710722e-05, 3.1417)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x[:, 0] = self.az(x[:, 0])\n",
    "        x[:, 1] = self.zn(x[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolingWithMask(nn.Module):\n",
    "    def __init__(self, pool_type):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        if self.pool_type == \"mean\":\n",
    "            # Sum the values along the sequence dimension\n",
    "            x = torch.sum(x, dim=1)\n",
    "\n",
    "            # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "            x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "        elif self.pool_type == \"max\":\n",
    "            # Find the maximum value along the sequence dimension\n",
    "            x, _ = torch.max(x, dim=1)\n",
    "        elif self.pool_type == \"min\":\n",
    "            # Find the minimum value along the sequence dimension\n",
    "            x, _ = torch.min(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pool_type. Choose from ['mean', 'max', 'min']\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=6,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        # self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1CombinePool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask('mean')\n",
    "        self.pool_max = PoolingWithMask('max')\n",
    "        self.head = FeedForward(128 * 2, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class always:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def l2norm(t, groups=1):\n",
    "    t = rearrange(t, \"... (g d) -> ... g d\", g=groups)\n",
    "    t = F.normalize(t, p=2, dim=-1)\n",
    "    return rearrange(t, \"... g d -> ... (g d)\")\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinng(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=14):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV1(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TokenEmbeddingV2(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV2(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=196,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV3(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "        self.sigmout = Adjustoutput()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        s = self.sigmout(x)\n",
    "        return x\n",
    "    \n",
    "class IceCubeModelEncoderSensorEmbeddinngV4(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=9):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5168)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=480,\n",
    "            post_emb_norm = True,\n",
    "            attn_layers=Encoder(dim=256, \n",
    "                                depth=8, \n",
    "                                heads=8, \n",
    "                                ff_glu = True,\n",
    "                                rotary_pos_emb = True),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        \n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256 * 2,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderSensorEmbeddinngV4().eval()\n",
    "event = torch.rand(4, 100, 9)\n",
    "mask = torch.ones(4, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (4, 100))\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.277827"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class IceCubeModelEncoderV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.out = DirectionReconstructionWithKappa(128)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstruction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.pool_min = PoolingWithMask(\"min\")\n",
    "        self.ae = FeedForward(384, 384)\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=384,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), \n",
    "                          self.pool_max(x, mask), \n",
    "                          self.pool_min(x, mask)], dim=1)\n",
    "        x = self.ae(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "class EncoderWithDirectionReconstructionV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "    \n",
    "class EncoderWithDirectionReconstructionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "\n",
    "        self.azimuth_task = AzimuthReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            loss_function=VonMisesFisher2DLoss(),\n",
    "            target_labels=[\"azimuth\", \"kappa\"],\n",
    "        )\n",
    "\n",
    "        self.zenith_task = ZenithReconstruction(\n",
    "            hidden_size=256,\n",
    "            loss_function=nn.L1Loss(),\n",
    "            target_labels=[\"zenith\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        az = self.azimuth_task(x)\n",
    "        zn = self.zenith_task(x)\n",
    "        return torch.concat([az, zn], dim=1)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "\n",
    "        self.azimuth_task = AzimuthReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            loss_function=VonMisesFisher2DLoss(),\n",
    "            target_labels=[\"azimuth\", \"kappa\"],\n",
    "        )\n",
    "\n",
    "        self.zenith_task = ZenithReconstruction(\n",
    "            hidden_size=256,\n",
    "            loss_function=nn.L1Loss(),\n",
    "            target_labels=[\"zenith\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        az = self.azimuth_task(x)\n",
    "        zn = self.zenith_task(x)\n",
    "        return torch.concat([az, zn], dim=1)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            post_emb_norm = True,\n",
    "            attn_layers=Encoder(dim=128,\n",
    "                                depth=8,\n",
    "                                heads=8,\n",
    "                                ff_glu = True,\n",
    "                                rotary_pos_emb = True),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "    \n",
    "\n",
    "class EncoderWithDirectionReconstructionV5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            post_emb_norm = True,\n",
    "            attn_layers=Encoder(dim=128,\n",
    "                                depth=8,\n",
    "                                heads=8,\n",
    "                                ff_glu = True,\n",
    "                                rotary_pos_emb = True),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, 128))\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=128,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        bs = x.shape[0]\n",
    "        cls_tokens  = self.cls_token.expand(bs, -1, -1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask, prepend_embeds=cls_tokens)\n",
    "        #pool on cls token\n",
    "        x = x[:, 0]\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class ExtractorV0(nn.Module):\n",
    "    def __init__(self, dim_base=128, dim=384, proj = True):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim_base)\n",
    "        self.emb2 = SinusoidalPosEmb(dim=dim_base//2)\n",
    "        self.aux_emb = nn.Embedding(2,dim_base//2)\n",
    "        self.qe_emb = nn.Embedding(2,dim_base//2)\n",
    "        self.proj = nn.Linear(dim_base*7,dim) if proj else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, Lmax=None):\n",
    "        pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "        charge = x['charge'] if Lmax is None else x['charge'][:,:Lmax]\n",
    "        time = x['time'] if Lmax is None else x['time'][:,:Lmax]\n",
    "        auxiliary = x['aux'] if Lmax is None else x['auxiliary'][:,:Lmax]\n",
    "        qe = x['qe'] if Lmax is None else x['qe'][:,:Lmax]\n",
    "        ice_properties = x['ice_properties'] if Lmax is None else x['ice_properties'][:,:Lmax]\n",
    "        \n",
    "        x = torch.cat([self.emb(100*pos).flatten(-2), self.emb(40*charge),\n",
    "                       self.emb(100*time),self.aux_emb(auxiliary),self.qe_emb(qe),\n",
    "                       self.emb2(50*ice_properties).flatten(-2)],-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "class ExtractorV1(nn.Module):\n",
    "    def __init__(self, dim_base=128):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim_base)\n",
    "        self.emb2 = SinusoidalPosEmb(dim=dim_base//2)\n",
    "        self.aux_emb = TokenEmbeddingV2(dim_base//4, 2, True)\n",
    "        self.qe_emb = TokenEmbeddingV2(dim_base//4, 2, True)\n",
    "        self.rank = TokenEmbeddingV2(dim_base//4, 4, True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ice_properties = torch.stack([x['scattering'], x['absorption']], dim=2)\n",
    "        \n",
    "        x = torch.cat([self.emb(100*x['pos']).flatten(-2), \n",
    "                       self.emb(40*x['charge']),\n",
    "                       self.emb(100*x['time']),\n",
    "                       self.aux_emb(x[\"aux\"]),\n",
    "                       self.qe_emb(x[\"qe\"]),\n",
    "                       self.rank(x[\"rank\"]),\n",
    "                       self.emb2(50*ice_properties).flatten(-2)],-1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ExtractorV2(nn.Module):\n",
    "    def __init__(self, dim_base=128, out_dim=196):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim_base)\n",
    "        self.emb2 = SinusoidalPosEmb(dim=dim_base//2)\n",
    "        self.aux_emb = TokenEmbeddingV2(dim_base//4, 2, True)\n",
    "        self.qe_emb = TokenEmbeddingV2(dim_base//4, 2, True)\n",
    "        self.rank = TokenEmbeddingV2(dim_base//4, 4, True)\n",
    "        self.out = nn.Linear(864, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ice_properties = torch.stack([x['scattering'], x['absorption']], dim=2)\n",
    "        \n",
    "        x = torch.cat([self.emb(100*x['pos']).flatten(-2), \n",
    "                       self.emb(40*x['charge']),\n",
    "                       self.emb(100*x['time']),\n",
    "                       self.aux_emb(x[\"aux\"]),\n",
    "                       self.qe_emb(x[\"qe\"]),\n",
    "                       self.rank(x[\"rank\"]),\n",
    "                       self.emb2(50*ice_properties).flatten(-2)],-1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ExtractorV2(nn.Module):\n",
    "    def __init__(self, dim_base=128, out_dim=196):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim_base)\n",
    "        self.emb2 = SinusoidalPosEmb(dim=dim_base//2)\n",
    "        self.aux_emb = TokenEmbeddingV2(dim_base//4, 2, True)\n",
    "        self.qe_emb = TokenEmbeddingV2(dim_base//4, 2, True)\n",
    "        self.rank = TokenEmbeddingV2(dim_base//4, 4, True)\n",
    "        self.out = nn.Linear(864, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ice_properties = torch.stack([x['scattering'], x['absorption']], dim=2)\n",
    "        \n",
    "        x = torch.cat([self.emb(100*x['pos']).flatten(-2), \n",
    "                       self.emb(40*x['charge']),\n",
    "                       self.emb(100*x['time']),\n",
    "                       self.aux_emb(x[\"aux\"]),\n",
    "                       self.qe_emb(x[\"qe\"]),\n",
    "                       self.rank(x[\"rank\"]),\n",
    "                       self.emb2(50*ice_properties).flatten(-2)],-1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV6(nn.Module):\n",
    "    def __init__(self, dim_in = 864, dim_out=256, attn_depth = 8, heads = 8):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=dim_in,\n",
    "            dim_out=dim_out,\n",
    "            max_seq_len=440,\n",
    "            post_emb_norm = True,\n",
    "            attn_layers=Encoder(dim=dim_out,\n",
    "                                depth=attn_depth,\n",
    "                                heads=heads,\n",
    "                                ff_glu = True,\n",
    "                                rotary_pos_emb = True),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, dim_in))\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=dim_out,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        self.fe = ExtractorV1()\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"]\n",
    "        x = self.fe(batch)\n",
    "        bs = x.shape[0]\n",
    "        cls_tokens  = self.cls_token.expand(bs, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim = -2)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x[:, 0]\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV7(nn.Module):\n",
    "    def __init__(self, dim_in = 864, dim_out=256, attn_depth = 12, heads = 12):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=dim_in,\n",
    "            dim_out=dim_out,\n",
    "            max_seq_len=440,\n",
    "            post_emb_norm = True,\n",
    "            use_abs_pos_emb = False, \n",
    "            emb_dropout = 0.1, \n",
    "            attn_layers=Encoder(dim=dim_out,\n",
    "                                depth=attn_depth,\n",
    "                                heads=heads,\n",
    "                                ff_glu = True,\n",
    "                                rotary_pos_emb = True, \n",
    "                                use_rmsnorm = True,\n",
    "                                layer_dropout = 0.1, \n",
    "                                attn_dropout = 0.1,    \n",
    "                                ff_dropout = 0.1)   \n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, dim_in))\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=dim_out * 3,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        self.fe = ExtractorV1()\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"]\n",
    "        x = self.fe(batch)\n",
    "        bs = x.shape[0]\n",
    "        cls_tokens  = self.cls_token.expand(bs, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim = -2)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask), x[:, 0]], dim=1)\n",
    "        return self.out(x)\n",
    "    \n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "class EncoderWithDirectionReconstructionV8(nn.Module):\n",
    "    def __init__(self, dim_in = 864, dim_out=256, attn_depth = 8, heads = 12):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=dim_out,\n",
    "            dim_out=dim_out,\n",
    "            max_seq_len=300,\n",
    "            post_emb_norm = True,\n",
    "            use_abs_pos_emb = False, \n",
    "            emb_dropout = 0.1, \n",
    "            attn_layers=Encoder(dim=dim_out,\n",
    "                                depth=attn_depth,\n",
    "                                heads=heads,\n",
    "                                ff_glu = True,\n",
    "                                rel_pos_bias = True, \n",
    "                                layer_dropout = 0.01, \n",
    "                                attn_dropout = 0.01,    \n",
    "                                ff_dropout = 0.01)   \n",
    "        )\n",
    "        \n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        \n",
    "        self.out = nn.Linear(dim_out * 3, 3)\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "        #torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"]\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        bs = x.shape[0]\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = x[:,:mask.sum(-1).max()]\n",
    "        mask = mask[:,:mask.sum(-1).max()]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask), x[:, 0]], dim=1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV9(nn.Module):\n",
    "    def __init__(self, dim_out=256, attn_depth = 10, heads = 12):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_out=dim_out,\n",
    "            max_seq_len=300,\n",
    "            post_emb_norm = True,\n",
    "            use_abs_pos_emb = False, \n",
    "            emb_dropout = 0.1, \n",
    "            attn_layers=Encoder(dim=dim_out,\n",
    "                                depth=attn_depth,\n",
    "                                heads=heads,\n",
    "                                use_rmsnorm = True,\n",
    "                                ff_glu = True,\n",
    "                                alibi_pos_bias = True, \n",
    "                                alibi_num_heads = 4 ,  \n",
    "                                layer_dropout = 0.01, \n",
    "                                attn_dropout = 0.01,    \n",
    "                                ff_dropout = 0.01)   \n",
    "        )\n",
    "        \n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)  \n",
    "        self.out = nn.Linear(dim_out, 3)\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.apply(self._init_weights)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "        #torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"]\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        bs = x.shape[0]\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = x[:,:mask.sum(-1).max()]\n",
    "        mask = mask[:,:mask.sum(-1).max()]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x[:, 0]\n",
    "        return self.out(x)\n",
    "    \n",
    "from torch_geometric.utils import to_dense_batch\n",
    "class EncoderWithDirectionReconstructionV10(nn.Module):\n",
    "    def __init__(self, dim_in = 864, dim_out=256, attn_depth = 8, heads = 12):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_out=dim_out,\n",
    "            max_seq_len=300,\n",
    "            post_emb_norm = True,\n",
    "            use_abs_pos_emb = False, \n",
    "            emb_dropout = 0.1, \n",
    "            attn_layers=Encoder(dim=dim_out,\n",
    "                                depth=attn_depth,\n",
    "                                heads=heads,\n",
    "                                ff_glu = True,\n",
    "                                rel_pos_bias = True, \n",
    "                                layer_dropout = 0.01, \n",
    "                                attn_dropout = 0.01,    \n",
    "                                ff_dropout = 0.01)   \n",
    "        )\n",
    "    \n",
    "        \n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.out = nn.Linear(dim_out, 3)\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.graph_feat = EGNNModeLFEAT( emb_dim=dim_out, num_layers=2)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "        #torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = pos, k=8, batch=batch_index).to(mask.device)\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = x[mask]\n",
    "        x = self.graph_feat(x, pos, edge_index)\n",
    "        x, mask = to_dense_batch(x, batch_index)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x[:, 0]\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from x_transformers.x_transformers import ScaledSinusoidalEmbedding\n",
    "from icecube.utils import collate_fn_v2\n",
    "from icecube.dataset import HuggingFaceDatasetV14\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = HuggingFaceDatasetV14(load_from_disk('/opt/slh/icecube/data/hf_cashe/batch_1.parquet'))\n",
    "dl = DataLoader(ds, batch_size=12, collate_fn=collate_fn_v2, num_workers=1, drop_last=True)\n",
    "batch=next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = EncoderWithDirectionReconstructionV10().eval()\n",
    "#with torch.no_grad():\n",
    "#    out = md(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.776453"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(md)/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderWithDirectionReconstructionV5().eval()\n",
    "event = torch.rand(4, 100, 9)\n",
    "mask = torch.ones(4, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (10, 100))\n",
    "label = torch.rand(4, 3)\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id, label=label)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DIST_KERNELS = {\n",
    "    \"exp\": {\n",
    "        \"fn\": lambda t: torch.exp(-t),\n",
    "        \"mask_value_fn\": lambda t: torch.finfo(t.dtype).max,\n",
    "    },\n",
    "    \"softmax\": {\n",
    "        \"fn\": lambda t: torch.softmax(t, dim=-1),\n",
    "        \"mask_value_fn\": lambda t: -torch.finfo(t.dtype).max,\n",
    "    },\n",
    "}\n",
    "\n",
    "# helpers\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return d if not exists(val) else val\n",
    "\n",
    "\n",
    "# helper classes\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x + self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForwardV1(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, heads=8, dim_head=64, Lg=0.5, Ld=0.5, La=1, dist_kernel_fn=\"exp\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        # hyperparameters controlling the weighted linear combination from\n",
    "        # self-attention (La)\n",
    "        # adjacency graph (Lg)\n",
    "        # pair-wise distance matrix (Ld)\n",
    "\n",
    "        self.La = La\n",
    "        self.Ld = Ld\n",
    "        self.Lg = Lg\n",
    "\n",
    "        self.dist_kernel_fn = dist_kernel_fn\n",
    "\n",
    "    def forward(self, x, mask=None, adjacency_mat=None, distance_mat=None):\n",
    "        h, La, Ld, Lg, dist_kernel_fn = (\n",
    "            self.heads,\n",
    "            self.La,\n",
    "            self.Ld,\n",
    "            self.Lg,\n",
    "            self.dist_kernel_fn,\n",
    "        )\n",
    "\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, \"b n (h qkv d) -> b h n qkv d\", h=h, qkv=3).unbind(\n",
    "            dim=-2\n",
    "        )\n",
    "        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        assert (\n",
    "            dist_kernel_fn in DIST_KERNELS\n",
    "        ), f\"distance kernel function needs to be one of {DIST_KERNELS.keys()}\"\n",
    "        dist_kernel_config = DIST_KERNELS[dist_kernel_fn]\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = rearrange(distance_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            adjacency_mat = rearrange(adjacency_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = torch.finfo(dots.dtype).max\n",
    "            mask = mask[:, None, :, None] * mask[:, None, None, :]\n",
    "\n",
    "            # mask attention\n",
    "            dots.masked_fill_(~mask, -mask_value)\n",
    "\n",
    "            if exists(distance_mat):\n",
    "                # mask distance to infinity\n",
    "                # todo - make sure for softmax distance kernel, use -infinity\n",
    "                dist_mask_value = dist_kernel_config[\"mask_value_fn\"](dots)\n",
    "                distance_mat.masked_fill_(~mask, dist_mask_value)\n",
    "\n",
    "            if exists(adjacency_mat):\n",
    "                adjacency_mat.masked_fill_(~mask, 0.0)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # sum contributions from adjacency and distance tensors\n",
    "        attn = attn * La\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            attn = attn + Lg * adjacency_mat\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = dist_kernel_config[\"fn\"](distance_mat)\n",
    "            attn = attn + Ld * distance_mat\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class MAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MATMaskedPool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=1.0,\n",
    "        Ld=1.0,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = self.pool(x, mask=mask)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class MATAdjusted(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim,\n",
    "        depth,\n",
    "        heads=12,\n",
    "        Lg=0.75,\n",
    "        Ld=0.75,\n",
    "        La=1.,\n",
    "        dist_kernel_fn=\"softmax\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, adjacency_mat, mask):\n",
    "\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMAT(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MAT(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMATMasked(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MATMaskedPool(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def batched_index_select(values, indices):\n",
    "    last_dim = values.shape[-1]\n",
    "    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class NMatrixAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 4,\n",
    "        dropout = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        #null_k and null_v parameters serve as learnable \"null\" key and value vectors.\n",
    "        #provids a default key and value for each attention head \n",
    "        #when there is no connection between two nodes or when adjacency information is missing.\n",
    "        #By including these null keys and values, the attention mechanism can learn to assign a\n",
    "        #ppropriate importance to the null entries in the adjacency matrix, effectively allowing the model to learn \n",
    "        #how to handle situations where neighborhood information is incomplete or scarce.\n",
    "        self.null_k = nn.Parameter(torch.randn(heads, dim_head))\n",
    "        self.null_v = nn.Parameter(torch.randn(heads, dim_head))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        adj_kv_indices,\n",
    "        mask\n",
    "    ):\n",
    "        b, n, d, h = *x.shape, self.heads\n",
    "        flat_indices = repeat(adj_kv_indices, 'b n a -> (b h) (n a)', h = h)\n",
    "        #splits the input tensor into query q, key k, and value v tensors using the to_qkv linear laye\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        #rearranges q, k, and v tensors to have separate head dimensions.\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        #batched_index_select to select the corresponding k and v tensors based on the adjacency indices\n",
    "        k, v = map(lambda t: rearrange(t, 'b h n d -> (b h) n d'), (k, v))\n",
    "        k = batched_index_select(k, flat_indices)\n",
    "        v = batched_index_select(v, flat_indices)\n",
    "        k, v = map(lambda t: rearrange(t, '(b h) (n a) d -> b h n a d', h = h, n = n), (k, v))\n",
    "\n",
    "        nk, nv = map(lambda t: rearrange(t, 'h d -> () h () () d').expand(b, -1, n, 1, -1), (self.null_k, self.null_v))\n",
    "        k = torch.cat((nk, k), dim = -2)\n",
    "        v = torch.cat((nv, v), dim = -2)\n",
    "        mask = F.pad(mask, (1, 0), value = 1)\n",
    "        #calculate the similarity scores between queries and keys, scales them, and applies the mask.\n",
    "        sim = einsum('b h n d, b h n a d -> b h n a', q, k) * self.scale\n",
    "\n",
    "        mask_value = -torch.finfo(sim.dtype).max\n",
    "        mask = rearrange(mask.bool(), 'b n a -> b () n a')\n",
    "        sim.masked_fill_(~mask.bool(), mask_value)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h n a, b h n a d -> b h n d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class LocalAttenNetwok(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_neighbors_cutoff = None,\n",
    "        attn_dropout = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_neighbors_cutoff = num_neighbors_cutoff\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = None\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, NMatrixAttention(\n",
    "                    dim = dim,\n",
    "                    dim_head = dim_head,\n",
    "                    heads = heads,\n",
    "                    dropout = attn_dropout\n",
    "                ))),\n",
    "                global_attn,\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, adjacency_mat, mask = None):\n",
    "        device, n = x.device, x.shape[1]\n",
    "\n",
    "        diag = torch.eye(adjacency_mat.shape[-1], device = device).bool()\n",
    "        adjacency_mat |= diag\n",
    "        if exists(mask):\n",
    "            adjacency_mat &= (mask[:, :, None] * mask[:, None, :])\n",
    "\n",
    "        adj_mat = adjacency_mat.float()\n",
    "        max_neighbors = int(adj_mat.sum(dim = -1).max())\n",
    "\n",
    "        if exists(self.num_neighbors_cutoff) and max_neighbors > self.num_neighbors_cutoff:\n",
    "            noise = torch.empty((n, n), device = device).uniform_(-0.01, 0.01)\n",
    "            adj_mat = adj_mat + noise\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = self.num_neighbors_cutoff)\n",
    "            adj_mask = (adj_mask > 0.5).float()\n",
    "        else:\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = max_neighbors)\n",
    "        for attn, _ in self.layers:\n",
    "            x = attn(\n",
    "                x,\n",
    "                adj_kv_indices = adj_kv_indices,\n",
    "                mask = adj_mask\n",
    "            )\n",
    "\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LAttentionV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        and_self_attend = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.and_self_attend = and_self_attend\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        context,\n",
    "        mask = None\n",
    "    ):\n",
    "        h, scale = self.heads, self.scale\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = -torch.finfo(dots.dtype).max\n",
    "            mask = rearrange(mask, 'b n -> b 1 1 n')\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        attn = dots.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LocalLatentsAttent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        num_latents = 64,\n",
    "        latent_self_attend = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "        self.attn1 = LAttentionV2(dim, heads, and_self_attend = latent_self_attend)\n",
    "        self.attn2 = LAttentionV2(dim, heads)\n",
    "\n",
    "    def forward(self, x, latents = None, mask = None):\n",
    "        b, *_ = x.shape\n",
    "\n",
    "        latents = self.latents\n",
    "\n",
    "        if latents.ndim == 2:\n",
    "            latents = repeat(latents, 'n d -> b n d', b = b)\n",
    "\n",
    "        latents = self.attn1(latents, x, mask = mask)\n",
    "        out     = self.attn2(x, latents)\n",
    "\n",
    "        return out, latents\n",
    "    \n",
    "class LocalAttenV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        num_latents = 64,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = PreNorm(dim, LocalLatentsAttent(\n",
    "                dim = dim,\n",
    "                heads = heads,\n",
    "                num_latents = num_latents\n",
    "            )) \n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                global_attn,\n",
    "                Residual(PreNorm(dim, FeedForward(\n",
    "                    dim = dim,\n",
    "                    dropout = ff_dropout\n",
    "                )))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        for attn, ff in self.layers:\n",
    "            out, _ = attn(x, mask = mask)\n",
    "            x = x + out\n",
    "            x = ff(x)\n",
    "        return x\n",
    "    \n",
    "class GlobalLocalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_neighbors_cutoff = None,\n",
    "        attn_dropout = 0.1,\n",
    "        ff_dropout=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_neighbors_cutoff = num_neighbors_cutoff\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = PreNorm(dim, LocalLatentsAttent(\n",
    "                dim = dim,\n",
    "                heads = heads,\n",
    "                num_latents = 64\n",
    "            )) \n",
    "        \n",
    "            self.layers.append(nn.ModuleList([\n",
    "                global_attn,\n",
    "                Residual(PreNorm(dim, NMatrixAttention(\n",
    "                    dim = dim,\n",
    "                    dim_head = dim_head,\n",
    "                    heads = heads,\n",
    "                    dropout = attn_dropout\n",
    "                ))),\n",
    "                Residual(PreNorm(dim, FeedForward(\n",
    "                    dim = dim,\n",
    "                    dropout = ff_dropout\n",
    "                )))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, adjacency_mat, mask = None):\n",
    "        device, n = x.device, x.shape[1]\n",
    "\n",
    "        diag = torch.eye(adjacency_mat.shape[-1], device = device).bool()\n",
    "        adjacency_mat |= diag\n",
    "        if exists(mask):\n",
    "            adjacency_mat &= (mask[:, :, None] * mask[:, None, :])\n",
    "\n",
    "        adj_mat = adjacency_mat.float()\n",
    "        max_neighbors = int(adj_mat.sum(dim = -1).max())\n",
    "\n",
    "        if exists(self.num_neighbors_cutoff) and max_neighbors > self.num_neighbors_cutoff:\n",
    "            noise = torch.empty((n, n), device = device).uniform_(-0.01, 0.01)\n",
    "            adj_mat = adj_mat + noise\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = self.num_neighbors_cutoff)\n",
    "            adj_mask = (adj_mask > 0.5).float()\n",
    "        else:\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = max_neighbors)\n",
    "        \n",
    "        for attn, locla_attn, ff in self.layers:\n",
    "            x, _ = attn(x, mask = mask)\n",
    "            out = locla_attn(\n",
    "                x,\n",
    "                adj_kv_indices = adj_kv_indices,\n",
    "                mask = adj_mask\n",
    "            )\n",
    "            x = x + out\n",
    "            x = ff(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "\n",
    "class LocalGlobalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        num_neighbors_cutoff = None,\n",
    "        attn_dropout = 0.1,\n",
    "        ff_dropout=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_neighbors_cutoff = num_neighbors_cutoff\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = PreNorm(dim, LocalLatentsAttent(\n",
    "                dim = dim,\n",
    "                heads = heads,\n",
    "                num_latents = 64\n",
    "            )) \n",
    "        \n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Residual(PreNorm(dim, NMatrixAttention(\n",
    "                    dim = dim,\n",
    "                    dim_head = dim_head,\n",
    "                    heads = heads,\n",
    "                    dropout = attn_dropout\n",
    "                ))),\n",
    "                global_attn,\n",
    "                Residual(PreNorm(dim, FeedForward(\n",
    "                    dim = dim,\n",
    "                    dropout = ff_dropout\n",
    "                )))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, adjacency_mat, mask = None):\n",
    "        device, n = x.device, x.shape[1]\n",
    "\n",
    "        diag = torch.eye(adjacency_mat.shape[-1], device = device).bool()\n",
    "        adjacency_mat |= diag\n",
    "        if exists(mask):\n",
    "            adjacency_mat &= (mask[:, :, None] * mask[:, None, :])\n",
    "\n",
    "        adj_mat = adjacency_mat.float()\n",
    "        max_neighbors = int(adj_mat.sum(dim = -1).max())\n",
    "\n",
    "        if exists(self.num_neighbors_cutoff) and max_neighbors > self.num_neighbors_cutoff:\n",
    "            noise = torch.empty((n, n), device = device).uniform_(-0.01, 0.01)\n",
    "            adj_mat = adj_mat + noise\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = self.num_neighbors_cutoff)\n",
    "            adj_mask = (adj_mask > 0.5).float()\n",
    "        else:\n",
    "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = max_neighbors)\n",
    "        \n",
    "        for attn, locla_attn, ff in self.layers:\n",
    "            x = attn(\n",
    "                x,\n",
    "                adj_kv_indices = adj_kv_indices,\n",
    "                mask = adj_mask\n",
    "            )\n",
    "            out, _ = locla_attn(x, mask = mask)\n",
    "            x = x + out\n",
    "            x = ff(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** 0.5\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        normed = F.normalize(x, dim = -1)\n",
    "        return normed * self.scale * self.gamma\n",
    "    \n",
    "class gAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        dim_hidden = dim_head * heads\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "        self.to_q = nn.Linear(dim, dim_hidden, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, dim_hidden * 2, bias = False)\n",
    "        self.to_out = nn.Linear(dim_hidden, dim, bias = False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        context = None,\n",
    "        mask = None,\n",
    "    ):\n",
    "        h = self.heads\n",
    "        x = self.norm(x)\n",
    "        if exists(context):\n",
    "            context = self.norm(context)\n",
    "        context = default(context, x)\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), (q, k, v))\n",
    "        q = q * self.scale\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b j -> b 1 1 j')\n",
    "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class GlobalAttentionV5(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        dim_heads = 64,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            global_attn = gAttention(\n",
    "                dim = dim,\n",
    "                heads = heads,\n",
    "                dim_head = dim_heads,\n",
    "            )\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                global_attn,\n",
    "                Residual(PreNorm(dim, FeedForward(\n",
    "                    dim = dim,\n",
    "                    dropout = ff_dropout\n",
    "                )))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, mask = None, context = None):\n",
    "        for attn, ff in self.layers:\n",
    "            out = attn(x, mask = mask, context = context)\n",
    "            x = x + out\n",
    "            x = ff(x)\n",
    "        return x\n",
    "\n",
    "class BeDropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(BeDropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class BeMLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement \n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "#BEiTv2 Beblock\n",
    "class BeBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = BeDropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = BeMLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if self.gamma_1 is None:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.gamma_1 * self.drop_path(self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0]))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BeDeepIceModel(nn.Module):\n",
    "    def __init__(self, dim=384, depth=12, out_class = 3, use_checkpoint=False, drop_b= 0., div_factor=64, attn_drop_b = 0., drop_path = 0.,  **kwargs):\n",
    "        super().__init__()\n",
    "        self.Beblocks = nn.ModuleList([ \n",
    "            BeBlock(\n",
    "                dim=dim, num_heads=dim//div_factor, mlp_ratio=4, drop_path=drop_path, init_values=1, attn_drop=attn_drop_b, drop=drop_b)\n",
    "            for i in range(depth)])\n",
    "        #self.Beblocks = nn.ModuleList([ \n",
    "        #    nn.TransformerEncoderLayer(dim,dim//64,dim*4,dropout=0,\n",
    "        #        activation=nn.GELU(), batch_first=True, norm_first=True)\n",
    "        #    for i in range(depth)])\n",
    "        self.out_class = out_class\n",
    "        self.proj_out = nn.Linear(dim,out_class) if out_class == 3 else nn.Identity()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.Beblocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "        self.apply(_init_weights)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        attn_mask = torch.zeros(mask.shape, device=mask.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        for blk in self.Beblocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, None, attn_mask)\n",
    "            else: x = blk(x, None, attn_mask)\n",
    "        if self.out_class == 3:\n",
    "            x = self.proj_out(x[:,0]) #cls token\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV11(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = LocalAttenNetwok(dim = dim_out, depth = 3, num_neighbors_cutoff = 24)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = pos, k=8, batch=batch_index).to(mask.device)\n",
    "        adj_matrix = to_dense_adj(edge_index, batch_index).int()\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, adj_matrix, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV11_V2_GLOBAL_LOCAL(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = GlobalLocalAttention(dim = dim_out, depth = 3, num_neighbors_cutoff = 24)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = pos, k=8, batch=batch_index).to(mask.device)\n",
    "        adj_matrix = to_dense_adj(edge_index, batch_index).int()\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, adj_matrix, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV11_V2_LOCAL_GLOBAL(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = LocalGlobalAttention(dim = dim_out, depth = 3, num_neighbors_cutoff = 24)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = pos, k=8, batch=batch_index).to(mask.device)\n",
    "        adj_matrix = to_dense_adj(edge_index, batch_index).int()\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, adj_matrix, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV12(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = MATAdjusted(model_dim = dim_out, depth = 3)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        xyz = batch[\"pos\"][mask]\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = xyz, k=8, batch=batch_index).to(mask.device)\n",
    "        adj_matrix = to_dense_adj(edge_index, batch_index).int()\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, adj_matrix, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "class EncoderWithDirectionReconstructionV12_V2(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_out=dim_out,\n",
    "            max_seq_len=256,\n",
    "            post_emb_norm = True,\n",
    "            use_abs_pos_emb = False, \n",
    "            attn_layers=Encoder(dim=dim_out,\n",
    "                                depth=8,\n",
    "                                heads=8,\n",
    "                                use_rmsnorm = True,\n",
    "                                ff_glu = True,\n",
    "                                rel_pos_bias = True,\n",
    "                                deepnorm=True)   \n",
    "        )\n",
    "    \n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = MATAdjusted(model_dim = dim_out, depth = 3)\n",
    "        self.out = nn.Linear(dim_out, 3)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        xyz = batch[\"pos\"][mask]\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = xyz, k=8 ,batch=batch_index).to(mask.device)\n",
    "        adj_matrix = to_dense_adj(edge_index, batch_index).int()\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, adj_matrix, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x[:, 0]\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV13(nn.Module):\n",
    "    def __init__(self, dim_out=256):\n",
    "        super().__init__()\n",
    "        self.encoder = BeDeepIceModel(dim_out)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.graphnet = DynEdgeFEXTRACTRO(dim_out + 4)\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        xyzt = torch.concat([batch[\"pos\"][mask] , batch['time'][mask].view(-1, 1)], dim=1)\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = xyzt, k=12, batch=batch_index).to(mask.device)\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = x[mask]\n",
    "        x = torch.cat([x, xyzt], dim=1)\n",
    "        x, _, _ = self.graphnet(x, edge_index, batch_index, mask.sum(-1))\n",
    "        x, mask = to_dense_batch(x, batch_index)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV14(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = LocalAttenV2(dim = dim_out, depth = 4)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.loacl_attn(x, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV15(nn.Module):\n",
    "    def __init__(self, dim_out=256 + 64, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=96)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.loacl_attn = LocalAttenV2(dim = dim_out, depth = 5)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.loacl_attn(x, mask)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def get_ds_matrix(x, batch_index, mask, Lmax=None):\n",
    "    pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "    time = x['time'] if Lmax is None else x['time'][:,:Lmax]\n",
    "    ds2 = (pos[:,:,None] - pos[:,None,:]).pow(2).sum(-1) - \\\n",
    "                ((time[:,:,None] - time[:,None,:])*(3e4/500*3e-1)).pow(2)\n",
    "    d = torch.sign(ds2)*torch.sqrt(torch.abs(ds2))\n",
    "    edge_index = knn_graph(x = d[mask], k=8, batch=batch_index).to(mask.device)\n",
    "    return edge_index\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV16(nn.Module):\n",
    "    def __init__(self, dim_out=256, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=32)\n",
    "        self.encoder = BeDeepIceModel(dim_out , drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.local_root= EGNNModeLFEAT( emb_dim=dim_out, num_layers=2)\n",
    "        self.global_root =  LocalAttenV2(dim = dim_out, depth =2)\n",
    "        self.gl_lc = GlobalAttentionV5(dim = dim_out, depth = 1)\n",
    "        self.lc_gl = GlobalAttentionV5(dim = dim_out, depth = 1)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = get_ds_matrix(batch, batch_index, mask, Lmax=mask.sum(-1).max())\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        \n",
    "        graph_featutre = self.local_root(x[mask], pos, edge_index)\n",
    "        graph_featutre, mask = to_dense_batch(graph_featutre, batch_index)\n",
    "        global_featutre = self.global_root(x, mask)\n",
    "        x = self.gl_lc(graph_featutre, mask, context = global_featutre) + self.lc_gl(global_featutre, mask, context =graph_featutre )\n",
    "        \n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV17(nn.Module):\n",
    "    def __init__(self, dim_out=256, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out//2, dim_base=32)\n",
    "        self.encoder = BeDeepIceModel(dim_out , drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.local_root= EGNNModeLFEAT( emb_dim=dim_out//2, num_layers=3)\n",
    "        self.global_root =  LocalAttenV2(dim = dim_out//2, depth =4)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        pos = batch[\"pos\"][mask] \n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = get_ds_matrix(batch, batch_index, mask, Lmax=mask.sum(-1).max())\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        \n",
    "        graph_featutre = self.local_root(x[mask], pos, edge_index)\n",
    "        graph_featutre, mask = to_dense_batch(graph_featutre, batch_index)\n",
    "        global_featutre = self.global_root(x, mask)\n",
    "        x = torch.cat([global_featutre, graph_featutre],2)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV18(nn.Module):\n",
    "    def __init__(self, dim_out=256, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out//2, dim_base=32)\n",
    "        self.encoder = BeDeepIceModel(dim_out , drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.local_root= DynEdgeFEXTRACTRO(9, \n",
    "                                           post_processing_layer_sizes = [336, dim_out//2], \n",
    "                                           dynedge_layer_sizes = [(128, 256), (336, 256), (336, 256), (336, 256)])\n",
    "        self.global_root =  LocalAttenV2(dim = dim_out//2, depth =4)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        graph_featutre = torch.concat([batch[\"pos\"][mask] , \n",
    "                             batch['time'][mask].view(-1, 1),\n",
    "                             batch['auxiliary'][mask].view(-1, 1),\n",
    "                             batch['qe'][mask].view(-1, 1),\n",
    "                             batch['charge'][mask].view(-1, 1),\n",
    "                             batch[\"ice_properties\"][mask], \n",
    "                              ], dim=1)\n",
    "        bs = mask.shape[0] # int\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = graph_featutre[:,:3], k=8, batch=batch_index).to(mask.device)\n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        \n",
    "        graph_featutre, _, _ = self.local_root(graph_featutre, edge_index, batch_index, mask.sum(-1))\n",
    "        graph_featutre, mask = to_dense_batch(graph_featutre, batch_index)\n",
    "        global_featutre = self.global_root(x, mask)\n",
    "        x = torch.cat([global_featutre, graph_featutre],2)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV19(nn.Module):\n",
    "    def __init__(self, dim_out=256, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.fe = ExtractorV0(dim=dim_out, dim_base=32)\n",
    "        self.encoder = BeDeepIceModel(dim_out, drop_path=drop_path)\n",
    "        self.cls_token = nn.Linear(dim_out,1,bias=False)\n",
    "        self.gl_attn = GlobalAttentionV5(dim = dim_out, depth = 12)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        mask = batch[\"mask\"] #bs, seq_len\n",
    "        bs = mask.shape[0] # int\n",
    "        mask = mask[:,:mask.sum(-1).max()] \n",
    "        x = self.fe(batch, mask.sum(-1).max())\n",
    "        x = self.gl_attn(x, mask)\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(bs,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        mask = torch.cat([torch.ones(bs, 1, dtype=torch.bool, device=x.device), mask], dim=1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement \n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "#BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        if self.gamma_1 is None:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            xn = self.norm1(x)\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(xn,xn,xn,\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class Attention_rel(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
    "            proj_drop=0.0, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.proj_q = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.proj_k = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        self.proj_v = nn.Linear(dim, all_head_dim, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, q, k, v, rel_pos_bias=None, key_padding_mask=None):\n",
    "        #rel_pos_bias: B L L C/h\n",
    "        #key_padding_mask - float with -inf\n",
    "        B, N, C = q.shape\n",
    "        #qkv_bias = None\n",
    "        #if self.q_bias is not None:\n",
    "        #    qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        #qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        #qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        #q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        \n",
    "        q = F.linear(input=q, weight=self.proj_q.weight, bias=self.q_bias)\n",
    "        q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        k = F.linear(input=k, weight=self.proj_k.weight, bias=None)\n",
    "        k = k.reshape(B, k.shape[1], self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        v = F.linear(input=v, weight=self.proj_v.weight, bias=self.v_bias)\n",
    "        v = v.reshape(B, v.shape[1], self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        if rel_pos_bias is not None:\n",
    "            bias = torch.einsum('bhic,bijc->bhij', q, rel_pos_bias)\n",
    "            attn = attn + bias\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.dtype == torch.float32 or key_padding_mask.dtype == torch.float16, \\\n",
    "                'incorrect mask dtype'\n",
    "            bias = torch.min(key_padding_mask[:,None,:], key_padding_mask[:,:,None])\n",
    "            bias[torch.max(key_padding_mask[:,None,:], key_padding_mask[:,:,None]) < 0] = 0\n",
    "            #print(bias.shape,bias.min(),bias.max())\n",
    "            attn = attn + bias.unsqueeze(1)\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2)\n",
    "        if rel_pos_bias is not None:\n",
    "            x = x + torch.einsum('bhij,bijc->bihc', attn, rel_pos_bias)\n",
    "        x = x.reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "#BEiTv2 block\n",
    "class Block_rel(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention_rel(dim, num_heads, attn_drop=attn_drop, qkv_bias=qkv_bias)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None, rel_pos_bias=None, kv=None):\n",
    "        if self.gamma_1 is None:\n",
    "            xn = self.norm1(x)\n",
    "            kv = xn if kv is None else self.norm1(kv)\n",
    "            x = x + self.drop_path(self.attn(xn, kv, kv,\n",
    "                            rel_pos_bias=rel_pos_bias,\n",
    "                            key_padding_mask=key_padding_mask))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            xn = self.norm1(x)\n",
    "            kv = xn if kv is None else self.norm1(kv)\n",
    "            x = x + self.drop_path(self.gamma_1 * self.drop_path(self.attn(xn, kv, kv,\n",
    "                            rel_pos_bias=rel_pos_bias,\n",
    "                            key_padding_mask=key_padding_mask)))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class ExtractorV11(nn.Module):\n",
    "    def __init__(self, dim_base=128, dim=384):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim_base)\n",
    "        self.aux_emb = nn.Embedding(2,dim_base//2)\n",
    "        self.proj = nn.Linear(11*dim_base//2,dim)\n",
    "        \n",
    "    def forward(self, x, Lmax=None):\n",
    "        pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "        charge = x['charge'] if Lmax is None else x['charge'][:,:Lmax]\n",
    "        time = x['time'] if Lmax is None else x['time'][:,:Lmax]\n",
    "        auxiliary = x['auxiliary'] if Lmax is None else x['auxiliary'][:,:Lmax]\n",
    "        qe = x['qe'] if Lmax is None else x['qe'][:,:Lmax]\n",
    "        ice_properties = x['ice_properties'] if Lmax is None else x['ice_properties'][:,:Lmax]\n",
    "        \n",
    "        x = torch.cat([self.emb(4096*pos).flatten(-2), self.emb(1024*charge),\n",
    "                       self.emb(4096*time),self.aux_emb(auxiliary)\n",
    "                      ],-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class Rel_ds(nn.Module):\n",
    "    def __init__(self, dim=32):\n",
    "        super().__init__()\n",
    "        self.emb = SinusoidalPosEmb(dim=dim)\n",
    "        self.proj = nn.Linear(dim,dim)\n",
    "        \n",
    "    def forward(self, x, Lmax=None):\n",
    "        pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "        time = x['time'] if Lmax is None else x['time'][:,:Lmax]\n",
    "        ds2 = (pos[:,:,None] - pos[:,None,:]).pow(2).sum(-1) - \\\n",
    "                ((time[:,:,None] - time[:,None,:])*(3e4/500*3e-1)).pow(2)\n",
    "        d = torch.sign(ds2)*torch.sqrt(torch.abs(ds2))\n",
    "        emb = self.emb(1024*d.clip(-4,4))\n",
    "        rel_attn = self.proj(emb)\n",
    "        return rel_attn,emb\n",
    "\n",
    "def get_nbs(x, Lmax=None, K=8):\n",
    "    pos = x['pos'] if Lmax is None else x['pos'][:,:Lmax]\n",
    "    mask = x['mask'][:,:Lmax]\n",
    "    B = pos.shape[0]\n",
    "    \n",
    "    d = -torch.cdist(pos, pos, p=2) \n",
    "    d -= 100*(~torch.min(mask[:,None,:],mask[:,:,None]))\n",
    "    d -= 200*torch.eye(Lmax, dtype=pos.dtype, device=pos.device).unsqueeze(0)\n",
    "    nbs = d.topk(K-1,dim=-1)[1]\n",
    "    nbs = torch.cat([\n",
    "            torch.arange(Lmax, dtype=nbs.dtype, device=nbs.device).unsqueeze(0).unsqueeze(-1).expand(B,-1,-1),\n",
    "            nbs],-1)\n",
    "    return nbs\n",
    "    \n",
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, dim=192, num_heads=192//64, mlp_ratio=4, drop_path=0, init_values=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.proj_rel_bias = nn.Linear(dim//num_heads,dim//num_heads)\n",
    "        self.block = Block_rel(dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio, \n",
    "                               drop_path=drop_path, init_values=init_values)\n",
    "        \n",
    "    def forward(self, x, nbs, key_padding_mask=None, rel_pos_bias=None):\n",
    "        B,Lmax,C = x.shape\n",
    "        mask = key_padding_mask if not (key_padding_mask is None) \\\n",
    "            else torch.ones(B, Lmax, dtype=torch.bool, device=x.deice)\n",
    "        \n",
    "        m = torch.gather(mask.unsqueeze(1).expand(-1,Lmax,-1), 2, nbs)\n",
    "        attn_mask = torch.zeros(m.shape, device=m.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        attn_mask = attn_mask[mask]\n",
    "        \n",
    "        if rel_pos_bias is not None:\n",
    "            rel_pos_bias = torch.gather(rel_pos_bias, 2, \n",
    "                                        nbs.unsqueeze(-1).expand(-1,-1,-1,rel_pos_bias.shape[-1]))\n",
    "            rel_pos_bias = rel_pos_bias[mask]\n",
    "            rel_pos_bias = self.proj_rel_bias(rel_pos_bias).unsqueeze(1)\n",
    "            \n",
    "        xl = torch.gather(x.unsqueeze(1).expand(-1,Lmax,-1,-1), 2, nbs.unsqueeze(-1).expand(-1,-1,-1,C))\n",
    "        xl = xl[mask]\n",
    "        # modify only the node (0th element)\n",
    "        #print(xl[:,:1].shape,rel_pos_bias.shape,attn_mask[:,:1].shape,xl.shape)\n",
    "        xl = self.block(xl[:,:1], rel_pos_bias=rel_pos_bias, key_padding_mask=attn_mask[:,:1], kv=xl)\n",
    "        x = torch.zeros(x.shape, device=x.device, dtype=xl.dtype)\n",
    "        x[mask] = xl.squeeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class EncoderWithDirectionReconstructionV20(nn.Module):\n",
    "    def __init__(self, dim=384, dim_base=128, depth=8, use_checkpoint=False, head_size=64, **kwargs):\n",
    "        super().__init__()\n",
    "        self.extractor = ExtractorV11(dim_base,dim)\n",
    "        self.rel_pos = Rel_ds(head_size)\n",
    "        self.sandwich = nn.ModuleList([ \n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "            Block_rel(dim=dim, num_heads=dim//head_size),\n",
    "        ])\n",
    "        self.cls_token = nn.Linear(dim * 2,1,bias=False)\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            Block(\n",
    "                dim=(dim * 2), num_heads=(dim * 2) //head_size, mlp_ratio=4, drop_path=0.0*(i/(depth-1)), init_values=1,)\n",
    "            for i in range(depth)])\n",
    "        self.proj_out = nn.Linear(dim * 2,3)\n",
    "        self.local_root= DynEdgeFEXTRACTRO(9, \n",
    "                                           post_processing_layer_sizes = [336, dim], \n",
    "                                           dynedge_layer_sizes = [(128, 256), (336, 256), (336, 256), (336, 256)])   \n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.apply(self._init_weights)\n",
    "        trunc_normal_(self.cls_token.weight, std=.02)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "        self.apply(_init_weights)\n",
    "        \n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'cls_token'}\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        graph_featutre = torch.concat([x0[\"pos\"][mask] , \n",
    "                             x0['time'][mask].view(-1, 1),\n",
    "                             x0['auxiliary'][mask].view(-1, 1),\n",
    "                             x0['qe'][mask].view(-1, 1),\n",
    "                             x0['charge'][mask].view(-1, 1),\n",
    "                             x0[\"ice_properties\"][mask], \n",
    "                              ], dim=1)\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        x = self.extractor(x0, Lmax)\n",
    "        rel_pos_bias, rel_enc = self.rel_pos(x0, Lmax)\n",
    "        #nbs = get_nbs(x0, Lmax)\n",
    "        mask = mask[:,:Lmax]\n",
    "        batch_index = mask.nonzero()[:,0] \n",
    "        edge_index = knn_graph(x = graph_featutre[:,:3], k=8, batch=batch_index).to(mask.device)\n",
    "        graph_featutre, _, _ = self.local_root(graph_featutre, edge_index, batch_index, x0['L0'])\n",
    "        graph_featutre, _ = to_dense_batch(graph_featutre, batch_index)\n",
    "        \n",
    "        B,_ = mask.shape\n",
    "        attn_mask = torch.zeros(mask.shape, device=mask.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        \n",
    "        for blk in self.sandwich:\n",
    "            if isinstance(blk,LocalBlock):\n",
    "                x = blk(x,nbs,mask,rel_enc)\n",
    "            else:\n",
    "                x = blk(x,attn_mask,rel_pos_bias)\n",
    "                rel_pos_bias = None\n",
    "        x = torch.cat([x,graph_featutre],2)\n",
    "        mask = torch.cat([torch.ones(B,1,dtype=mask.dtype, device=mask.device),mask],1)\n",
    "        attn_mask = torch.zeros(mask.shape, device=mask.device)\n",
    "        attn_mask[~mask] = -torch.inf\n",
    "        cls_token = self.cls_token.weight.unsqueeze(0).expand(B,-1,-1)\n",
    "        x = torch.cat([cls_token,x],1)\n",
    "        \n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, None, attn_mask)\n",
    "            else: x = blk(x, None, attn_mask)\n",
    "                \n",
    "        x = self.proj_out(x[:,0]) #cls token\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderWithDirectionReconstructionV20(\n",
       "  (extractor): ExtractorV11(\n",
       "    (emb): SinusoidalPosEmb()\n",
       "    (aux_emb): Embedding(2, 64)\n",
       "    (proj): Linear(in_features=704, out_features=384, bias=True)\n",
       "  )\n",
       "  (rel_pos): Rel_ds(\n",
       "    (emb): SinusoidalPosEmb()\n",
       "    (proj): Linear(in_features=32, out_features=32, bias=True)\n",
       "  )\n",
       "  (sandwich): ModuleList(\n",
       "    (0): Block_rel(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention_rel(\n",
       "        (proj_q): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_k): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_v): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block_rel(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention_rel(\n",
       "        (proj_q): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_k): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_v): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block_rel(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention_rel(\n",
       "        (proj_q): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_k): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_v): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block_rel(\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention_rel(\n",
       "        (proj_q): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_k): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (proj_v): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls_token): Linear(in_features=768, out_features=1, bias=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (local_root): DynEdgeFEXTRACTRO(\n",
       "    (_activation): GELU()\n",
       "    (_conv_layers): ModuleList(\n",
       "      (0): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=46, out_features=128, bias=True)\n",
       "        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): GELU()\n",
       "        (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): GELU()\n",
       "      ))\n",
       "      (1): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
       "        (1): LayerNorm((336,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): GELU()\n",
       "        (3): Linear(in_features=336, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): GELU()\n",
       "      ))\n",
       "      (2): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
       "        (1): LayerNorm((336,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): GELU()\n",
       "        (3): Linear(in_features=336, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): GELU()\n",
       "      ))\n",
       "      (3): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
       "        (1): LayerNorm((336,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): GELU()\n",
       "        (3): Linear(in_features=336, out_features=256, bias=True)\n",
       "        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (5): GELU()\n",
       "      ))\n",
       "    )\n",
       "    (_post_processing): Sequential(\n",
       "      (0): Linear(in_features=1047, out_features=336, bias=True)\n",
       "      (1): LayerNorm((336,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): GELU()\n",
       "      (3): Linear(in_features=336, out_features=384, bias=True)\n",
       "      (4): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (5): GELU()\n",
       "    )\n",
       "    (_readout): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
       "      (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EncoderWithDirectionReconstructionV20(dim=384, dim_base=128, depth=8, head_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.149443"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(EncoderWithDirectionReconstructionV19())/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderMATMasked()\n",
    "event = torch.randn(2, 100, 6)\n",
    "mask = torch.ones(2, 100).bool()\n",
    "adjecent_matrix = torch.empty(2, 100, 100).random_(2).float()\n",
    "distance_matrix = torch.randn(2, 100, 100)\n",
    "batch = dict(\n",
    "    event=event,\n",
    "    mask=mask,\n",
    "    adjecent_matrix=adjecent_matrix,\n",
    "    distance_matrix=distance_matrix,\n",
    ")\n",
    "out = model(batch)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu115'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
