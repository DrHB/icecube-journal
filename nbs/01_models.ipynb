{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out = None, mult = 4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV0(nn.Module):\n",
    "    def __init__(slf):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=6,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128,\n",
    "                        depth=3, \n",
    "                        heads=8),\n",
    "        )\n",
    "\n",
    "        #self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.encoder(x, mask = mask)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=6,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128,\n",
    "                        depth=3, \n",
    "                        heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.encoder(x, mask = mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IceCubeModelEncoderV1(\n",
       "  (encoder): ContinuousTransformerWrapper(\n",
       "    (pos_emb): AbsolutePositionalEmbedding(\n",
       "      (emb): Embedding(150, 128)\n",
       "    )\n",
       "    (post_emb_norm): Identity()\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (project_in): Linear(in_features=6, out_features=128, bias=True)\n",
       "    (attn_layers): Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): None\n",
       "            (2): None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=128, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): None\n",
       "            (2): None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): None\n",
       "            (2): None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=128, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): None\n",
       "            (2): None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): None\n",
       "            (2): None\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=128, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_out): Linear(in_features=512, out_features=128, bias=False)\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): None\n",
       "            (2): None\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (1): GELU(approximate='none')\n",
       "              )\n",
       "              (1): Identity()\n",
       "              (2): Dropout(p=0.0, inplace=False)\n",
       "              (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Residual()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (project_out): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (pool): MeanPoolingWithMask()\n",
       "  (head): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=512, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IceCubeModelEncoderV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(1, 1024, 6)\n",
    "# mask = torch.ones(1, 1024, dtype=torch.bool)\n",
    "# y = model(x, mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
