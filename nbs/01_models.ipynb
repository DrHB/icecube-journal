{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-07 13:14:54 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230207-131454.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/archive/software/graphnet/src')\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "from abc import abstractmethod\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "import scipy\n",
    "import numpy as np\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss\n",
    "from graphnet.training.labels import Direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "class SigmoidRange(nn.Module):\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * (self.high - self.low) + self.low\n",
    "\n",
    "\n",
    "class Adjustoutput(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.az = SigmoidRange(6.436839548775502e-08, 6.2891)\n",
    "        self.zn = SigmoidRange(8.631674577710722e-05, 3.1417)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x[:, 0] = self.az(x[:, 0])\n",
    "        x[:, 1] = self.zn(x[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolingWithMask(nn.Module):\n",
    "    def __init__(self, pool_type):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        if self.pool_type == \"mean\":\n",
    "            # Sum the values along the sequence dimension\n",
    "            x = torch.sum(x, dim=1)\n",
    "\n",
    "            # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "            x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "        elif self.pool_type == \"max\":\n",
    "            # Find the maximum value along the sequence dimension\n",
    "            x, _ = torch.max(x, dim=1)\n",
    "        elif self.pool_type == \"min\":\n",
    "            # Find the minimum value along the sequence dimension\n",
    "            x, _ = torch.min(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pool_type. Choose from ['mean', 'max', 'min']\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=6,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        # self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1CombinePool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask('mean')\n",
    "        self.pool_max = PoolingWithMask('max')\n",
    "        self.head = FeedForward(128 * 2, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class always:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def l2norm(t, groups=1):\n",
    "    t = rearrange(t, \"... (g d) -> ... g d\", g=groups)\n",
    "    t = F.normalize(t, p=2, dim=-1)\n",
    "    return rearrange(t, \"... g d -> ... (g d)\")\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinng(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=14):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV1(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TokenEmbeddingV2(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV2(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=196,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV3(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "        self.sigmout = Adjustoutput()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        s = self.sigmout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderV1CombinePool().eval()\n",
    "event = torch.rand(1, 100, 8)\n",
    "mask = torch.ones(1, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (1, 100))\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.out = DirectionReconstructionWithKappa(128)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstruction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphnet.training.labels.Direction at 0x7fb365023b10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderWithDirectionReconstruction().eval()\n",
    "event = torch.rand(1, 100, 8)\n",
    "mask = torch.ones(1, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (1, 100))\n",
    "label = torch.rand(1, 2)\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id, label=label)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MOLECULAR TRANFORMER\n",
    "DIST_KERNELS = {\n",
    "    \"exp\": {\n",
    "        \"fn\": lambda t: torch.exp(-t),\n",
    "        \"mask_value_fn\": lambda t: torch.finfo(t.dtype).max,\n",
    "    },\n",
    "    \"softmax\": {\n",
    "        \"fn\": lambda t: torch.softmax(t, dim=-1),\n",
    "        \"mask_value_fn\": lambda t: -torch.finfo(t.dtype).max,\n",
    "    },\n",
    "}\n",
    "\n",
    "# helpers\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return d if not exists(val) else val\n",
    "\n",
    "\n",
    "# helper classes\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x + self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForwardV1(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, heads=8, dim_head=64, Lg=0.5, Ld=0.5, La=1, dist_kernel_fn=\"exp\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        # hyperparameters controlling the weighted linear combination from\n",
    "        # self-attention (La)\n",
    "        # adjacency graph (Lg)\n",
    "        # pair-wise distance matrix (Ld)\n",
    "\n",
    "        self.La = La\n",
    "        self.Ld = Ld\n",
    "        self.Lg = Lg\n",
    "\n",
    "        self.dist_kernel_fn = dist_kernel_fn\n",
    "\n",
    "    def forward(self, x, mask=None, adjacency_mat=None, distance_mat=None):\n",
    "        h, La, Ld, Lg, dist_kernel_fn = (\n",
    "            self.heads,\n",
    "            self.La,\n",
    "            self.Ld,\n",
    "            self.Lg,\n",
    "            self.dist_kernel_fn,\n",
    "        )\n",
    "\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, \"b n (h qkv d) -> b h n qkv d\", h=h, qkv=3).unbind(\n",
    "            dim=-2\n",
    "        )\n",
    "        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        assert (\n",
    "            dist_kernel_fn in DIST_KERNELS\n",
    "        ), f\"distance kernel function needs to be one of {DIST_KERNELS.keys()}\"\n",
    "        dist_kernel_config = DIST_KERNELS[dist_kernel_fn]\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = rearrange(distance_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            adjacency_mat = rearrange(adjacency_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = torch.finfo(dots.dtype).max\n",
    "            mask = mask[:, None, :, None] * mask[:, None, None, :]\n",
    "\n",
    "            # mask attention\n",
    "            dots.masked_fill_(~mask, -mask_value)\n",
    "\n",
    "            if exists(distance_mat):\n",
    "                # mask distance to infinity\n",
    "                # todo - make sure for softmax distance kernel, use -infinity\n",
    "                dist_mask_value = dist_kernel_config[\"mask_value_fn\"](dots)\n",
    "                distance_mat.masked_fill_(~mask, dist_mask_value)\n",
    "\n",
    "            if exists(adjacency_mat):\n",
    "                adjacency_mat.masked_fill_(~mask, 0.0)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # sum contributions from adjacency and distance tensors\n",
    "        attn = attn * La\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            attn = attn + Lg * adjacency_mat\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = dist_kernel_config[\"fn\"](distance_mat)\n",
    "            attn = attn + Ld * distance_mat\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class MAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MATMaskedPool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = self.pool(x, mask=mask)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMAT(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MAT(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMATMasked(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MATMaskedPool(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderMATMasked()\n",
    "event = torch.randn(2, 100, 6)\n",
    "mask = torch.ones(2, 100).bool()\n",
    "adjecent_matrix = torch.empty(2, 100, 100).random_(2).float()\n",
    "distance_matrix = torch.randn(2, 100, 100)\n",
    "batch = dict(\n",
    "    event=event,\n",
    "    mask=mask,\n",
    "    adjecent_matrix=adjecent_matrix,\n",
    "    distance_matrix=distance_matrix,\n",
    ")\n",
    "out = model(batch)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu115'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum, broadcast_tensors\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def safe_div(num, den, eps = 1e-8):\n",
    "    res = num.div(den.clamp(min = eps))\n",
    "    res.masked_fill_(den == 0, 0.)\n",
    "    return res\n",
    "\n",
    "def batched_index_select(values, indices, dim = 1):\n",
    "    value_dims = values.shape[(dim + 1):]\n",
    "    values_shape, indices_shape = map(lambda t: list(t.shape), (values, indices))\n",
    "    indices = indices[(..., *((None,) * len(value_dims)))]\n",
    "    indices = indices.expand(*((-1,) * len(indices_shape)), *value_dims)\n",
    "    value_expand_len = len(indices_shape) - (dim + 1)\n",
    "    values = values[(*((slice(None),) * dim), *((None,) * value_expand_len), ...)]\n",
    "\n",
    "    value_expand_shape = [-1] * len(values.shape)\n",
    "    expand_slice = slice(dim, (dim + value_expand_len))\n",
    "    value_expand_shape[expand_slice] = indices.shape[expand_slice]\n",
    "    values = values.expand(*value_expand_shape)\n",
    "\n",
    "    dim += value_expand_len\n",
    "    return values.gather(dim, indices)\n",
    "\n",
    "def fourier_encode_dist(x, num_encodings = 4, include_self = True):\n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtype, x\n",
    "    scales = 2 ** torch.arange(num_encodings, device = device, dtype = dtype)\n",
    "    x = x / scales\n",
    "    x = torch.cat([x.sin(), x.cos()], dim=-1)\n",
    "    x = torch.cat((x, orig_x), dim = -1) if include_self else x\n",
    "    return x\n",
    "\n",
    "def embedd_token(x, dims, layers):\n",
    "    stop_concat = -len(dims)\n",
    "    to_embedd = x[:, stop_concat:].long()\n",
    "    for i,emb_layer in enumerate(layers):\n",
    "        # the portion corresponding to `to_embedd` part gets dropped\n",
    "        x = torch.cat([ x[:, :stop_concat], \n",
    "                        emb_layer( to_embedd[:, i] ) \n",
    "                      ], dim=-1)\n",
    "        stop_concat = x.shape[-1]\n",
    "    return x\n",
    "\n",
    "# swish activation fallback\n",
    "\n",
    "class Swish_(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x.sigmoid()\n",
    "\n",
    "SiLU = nn.SiLU if hasattr(nn, 'SiLU') else Swish_\n",
    "\n",
    "# helper classes\n",
    "\n",
    "# this follows the same strategy for normalization as done in SE3 Transformers\n",
    "# https://github.com/lucidrains/se3-transformer-pytorch/blob/main/se3_transformer_pytorch/se3_transformer_pytorch.py#L95\n",
    "\n",
    "class CoorsNorm(nn.Module):\n",
    "    def __init__(self, eps = 1e-8, scale_init = 1.):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        scale = torch.zeros(1).fill_(scale_init)\n",
    "        self.scale = nn.Parameter(scale)\n",
    "\n",
    "    def forward(self, coors):\n",
    "        norm = coors.norm(dim = -1, keepdim = True)\n",
    "        normed_coors = coors / norm.clamp(min = self.eps)\n",
    "        return normed_coors * self.scale\n",
    "\n",
    "# global linear attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = heads * dim_head\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x, context, mask = None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        kv = self.to_kv(context).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = -torch.finfo(dots.dtype).max\n",
    "            mask = rearrange(mask, 'b n -> b () () n')\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        attn = dots.softmax(dim = -1)\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class GlobalLinearAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_seq = nn.LayerNorm(dim)\n",
    "        self.norm_queries = nn.LayerNorm(dim)\n",
    "        self.attn1 = Attention(dim, heads, dim_head)\n",
    "        self.attn2 = Attention(dim, heads, dim_head)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, queries, mask = None):\n",
    "        res_x, res_queries = x, queries\n",
    "        x, queries = self.norm_seq(x), self.norm_queries(queries)\n",
    "\n",
    "        induced = self.attn1(queries, x, mask = mask)\n",
    "        out     = self.attn2(x, induced)\n",
    "\n",
    "        x =  out + res_x\n",
    "        queries = induced + res_queries\n",
    "\n",
    "        x = self.ff(x) + x\n",
    "        return x, queries\n",
    "\n",
    "# classes\n",
    "\n",
    "class EGNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        edge_dim = 0,\n",
    "        m_dim = 16,\n",
    "        fourier_features = 0,\n",
    "        num_nearest_neighbors = 0,\n",
    "        dropout = 0.0,\n",
    "        init_eps = 1e-3,\n",
    "        norm_feats = False,\n",
    "        norm_coors = False,\n",
    "        norm_coors_scale_init = 1e-2,\n",
    "        update_feats = True,\n",
    "        update_coors = True,\n",
    "        only_sparse_neighbors = False,\n",
    "        valid_radius = float('inf'),\n",
    "        m_pool_method = 'sum',\n",
    "        soft_edges = False,\n",
    "        coor_weights_clamp_value = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert m_pool_method in {'sum', 'mean'}, 'pool method must be either sum or mean'\n",
    "        assert update_feats or update_coors, 'you must update either features, coordinates, or both'\n",
    "\n",
    "        self.fourier_features = fourier_features\n",
    "\n",
    "        edge_input_dim = (fourier_features * 2) + (dim * 2) + edge_dim + 1\n",
    "        dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_input_dim, edge_input_dim * 2),\n",
    "            dropout,\n",
    "            SiLU(),\n",
    "            nn.Linear(edge_input_dim * 2, m_dim),\n",
    "            SiLU()\n",
    "        )\n",
    "\n",
    "        self.edge_gate = nn.Sequential(\n",
    "            nn.Linear(m_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        ) if soft_edges else None\n",
    "\n",
    "        self.node_norm = nn.LayerNorm(dim) if norm_feats else nn.Identity()\n",
    "        self.coors_norm = CoorsNorm(scale_init = norm_coors_scale_init) if norm_coors else nn.Identity()\n",
    "\n",
    "        self.m_pool_method = m_pool_method\n",
    "\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(dim + m_dim, dim * 2),\n",
    "            dropout,\n",
    "            SiLU(),\n",
    "            nn.Linear(dim * 2, dim),\n",
    "        ) if update_feats else None\n",
    "\n",
    "        self.coors_mlp = nn.Sequential(\n",
    "            nn.Linear(m_dim, m_dim * 4),\n",
    "            dropout,\n",
    "            SiLU(),\n",
    "            nn.Linear(m_dim * 4, 1)\n",
    "        ) if update_coors else None\n",
    "\n",
    "        self.num_nearest_neighbors = num_nearest_neighbors\n",
    "        self.only_sparse_neighbors = only_sparse_neighbors\n",
    "        self.valid_radius = valid_radius\n",
    "\n",
    "        self.coor_weights_clamp_value = coor_weights_clamp_value\n",
    "\n",
    "        self.init_eps = init_eps\n",
    "        self.apply(self.init_)\n",
    "\n",
    "    def init_(self, module):\n",
    "        if type(module) in {nn.Linear}:\n",
    "            # seems to be needed to keep the network from exploding to NaN with greater depths\n",
    "            nn.init.normal_(module.weight, std = self.init_eps)\n",
    "\n",
    "    def forward(self, feats, coors, edges = None, mask = None, adj_mat = None):\n",
    "        b, n, d, device, fourier_features, num_nearest, valid_radius, only_sparse_neighbors = *feats.shape, feats.device, self.fourier_features, self.num_nearest_neighbors, self.valid_radius, self.only_sparse_neighbors\n",
    "\n",
    "        if exists(mask):\n",
    "            num_nodes = mask.sum(dim = -1)\n",
    "\n",
    "        use_nearest = num_nearest > 0 or only_sparse_neighbors\n",
    "\n",
    "        rel_coors = rearrange(coors, 'b i d -> b i () d') - rearrange(coors, 'b j d -> b () j d')\n",
    "        rel_dist = (rel_coors ** 2).sum(dim = -1, keepdim = True)\n",
    "\n",
    "        i = j = n\n",
    "\n",
    "        if use_nearest:\n",
    "            ranking = rel_dist[..., 0].clone()\n",
    "\n",
    "            if exists(mask):\n",
    "                rank_mask = mask[:, :, None] * mask[:, None, :]\n",
    "                ranking.masked_fill_(~rank_mask, 1e5)\n",
    "\n",
    "            if exists(adj_mat):\n",
    "                if len(adj_mat.shape) == 2:\n",
    "                    adj_mat = repeat(adj_mat.clone(), 'i j -> b i j', b = b)\n",
    "\n",
    "                if only_sparse_neighbors:\n",
    "                    num_nearest = int(adj_mat.float().sum(dim = -1).max().item())\n",
    "                    valid_radius = 0\n",
    "\n",
    "                self_mask = rearrange(torch.eye(n, device = device, dtype = torch.bool), 'i j -> () i j')\n",
    "\n",
    "                adj_mat = adj_mat.masked_fill(self_mask, False)\n",
    "                ranking.masked_fill_(self_mask, -1.)\n",
    "                ranking.masked_fill_(adj_mat, 0.)\n",
    "\n",
    "            nbhd_ranking, nbhd_indices = ranking.topk(num_nearest, dim = -1, largest = False)\n",
    "\n",
    "            nbhd_mask = nbhd_ranking <= valid_radius\n",
    "\n",
    "            rel_coors = batched_index_select(rel_coors, nbhd_indices, dim = 2)\n",
    "            rel_dist = batched_index_select(rel_dist, nbhd_indices, dim = 2)\n",
    "\n",
    "            if exists(edges):\n",
    "                edges = batched_index_select(edges, nbhd_indices, dim = 2)\n",
    "\n",
    "            j = num_nearest\n",
    "\n",
    "        if fourier_features > 0:\n",
    "            rel_dist = fourier_encode_dist(rel_dist, num_encodings = fourier_features)\n",
    "            rel_dist = rearrange(rel_dist, 'b i j () d -> b i j d')\n",
    "\n",
    "        if use_nearest:\n",
    "            feats_j = batched_index_select(feats, nbhd_indices, dim = 1)\n",
    "        else:\n",
    "            feats_j = rearrange(feats, 'b j d -> b () j d')\n",
    "\n",
    "        feats_i = rearrange(feats, 'b i d -> b i () d')\n",
    "        feats_i, feats_j = broadcast_tensors(feats_i, feats_j)\n",
    "\n",
    "        edge_input = torch.cat((feats_i, feats_j, rel_dist), dim = -1)\n",
    "\n",
    "        if exists(edges):\n",
    "            edge_input = torch.cat((edge_input, edges), dim = -1)\n",
    "\n",
    "        m_ij = self.edge_mlp(edge_input)\n",
    "\n",
    "        if exists(self.edge_gate):\n",
    "            m_ij = m_ij * self.edge_gate(m_ij)\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_i = rearrange(mask, 'b i -> b i ()')\n",
    "\n",
    "            if use_nearest:\n",
    "                mask_j = batched_index_select(mask, nbhd_indices, dim = 1)\n",
    "                mask = (mask_i * mask_j) & nbhd_mask\n",
    "            else:\n",
    "                mask_j = rearrange(mask, 'b j -> b () j')\n",
    "                mask = mask_i * mask_j\n",
    "\n",
    "        if exists(self.coors_mlp):\n",
    "            coor_weights = self.coors_mlp(m_ij)\n",
    "            coor_weights = rearrange(coor_weights, 'b i j () -> b i j')\n",
    "\n",
    "            rel_coors = self.coors_norm(rel_coors)\n",
    "\n",
    "            if exists(mask):\n",
    "                coor_weights.masked_fill_(~mask, 0.)\n",
    "\n",
    "            if exists(self.coor_weights_clamp_value):\n",
    "                clamp_value = self.coor_weights_clamp_value\n",
    "                coor_weights.clamp_(min = -clamp_value, max = clamp_value)\n",
    "\n",
    "            coors_out = einsum('b i j, b i j c -> b i c', coor_weights, rel_coors) + coors\n",
    "        else:\n",
    "            coors_out = coors\n",
    "\n",
    "        if exists(self.node_mlp):\n",
    "            if exists(mask):\n",
    "                m_ij_mask = rearrange(mask, '... -> ... ()')\n",
    "                m_ij = m_ij.masked_fill(~m_ij_mask, 0.)\n",
    "\n",
    "            if self.m_pool_method == 'mean':\n",
    "                if exists(mask):\n",
    "                    # masked mean\n",
    "                    mask_sum = m_ij_mask.sum(dim = -2)\n",
    "                    m_i = safe_div(m_ij.sum(dim = -2), mask_sum)\n",
    "                else:\n",
    "                    m_i = m_ij.mean(dim = -2)\n",
    "\n",
    "            elif self.m_pool_method == 'sum':\n",
    "                m_i = m_ij.sum(dim = -2)\n",
    "\n",
    "            normed_feats = self.node_norm(feats)\n",
    "            node_mlp_input = torch.cat((normed_feats, m_i), dim = -1)\n",
    "            node_out = self.node_mlp(node_mlp_input) + feats\n",
    "        else:\n",
    "            node_out = feats\n",
    "\n",
    "        return node_out, coors_out\n",
    "\n",
    "class EGNN_Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth,\n",
    "        dim,\n",
    "        num_tokens = None,\n",
    "        num_edge_tokens = None,\n",
    "        num_positions = None,\n",
    "        edge_dim = 0,\n",
    "        num_adj_degrees = None,\n",
    "        adj_dim = 0,\n",
    "        global_linear_attn_every = 0,\n",
    "        global_linear_attn_heads = 8,\n",
    "        global_linear_attn_dim_head = 64,\n",
    "        num_global_tokens = 4,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (exists(num_adj_degrees) and num_adj_degrees < 1), 'make sure adjacent degrees is greater than 1'\n",
    "        self.num_positions = num_positions\n",
    "\n",
    "        self.token_emb = nn.Linear(num_tokens, dim) if exists(num_tokens) else None\n",
    "        self.pos_emb = nn.Embedding(num_positions, dim) if exists(num_positions) else None\n",
    "        self.edge_emb = nn.Embedding(num_edge_tokens, edge_dim) if exists(num_edge_tokens) else None\n",
    "        self.has_edges = edge_dim > 0\n",
    "\n",
    "        self.num_adj_degrees = num_adj_degrees\n",
    "        self.adj_emb = nn.Embedding(num_adj_degrees + 1, adj_dim) if exists(num_adj_degrees) and adj_dim > 0 else None\n",
    "\n",
    "        edge_dim = edge_dim if self.has_edges else 0\n",
    "        adj_dim = adj_dim if exists(num_adj_degrees) else 0\n",
    "\n",
    "        has_global_attn = global_linear_attn_every > 0\n",
    "        self.global_tokens = None\n",
    "        if has_global_attn:\n",
    "            self.global_tokens = nn.Parameter(torch.randn(num_global_tokens, dim))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for ind in range(depth):\n",
    "            is_global_layer = has_global_attn and (ind % global_linear_attn_every) == 0\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                GlobalLinearAttention(dim = dim, heads = global_linear_attn_heads, dim_head = global_linear_attn_dim_head) if is_global_layer else None,\n",
    "                EGNN(dim = dim, edge_dim = (edge_dim + adj_dim), norm_feats = True, **kwargs),\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feats,\n",
    "        coors,\n",
    "        adj_mat = None,\n",
    "        edges = None,\n",
    "        mask = None,\n",
    "        return_coor_changes = False\n",
    "    ):\n",
    "        b, device = feats.shape[0], feats.device\n",
    "\n",
    "        if exists(self.token_emb):\n",
    "            feats = self.token_emb(feats)\n",
    "            print(feats.shape)\n",
    "\n",
    "        if exists(self.pos_emb):\n",
    "            n = feats.shape[1]\n",
    "            assert n <= self.num_positions, f'given sequence length {n} must be less than the number of positions {self.num_positions} set at init'\n",
    "            pos_emb = self.pos_emb(torch.arange(n, device = device))\n",
    "            feats += rearrange(pos_emb, 'n d -> () n d')\n",
    "\n",
    "        if exists(edges) and exists(self.edge_emb):\n",
    "            edges = self.edge_emb(edges)\n",
    "\n",
    "        # create N-degrees adjacent matrix from 1st degree connections\n",
    "        if exists(self.num_adj_degrees):\n",
    "            assert exists(adj_mat), 'adjacency matrix must be passed in (keyword argument adj_mat)'\n",
    "\n",
    "            if len(adj_mat.shape) == 2:\n",
    "                adj_mat = repeat(adj_mat.clone(), 'i j -> b i j', b = b)\n",
    "\n",
    "            adj_indices = adj_mat.clone().long()\n",
    "\n",
    "            for ind in range(self.num_adj_degrees - 1):\n",
    "                degree = ind + 2\n",
    "\n",
    "                next_degree_adj_mat = (adj_mat.float() @ adj_mat.float()) > 0\n",
    "                next_degree_mask = (next_degree_adj_mat.float() - adj_mat.float()).bool()\n",
    "                adj_indices.masked_fill_(next_degree_mask, degree)\n",
    "                adj_mat = next_degree_adj_mat.clone()\n",
    "\n",
    "            if exists(self.adj_emb):\n",
    "                adj_emb = self.adj_emb(adj_indices)\n",
    "                edges = torch.cat((edges, adj_emb), dim = -1) if exists(edges) else adj_emb\n",
    "\n",
    "        # setup global attention\n",
    "\n",
    "        global_tokens = None\n",
    "        if exists(self.global_tokens):\n",
    "            global_tokens = repeat(self.global_tokens, 'n d -> b n d', b = b)\n",
    "\n",
    "        # go through layers\n",
    "\n",
    "        coor_changes = [coors]\n",
    "\n",
    "        for global_attn, egnn in self.layers:\n",
    "            if exists(global_attn):\n",
    "                feats, global_tokens = global_attn(feats, global_tokens, mask = mask)\n",
    "\n",
    "            feats, coors = egnn(feats, coors, adj_mat = adj_mat, edges = edges, mask = mask)\n",
    "            coor_changes.append(coors)\n",
    "\n",
    "        if return_coor_changes:\n",
    "            return feats, coors, coor_changes\n",
    "\n",
    "        return feats, coors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EGNN_Network(\n",
    "    num_tokens = 8,\n",
    "    num_positions = 200,           # unless what you are passing in is an unordered set, set this to the maximum sequence length\n",
    "    dim = 128,\n",
    "    depth = 3,\n",
    "    num_nearest_neighbors = 8,\n",
    "    coor_weights_clamp_value = 2.   # absolute clamped value for the coordinate weights, needed if you increase the num neareest neighbors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 128])\n"
     ]
    }
   ],
   "source": [
    "#feats = torch.randint(0, 21, (1, 200)) # (1, 1024)\n",
    "feats = torch.rand(1, 200, 8)         # (1, 1024, 3)\n",
    "coors = torch.randn(1, 200, 4)         # (1, 1024, 3)\n",
    "mask = torch.ones_like(torch.rand(1, 200)).bool()    # (1, 1024)\n",
    "\n",
    "feats_out, coors_out = net(feats, coors, mask = mask) # (1, 1024, 32), (1, 1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.token_emb(feats).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5054,  0.9501,  0.2648,  ..., -2.0167,  0.0553,  0.1345],\n",
       "         [ 0.2475,  0.0620, -0.2409,  ..., -2.4148, -0.4581,  0.6646],\n",
       "         [ 1.1906,  1.0411,  1.4702,  ...,  1.1684,  0.7786, -0.9356],\n",
       "         ...,\n",
       "         [ 0.5450, -3.1338, -1.2814,  ..., -0.1862, -1.1438,  0.6735],\n",
       "         [ 1.0932,  1.0806,  2.2097,  ..., -0.8507, -1.7636,  2.1408],\n",
       "         [ 2.1567, -0.6222, -2.5048,  ..., -1.9115,  1.3361,  0.9164]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
