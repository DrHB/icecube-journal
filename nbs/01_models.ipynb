{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "from abc import abstractmethod\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "import scipy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "class SigmoidRange(nn.Module):\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * (self.high - self.low) + self.low\n",
    "\n",
    "\n",
    "class Adjustoutput(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.az = SigmoidRange(6.436839548775502e-08, 6.2891)\n",
    "        self.zn = SigmoidRange(8.631674577710722e-05, 3.1417)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x[:, 0] = self.az(x[:, 0])\n",
    "        x[:, 1] = self.zn(x[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolingWithMask(nn.Module):\n",
    "    def __init__(self, pool_type):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        if self.pool_type == \"mean\":\n",
    "            # Sum the values along the sequence dimension\n",
    "            x = torch.sum(x, dim=1)\n",
    "\n",
    "            # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "            x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "        elif self.pool_type == \"max\":\n",
    "            # Find the maximum value along the sequence dimension\n",
    "            x, _ = torch.max(x, dim=1)\n",
    "        elif self.pool_type == \"min\":\n",
    "            # Find the minimum value along the sequence dimension\n",
    "            x, _ = torch.min(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pool_type. Choose from ['mean', 'max', 'min']\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=6,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        # self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1CombinePool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask('mean')\n",
    "        self.pool_max = PoolingWithMask('max')\n",
    "        self.head = FeedForward(128 * 2, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class always:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def l2norm(t, groups=1):\n",
    "    t = rearrange(t, \"... (g d) -> ... g d\", g=groups)\n",
    "    t = F.normalize(t, p=2, dim=-1)\n",
    "    return rearrange(t, \"... g d -> ... (g d)\")\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinng(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=14):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV1(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TokenEmbeddingV2(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV2(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=196,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV3(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "        self.sigmout = Adjustoutput()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        s = self.sigmout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderV2CombinePool().eval()\n",
    "event = torch.rand(1, 100, 8)\n",
    "mask = torch.ones(1, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (1, 100))\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class LogCMK(torch.autograd.Function):\n",
    "    \"\"\"MIT License.\n",
    "    Copyright (c) 2019 Max Ryabinin\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "    of this software and associated documentation files (the \"Software\"), to deal\n",
    "    in the Software without restriction, including without limitation the rights\n",
    "    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "    copies of the Software, and to permit persons to whom the Software is\n",
    "    furnished to do so, subject to the following conditions:\n",
    "    The above copyright notice and this permission notice shall be included in all\n",
    "    copies or substantial portions of the Software.\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "    SOFTWARE.\n",
    "    _____________________\n",
    "    From [https://github.com/mryab/vmf_loss/blob/master/losses.py]\n",
    "    Modified to use modified Bessel function instead of exponentially scaled ditto\n",
    "    (i.e. `.ive` -> `.iv`) as indiciated in [1812.04616] in spite of suggestion in\n",
    "    Sec. 8.2 of this paper. The change has been validated through comparison with\n",
    "    exact calculations for `m=2` and `m=3` and found to yield the correct results.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx: Any, m: int, kappa: Tensor\n",
    "    ) -> Tensor:  # pylint: disable=invalid-name,arguments-differ\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        dtype = kappa.dtype\n",
    "        ctx.save_for_backward(kappa)\n",
    "        ctx.m = m\n",
    "        ctx.dtype = dtype\n",
    "        kappa = kappa.double()\n",
    "        iv = torch.from_numpy(\n",
    "            scipy.special.iv(m / 2.0 - 1, kappa.cpu().numpy())\n",
    "        ).to(kappa.device)\n",
    "        return (\n",
    "            (m / 2.0 - 1) * torch.log(kappa)\n",
    "            - torch.log(iv)\n",
    "            - (m / 2) * np.log(2 * np.pi)\n",
    "        ).type(dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx: Any, grad_output: Tensor\n",
    "    ) -> Tensor:  # pylint: disable=invalid-name,arguments-differ\n",
    "        \"\"\"Backward pass.\"\"\"\n",
    "        kappa = ctx.saved_tensors[0]\n",
    "        m = ctx.m\n",
    "        dtype = ctx.dtype\n",
    "        kappa = kappa.double().cpu().numpy()\n",
    "        grads = -(\n",
    "            (scipy.special.iv(m / 2.0, kappa))\n",
    "            / (scipy.special.iv(m / 2.0 - 1, kappa))\n",
    "        )\n",
    "        return (\n",
    "            None,\n",
    "            grad_output\n",
    "            * torch.from_numpy(grads).to(grad_output.device).type(dtype),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class LossFunction(nn.Module):\n",
    "    \"\"\"Base class for loss functions in `graphnet`.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(  # type: ignore[override]\n",
    "        self,\n",
    "        prediction: Tensor,\n",
    "        target: Tensor,\n",
    "        weights: Optional[Tensor] = None,\n",
    "        return_elements: bool = False,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Forward pass for all loss functions.\n",
    "        Args:\n",
    "            prediction: Tensor containing predictions. Shape [N,P]\n",
    "            target: Tensor containing targets. Shape [N,T]\n",
    "            return_elements: Whether elementwise loss terms should be returned.\n",
    "                The alternative is to return the averaged loss across examples.\n",
    "        Returns:\n",
    "            Loss, either averaged to a scalar (if `return_elements = False`) or\n",
    "            elementwise terms with shape [N,] (if `return_elements = True`).\n",
    "        \"\"\"\n",
    "        elements = self._forward(prediction, target)\n",
    "        if weights is not None:\n",
    "            elements = elements * weights\n",
    "        assert elements.size(dim=0) == target.size(\n",
    "            dim=0\n",
    "        ), \"`_forward` should return elementwise loss terms.\"\n",
    "\n",
    "        return elements if return_elements else torch.mean(elements)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _forward(self, prediction: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"Syntax like `.forward`, for implentation in inheriting classes.\"\"\"\n",
    "\n",
    "class VonMisesFisherLoss(LossFunction):\n",
    "    \"\"\"General class for calculating von Mises-Fisher loss.\n",
    "    Requires implementation for specific dimension `m` in which the target and\n",
    "    prediction vectors need to be prepared.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def log_cmk_exact(\n",
    "        cls, m: int, kappa: Tensor\n",
    "    ) -> Tensor:  # pylint: disable=invalid-name\n",
    "        \"\"\"Calculate $log C_{m}(k)$ term in von Mises-Fisher loss exactly.\"\"\"\n",
    "        return LogCMK.apply(m, kappa)\n",
    "\n",
    "    @classmethod\n",
    "    def log_cmk_approx(\n",
    "        cls, m: int, kappa: Tensor\n",
    "    ) -> Tensor:  # pylint: disable=invalid-name\n",
    "        \"\"\"Calculate $log C_{m}(k)$ term in von Mises-Fisher loss approx.\n",
    "        [https://arxiv.org/abs/1812.04616] Sec. 8.2 with additional minus sign.\n",
    "        \"\"\"\n",
    "        v = m / 2.0 - 0.5\n",
    "        a = torch.sqrt((v + 1) ** 2 + kappa**2)\n",
    "        b = v - 1\n",
    "        return -a + b * torch.log(b + a)\n",
    "\n",
    "    @classmethod\n",
    "    def log_cmk(\n",
    "        cls, m: int, kappa: Tensor, kappa_switch: float = 100.0\n",
    "    ) -> Tensor:  # pylint: disable=invalid-name\n",
    "        \"\"\"Calculate $log C_{m}(k)$ term in von Mises-Fisher loss.\n",
    "        Since `log_cmk_exact` is diverges for `kappa` >~ 700 (using float64\n",
    "        precision), and since `log_cmk_approx` is unaccurate for small `kappa`,\n",
    "        this method automatically switches between the two at `kappa_switch`,\n",
    "        ensuring continuity at this point.\n",
    "        \"\"\"\n",
    "        kappa_switch = torch.tensor([kappa_switch]).to(kappa.device)\n",
    "        mask_exact = kappa < kappa_switch\n",
    "\n",
    "        # Ensure continuity at `kappa_switch`\n",
    "        offset = cls.log_cmk_approx(m, kappa_switch) - cls.log_cmk_exact(\n",
    "            m, kappa_switch\n",
    "        )\n",
    "        ret = cls.log_cmk_approx(m, kappa) - offset\n",
    "        ret[mask_exact] = cls.log_cmk_exact(m, kappa[mask_exact])\n",
    "        return ret\n",
    "\n",
    "    def _evaluate(self, prediction: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"Calculate von Mises-Fisher loss for a vector in D dimensons.\n",
    "        This loss utilises the von Mises-Fisher distribution, which is a\n",
    "        probability distribution on the (D - 1) sphere in D-dimensional space.\n",
    "        Args:\n",
    "            prediction: Predicted vector, of shape [batch_size, D].\n",
    "            target: Target unit vector, of shape [batch_size, D].\n",
    "        Returns:\n",
    "            Elementwise von Mises-Fisher loss terms.\n",
    "        \"\"\"\n",
    "        # Check(s)\n",
    "        assert prediction.dim() == 2\n",
    "        assert target.dim() == 2\n",
    "        assert prediction.size() == target.size()\n",
    "\n",
    "        # Computing loss\n",
    "        m = target.size()[1]\n",
    "        k = torch.norm(prediction, dim=1)\n",
    "        dotprod = torch.sum(prediction * target, dim=1)\n",
    "        elements = -self.log_cmk(m, k) - dotprod\n",
    "        return elements\n",
    "\n",
    "    @abstractmethod\n",
    "    def _forward(self, prediction: Tensor, target: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class VonMisesFisher2DLoss(VonMisesFisherLoss):\n",
    "    \"\"\"von Mises-Fisher loss function vectors in the 2D plane.\"\"\"\n",
    "\n",
    "    def _forward(self, prediction: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"Calculate von Mises-Fisher loss for an angle in the 2D plane.\n",
    "        Args:\n",
    "            prediction: Output of the model. Must have shape [N, 2] where 0th\n",
    "                column is a prediction of `angle` and 1st column is an estimate\n",
    "                of `kappa`.\n",
    "            target: Target tensor, extracted from graph object.\n",
    "        Returns:\n",
    "            loss: Elementwise von Mises-Fisher loss terms. Shape [N,]\n",
    "        \"\"\"\n",
    "        # Check(s)\n",
    "        assert prediction.dim() == 2 and prediction.size()[1] == 2\n",
    "        assert target.dim() == 2\n",
    "        assert prediction.size()[0] == target.size()[0]\n",
    "\n",
    "        # Formatting target\n",
    "        angle_true = target[:, 0]\n",
    "        t = torch.stack(\n",
    "            [\n",
    "                torch.cos(angle_true),\n",
    "                torch.sin(angle_true),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Formatting prediction\n",
    "        angle_pred = prediction[:, 0]\n",
    "        kappa = prediction[:, 1]\n",
    "        p = kappa.unsqueeze(1) * torch.stack(\n",
    "            [\n",
    "                torch.cos(angle_pred),\n",
    "                torch.sin(angle_pred),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return self._evaluate(p, t)\n",
    "\n",
    "class VonMisesFisher3DLoss(VonMisesFisherLoss):\n",
    "    \"\"\"von Mises-Fisher loss function vectors in the 3D plane.\"\"\"\n",
    "\n",
    "    def _forward(self, prediction: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"Calculate von Mises-Fisher loss for a direction in the 3D.\n",
    "        Args:\n",
    "            prediction: Output of the model. Must have shape [N, 4] where\n",
    "                columns 0, 1, 2 are predictions of `direction` and last column\n",
    "                is an estimate of `kappa`.\n",
    "            target: Target tensor, extracted from graph object.\n",
    "        Returns:\n",
    "            Elementwise von Mises-Fisher loss terms. Shape [N,]\n",
    "        \"\"\"\n",
    "        target = target.reshape(-1, 3)\n",
    "        # Check(s)\n",
    "        assert prediction.dim() == 2 and prediction.size()[1] == 4\n",
    "        assert target.dim() == 2\n",
    "        assert prediction.size()[0] == target.size()[0]\n",
    "\n",
    "        kappa = prediction[:, 3]\n",
    "        p = kappa.unsqueeze(1) * prediction[:, [0, 1, 2]]\n",
    "        return self._evaluate(p, target)\n",
    "\n",
    "def eps_like(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Return `eps` matching `tensor`'s dtype.\"\"\"\n",
    "    return torch.finfo(tensor.dtype).eps\n",
    "\n",
    "class AzimuthReconstructionWithKappa(nn.Module):\n",
    "    \"\"\"Module for predicting azimuth angle and kappa.\"\"\"\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear(x)\n",
    "        kappa = torch.linalg.vector_norm(x, dim=1) + eps_like(x)\n",
    "        angle = torch.atan2(x[:, 1], x[:, 0])\n",
    "        angle = torch.where(\n",
    "            angle < 0, angle + 2 * np.pi, angle\n",
    "        )  \n",
    "        return torch.stack((angle, kappa), dim=1)\n",
    "        \n",
    "class DirectionReconstructionWithKappa(nn.Module):\n",
    "    \"\"\"Module for predicting direction and kappa.\"\"\"\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 3)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Transform outputs to angle and prepare prediction\n",
    "        x = self.linear(x)\n",
    "        kappa = torch.linalg.vector_norm(x, dim=1) + eps_like(x)\n",
    "        vec_x = x[:, 0] / kappa\n",
    "        vec_y = x[:, 1] / kappa\n",
    "        vec_z = x[:, 2] / kappa\n",
    "        return torch.stack((vec_x, vec_y, vec_z, kappa), dim=1)\n",
    "\n",
    "class ZenithReconstruction(nn.Module):\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x[:, :1]) * np.pi\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.out = DirectionReconstructionWithKappa(128)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch['event'], batch['mask']\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = IceCubeModelEncoderV2()\n",
    "event = torch.rand(5, 100, 8)\n",
    "mask = torch.ones(5, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (5, 100))\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id)\n",
    "with torch.no_grad():\n",
    "    out = md(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MOLECULAR TRANFORMER\n",
    "DIST_KERNELS = {\n",
    "    \"exp\": {\n",
    "        \"fn\": lambda t: torch.exp(-t),\n",
    "        \"mask_value_fn\": lambda t: torch.finfo(t.dtype).max,\n",
    "    },\n",
    "    \"softmax\": {\n",
    "        \"fn\": lambda t: torch.softmax(t, dim=-1),\n",
    "        \"mask_value_fn\": lambda t: -torch.finfo(t.dtype).max,\n",
    "    },\n",
    "}\n",
    "\n",
    "# helpers\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return d if not exists(val) else val\n",
    "\n",
    "\n",
    "# helper classes\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x + self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForwardV1(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, heads=8, dim_head=64, Lg=0.5, Ld=0.5, La=1, dist_kernel_fn=\"exp\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        # hyperparameters controlling the weighted linear combination from\n",
    "        # self-attention (La)\n",
    "        # adjacency graph (Lg)\n",
    "        # pair-wise distance matrix (Ld)\n",
    "\n",
    "        self.La = La\n",
    "        self.Ld = Ld\n",
    "        self.Lg = Lg\n",
    "\n",
    "        self.dist_kernel_fn = dist_kernel_fn\n",
    "\n",
    "    def forward(self, x, mask=None, adjacency_mat=None, distance_mat=None):\n",
    "        h, La, Ld, Lg, dist_kernel_fn = (\n",
    "            self.heads,\n",
    "            self.La,\n",
    "            self.Ld,\n",
    "            self.Lg,\n",
    "            self.dist_kernel_fn,\n",
    "        )\n",
    "\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, \"b n (h qkv d) -> b h n qkv d\", h=h, qkv=3).unbind(\n",
    "            dim=-2\n",
    "        )\n",
    "        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        assert (\n",
    "            dist_kernel_fn in DIST_KERNELS\n",
    "        ), f\"distance kernel function needs to be one of {DIST_KERNELS.keys()}\"\n",
    "        dist_kernel_config = DIST_KERNELS[dist_kernel_fn]\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = rearrange(distance_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            adjacency_mat = rearrange(adjacency_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = torch.finfo(dots.dtype).max\n",
    "            mask = mask[:, None, :, None] * mask[:, None, None, :]\n",
    "\n",
    "            # mask attention\n",
    "            dots.masked_fill_(~mask, -mask_value)\n",
    "\n",
    "            if exists(distance_mat):\n",
    "                # mask distance to infinity\n",
    "                # todo - make sure for softmax distance kernel, use -infinity\n",
    "                dist_mask_value = dist_kernel_config[\"mask_value_fn\"](dots)\n",
    "                distance_mat.masked_fill_(~mask, dist_mask_value)\n",
    "\n",
    "            if exists(adjacency_mat):\n",
    "                adjacency_mat.masked_fill_(~mask, 0.0)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # sum contributions from adjacency and distance tensors\n",
    "        attn = attn * La\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            attn = attn + Lg * adjacency_mat\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = dist_kernel_config[\"fn\"](distance_mat)\n",
    "            attn = attn + Ld * distance_mat\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class MAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MATMaskedPool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = self.pool(x, mask=mask)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMAT(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MAT(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMATMasked(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MATMaskedPool(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderMATMasked()\n",
    "event = torch.randn(2, 100, 6)\n",
    "mask = torch.ones(2, 100).bool()\n",
    "adjecent_matrix = torch.empty(2, 100, 100).random_(2).float()\n",
    "distance_matrix = torch.randn(2, 100, 100)\n",
    "batch = dict(\n",
    "    event=event,\n",
    "    mask=mask,\n",
    "    adjecent_matrix=adjecent_matrix,\n",
    "    distance_matrix=distance_matrix,\n",
    ")\n",
    "out = model(batch)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
