{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/lib/python3.7/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-10 12:36:50 - get_logger - Writing log to \u001b[1mlogs/graphnet_20230210-123650.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/archive/software/graphnet/src')\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "from abc import abstractmethod\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "import scipy\n",
    "import numpy as np\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa, AzimuthReconstructionWithKappa, ZenithReconstruction\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss,  VonMisesFisher2DLoss, EuclideanDistanceLoss\n",
    "from graphnet.training.labels import Direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# | export\n",
    "\n",
    "class VonMisesFisher3DLossCosineSimularityLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (self.vonmis(y_pred, y_true) + (1-self.cosine(y_pred[:, :3], y_true).mean()))/2\n",
    "    \n",
    "    \n",
    "class VonMisesFisher3DLossEcludeLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = EuclideanDistanceLoss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (self.vonmis(y_pred, y_true) + self.cosine(y_pred[:, :3], y_true))/2\n",
    "    \n",
    "class VonMisesFisher3DLossEcludeLossCosine(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine =  nn.CosineSimilarity(dim=1, eps=eps)\n",
    "        self.euclud = EuclideanDistanceLoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (self.vonmis(y_pred, y_true) + \n",
    "                (self.euclud(y_pred[:, :3], y_true) + self.eps) +\n",
    "                (1-self.cosine(y_pred[:, :3], y_true).mean()))/3\n",
    "    \n",
    "\n",
    "\n",
    "class VonMisesFisher2DLossL1Loss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher2DLoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        vm = self.vonmis(y_pred[:, :2], y_true)\n",
    "        l1 = self.l1(y_pred[:, 2], y_true[:, -1])\n",
    "        return (vm + l1)/2\n",
    "        \n",
    "    \n",
    "    \n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "class SigmoidRange(nn.Module):\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * (self.high - self.low) + self.low\n",
    "\n",
    "\n",
    "class Adjustoutput(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.az = SigmoidRange(6.436839548775502e-08, 6.2891)\n",
    "        self.zn = SigmoidRange(8.631674577710722e-05, 3.1417)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x[:, 0] = self.az(x[:, 0])\n",
    "        x[:, 1] = self.zn(x[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolingWithMask(nn.Module):\n",
    "    def __init__(self, pool_type):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        if self.pool_type == \"mean\":\n",
    "            # Sum the values along the sequence dimension\n",
    "            x = torch.sum(x, dim=1)\n",
    "\n",
    "            # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "            x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "        elif self.pool_type == \"max\":\n",
    "            # Find the maximum value along the sequence dimension\n",
    "            x, _ = torch.max(x, dim=1)\n",
    "        elif self.pool_type == \"min\":\n",
    "            # Find the minimum value along the sequence dimension\n",
    "            x, _ = torch.min(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pool_type. Choose from ['mean', 'max', 'min']\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=6,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        # self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1CombinePool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask('mean')\n",
    "        self.pool_max = PoolingWithMask('max')\n",
    "        self.head = FeedForward(128 * 2, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class always:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def l2norm(t, groups=1):\n",
    "    t = rearrange(t, \"... (g d) -> ... g d\", g=groups)\n",
    "    t = F.normalize(t, p=2, dim=-1)\n",
    "    return rearrange(t, \"... g d -> ... (g d)\")\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinng(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=14):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV1(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TokenEmbeddingV2(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV2(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=196,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV3(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "        self.sigmout = Adjustoutput()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        s = self.sigmout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderV1CombinePool().eval()\n",
    "event = torch.rand(1, 100, 8)\n",
    "mask = torch.ones(1, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (1, 100))\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.out = DirectionReconstructionWithKappa(128)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstruction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.pool_min = PoolingWithMask(\"min\")\n",
    "        self.ae = FeedForward(384, 384)\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=384,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), \n",
    "                          self.pool_max(x, mask), \n",
    "                          self.pool_min(x, mask)], dim=1)\n",
    "        x = self.ae(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "class EncoderWithDirectionReconstructionV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "    \n",
    "class EncoderWithDirectionReconstructionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "\n",
    "        self.azimuth_task = AzimuthReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            loss_function=VonMisesFisher2DLoss(),\n",
    "            target_labels=[\"azimuth\", \"kappa\"],\n",
    "        )\n",
    "\n",
    "        self.zenith_task = ZenithReconstruction(\n",
    "            hidden_size=256,\n",
    "            loss_function=nn.L1Loss(),\n",
    "            target_labels=[\"zenith\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        az = self.azimuth_task(x)\n",
    "        zn = self.zenith_task(x)\n",
    "        return torch.concat([az, zn], dim=1)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "\n",
    "        self.azimuth_task = AzimuthReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            loss_function=VonMisesFisher2DLoss(),\n",
    "            target_labels=[\"azimuth\", \"kappa\"],\n",
    "        )\n",
    "\n",
    "        self.zenith_task = ZenithReconstruction(\n",
    "            hidden_size=256,\n",
    "            loss_function=nn.L1Loss(),\n",
    "            target_labels=[\"zenith\"],\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        az = self.azimuth_task(x)\n",
    "        zn = self.zenith_task(x)\n",
    "        return torch.concat([az, zn], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphnet.training.labels.Direction at 0x7f9f90636890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderWithDirectionReconstructionV2().eval()\n",
    "event = torch.rand(10, 100, 9)\n",
    "mask = torch.ones(10, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (10, 100))\n",
    "label = torch.rand(10, 3)\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id, label=label)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5837)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VonMisesFisher3DLossEcludeLossCosine()(y, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EuclideanDistanceLoss()(y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3048, 0.5416, 0.5630],\n",
       "        [0.1234, 0.7242, 0.5422],\n",
       "        [0.2699, 0.1707, 0.5439],\n",
       "        [0.1147, 0.7883, 0.8248],\n",
       "        [0.9983, 0.7349, 0.8983],\n",
       "        [0.0698, 0.4255, 0.5014],\n",
       "        [0.5929, 0.7157, 0.1001],\n",
       "        [0.2983, 0.1875, 0.4487],\n",
       "        [0.1253, 0.5326, 0.3876],\n",
       "        [0.5805, 0.2723, 0.6983]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MOLECULAR TRANFORMER\n",
    "DIST_KERNELS = {\n",
    "    \"exp\": {\n",
    "        \"fn\": lambda t: torch.exp(-t),\n",
    "        \"mask_value_fn\": lambda t: torch.finfo(t.dtype).max,\n",
    "    },\n",
    "    \"softmax\": {\n",
    "        \"fn\": lambda t: torch.softmax(t, dim=-1),\n",
    "        \"mask_value_fn\": lambda t: -torch.finfo(t.dtype).max,\n",
    "    },\n",
    "}\n",
    "\n",
    "# helpers\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return d if not exists(val) else val\n",
    "\n",
    "\n",
    "# helper classes\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x + self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForwardV1(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, heads=8, dim_head=64, Lg=0.5, Ld=0.5, La=1, dist_kernel_fn=\"exp\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        # hyperparameters controlling the weighted linear combination from\n",
    "        # self-attention (La)\n",
    "        # adjacency graph (Lg)\n",
    "        # pair-wise distance matrix (Ld)\n",
    "\n",
    "        self.La = La\n",
    "        self.Ld = Ld\n",
    "        self.Lg = Lg\n",
    "\n",
    "        self.dist_kernel_fn = dist_kernel_fn\n",
    "\n",
    "    def forward(self, x, mask=None, adjacency_mat=None, distance_mat=None):\n",
    "        h, La, Ld, Lg, dist_kernel_fn = (\n",
    "            self.heads,\n",
    "            self.La,\n",
    "            self.Ld,\n",
    "            self.Lg,\n",
    "            self.dist_kernel_fn,\n",
    "        )\n",
    "\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, \"b n (h qkv d) -> b h n qkv d\", h=h, qkv=3).unbind(\n",
    "            dim=-2\n",
    "        )\n",
    "        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        assert (\n",
    "            dist_kernel_fn in DIST_KERNELS\n",
    "        ), f\"distance kernel function needs to be one of {DIST_KERNELS.keys()}\"\n",
    "        dist_kernel_config = DIST_KERNELS[dist_kernel_fn]\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = rearrange(distance_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            adjacency_mat = rearrange(adjacency_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = torch.finfo(dots.dtype).max\n",
    "            mask = mask[:, None, :, None] * mask[:, None, None, :]\n",
    "\n",
    "            # mask attention\n",
    "            dots.masked_fill_(~mask, -mask_value)\n",
    "\n",
    "            if exists(distance_mat):\n",
    "                # mask distance to infinity\n",
    "                # todo - make sure for softmax distance kernel, use -infinity\n",
    "                dist_mask_value = dist_kernel_config[\"mask_value_fn\"](dots)\n",
    "                distance_mat.masked_fill_(~mask, dist_mask_value)\n",
    "\n",
    "            if exists(adjacency_mat):\n",
    "                adjacency_mat.masked_fill_(~mask, 0.0)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # sum contributions from adjacency and distance tensors\n",
    "        attn = attn * La\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            attn = attn + Lg * adjacency_mat\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = dist_kernel_config[\"fn\"](distance_mat)\n",
    "            attn = attn + Ld * distance_mat\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class MAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MATMaskedPool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = self.pool(x, mask=mask)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMAT(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MAT(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMATMasked(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MATMaskedPool(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderMATMasked()\n",
    "event = torch.randn(2, 100, 6)\n",
    "mask = torch.ones(2, 100).bool()\n",
    "adjecent_matrix = torch.empty(2, 100, 100).random_(2).float()\n",
    "distance_matrix = torch.randn(2, 100, 100)\n",
    "batch = dict(\n",
    "    event=event,\n",
    "    mask=mask,\n",
    "    adjecent_matrix=adjecent_matrix,\n",
    "    distance_matrix=distance_matrix,\n",
    ")\n",
    "out = model(batch)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu115'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
