{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "sys.path.append('/opt/slh/archive/software/graphnet/src')\n",
    "import torch\n",
    "from x_transformers import ContinuousTransformerWrapper, Encoder, Decoder\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_from_disk\n",
    "from abc import abstractmethod\n",
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "import scipy\n",
    "import numpy as np\n",
    "from graphnet.models.task.reconstruction import DirectionReconstructionWithKappa\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss\n",
    "from graphnet.training.labels import Direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class VonMisesFisher3DLossCosineSimularityLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.vonmis = VonMisesFisher3DLoss()\n",
    "        self.cosine = nn.CosineSimilarity(dim=1, eps=eps)\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        return (self.vonmis(y_pred, y_true) + (1-self.cosine(y_pred[:, :3], y_true).mean()))/2\n",
    "        \n",
    "    \n",
    "    \n",
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "class SigmoidRange(nn.Module):\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x) * (self.high - self.low) + self.low\n",
    "\n",
    "\n",
    "class Adjustoutput(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.az = SigmoidRange(6.436839548775502e-08, 6.2891)\n",
    "        self.zn = SigmoidRange(8.631674577710722e-05, 3.1417)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x[:, 0] = self.az(x[:, 0])\n",
    "        x[:, 1] = self.zn(x[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class PoolingWithMask(nn.Module):\n",
    "    def __init__(self, pool_type):\n",
    "        super().__init__()\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        if self.pool_type == \"mean\":\n",
    "            # Sum the values along the sequence dimension\n",
    "            x = torch.sum(x, dim=1)\n",
    "\n",
    "            # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "            x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "        elif self.pool_type == \"max\":\n",
    "            # Find the maximum value along the sequence dimension\n",
    "            x, _ = torch.max(x, dim=1)\n",
    "        elif self.pool_type == \"min\":\n",
    "            # Find the minimum value along the sequence dimension\n",
    "            x, _ = torch.min(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pool_type. Choose from ['mean', 'max', 'min']\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MeanPoolingWithMask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPoolingWithMask, self).__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multiply the mask with the input tensor to zero out the padded values\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "\n",
    "        # Sum the values along the sequence dimension\n",
    "        x = torch.sum(x, dim=1)\n",
    "\n",
    "        # Divide the sum by the number of non-padded values (i.e. the sum of the mask)\n",
    "        x = x / torch.sum(mask, dim=1, keepdim=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=6,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        # self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(128, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV1CombinePool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask('mean')\n",
    "        self.pool_max = PoolingWithMask('max')\n",
    "        self.head = FeedForward(128 * 2, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class always:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def l2norm(t, groups=1):\n",
    "    t = rearrange(t, \"... (g d) -> ... g d\", g=groups)\n",
    "    t = F.normalize(t, p=2, dim=-1)\n",
    "    return rearrange(t, \"... g d -> ... (g d)\")\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinng(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=14):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV1(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbedding(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        embed = self.post_norma(embed)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TokenEmbeddingV2(nn.Module):\n",
    "    def __init__(self, dim, num_tokens, l2norm_embed=False):\n",
    "        super().__init__()\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_emb = self.emb(x)\n",
    "        return l2norm(token_emb) if self.l2norm_embed else token_emb\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.kaiming_normal_(self.emb.weight)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV2(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=196,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderSensorEmbeddinngV3(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = TokenEmbeddingV2(dim, num_tokens=5161)\n",
    "        self.post_norma = nn.LayerNorm(dim)\n",
    "        self.token_emb.init_()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=in_features + dim,\n",
    "            dim_out=256,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=256, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.head = FeedForward(256, 2)\n",
    "        self.sigmout = Adjustoutput()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask, sensor_id = batch[\"event\"], batch[\"mask\"], batch[\"sensor_id\"]\n",
    "        embed = self.token_emb(sensor_id)\n",
    "        x = torch.cat([x, embed], dim=-1)\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        x = self.head(x)\n",
    "        s = self.sigmout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderV1CombinePool().eval()\n",
    "event = torch.rand(1, 100, 8)\n",
    "mask = torch.ones(1, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (1, 100))\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.out = DirectionReconstructionWithKappa(128)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = self.pool(x, mask)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstruction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=8,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class EncoderWithDirectionReconstructionV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.pool_min = PoolingWithMask(\"min\")\n",
    "        self.ae = FeedForward(384, 384)\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=384,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), \n",
    "                          self.pool_max(x, mask), \n",
    "                          self.pool_min(x, mask)], dim=1)\n",
    "        x = self.ae(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "\n",
    "class EncoderWithDirectionReconstructionV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = ContinuousTransformerWrapper(\n",
    "            dim_in=9,\n",
    "            dim_out=128,\n",
    "            max_seq_len=150,\n",
    "            attn_layers=Encoder(dim=128, depth=6, heads=8),\n",
    "        )\n",
    "\n",
    "        self.pool_mean = PoolingWithMask(\"mean\")\n",
    "        self.pool_max = PoolingWithMask(\"max\")\n",
    "        self.out = DirectionReconstructionWithKappa(\n",
    "            hidden_size=256,\n",
    "            target_labels='direction',\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, mask = batch[\"event\"], batch[\"mask\"]\n",
    "        x = self.encoder(x, mask=mask)\n",
    "        x = torch.concat([self.pool_mean(x, mask), self.pool_max(x, mask)], dim=1)\n",
    "        return self.out(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphnet.training.labels.Direction at 0x7f9312604650>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderWithDirectionReconstructionV1().eval()\n",
    "event = torch.rand(1, 100, 9)\n",
    "mask = torch.ones(1, 100, dtype=torch.bool)\n",
    "sensor_id = torch.randint(0, 5161, (1, 100))\n",
    "label = torch.rand(1, 3)\n",
    "input = dict(event=event, mask=mask, sensor_id=sensor_id, label=label)\n",
    "with torch.no_grad():\n",
    "    y = model(input)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8334)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CosineSimilarity(dim=1, eps=1e-6)(torch.rand(10, 4), torch.rand(10, 4)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8100)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VonMisesFisher3DLossCosineSimularityLoss()(y, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# MOLECULAR TRANFORMER\n",
    "DIST_KERNELS = {\n",
    "    \"exp\": {\n",
    "        \"fn\": lambda t: torch.exp(-t),\n",
    "        \"mask_value_fn\": lambda t: torch.finfo(t.dtype).max,\n",
    "    },\n",
    "    \"softmax\": {\n",
    "        \"fn\": lambda t: torch.softmax(t, dim=-1),\n",
    "        \"mask_value_fn\": lambda t: -torch.finfo(t.dtype).max,\n",
    "    },\n",
    "}\n",
    "\n",
    "# helpers\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return d if not exists(val) else val\n",
    "\n",
    "\n",
    "# helper classes\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x + self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForwardV1(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4):\n",
    "        super().__init__()\n",
    "        dim_out = default(dim_out, dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult), nn.GELU(), nn.Linear(dim * mult, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, heads=8, dim_head=64, Lg=0.5, Ld=0.5, La=1, dist_kernel_fn=\"exp\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        # hyperparameters controlling the weighted linear combination from\n",
    "        # self-attention (La)\n",
    "        # adjacency graph (Lg)\n",
    "        # pair-wise distance matrix (Ld)\n",
    "\n",
    "        self.La = La\n",
    "        self.Ld = Ld\n",
    "        self.Lg = Lg\n",
    "\n",
    "        self.dist_kernel_fn = dist_kernel_fn\n",
    "\n",
    "    def forward(self, x, mask=None, adjacency_mat=None, distance_mat=None):\n",
    "        h, La, Ld, Lg, dist_kernel_fn = (\n",
    "            self.heads,\n",
    "            self.La,\n",
    "            self.Ld,\n",
    "            self.Lg,\n",
    "            self.dist_kernel_fn,\n",
    "        )\n",
    "\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, \"b n (h qkv d) -> b h n qkv d\", h=h, qkv=3).unbind(\n",
    "            dim=-2\n",
    "        )\n",
    "        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        assert (\n",
    "            dist_kernel_fn in DIST_KERNELS\n",
    "        ), f\"distance kernel function needs to be one of {DIST_KERNELS.keys()}\"\n",
    "        dist_kernel_config = DIST_KERNELS[dist_kernel_fn]\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = rearrange(distance_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            adjacency_mat = rearrange(adjacency_mat, \"b i j -> b () i j\")\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_value = torch.finfo(dots.dtype).max\n",
    "            mask = mask[:, None, :, None] * mask[:, None, None, :]\n",
    "\n",
    "            # mask attention\n",
    "            dots.masked_fill_(~mask, -mask_value)\n",
    "\n",
    "            if exists(distance_mat):\n",
    "                # mask distance to infinity\n",
    "                # todo - make sure for softmax distance kernel, use -infinity\n",
    "                dist_mask_value = dist_kernel_config[\"mask_value_fn\"](dots)\n",
    "                distance_mat.masked_fill_(~mask, dist_mask_value)\n",
    "\n",
    "            if exists(adjacency_mat):\n",
    "                adjacency_mat.masked_fill_(~mask, 0.0)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        # sum contributions from adjacency and distance tensors\n",
    "        attn = attn * La\n",
    "\n",
    "        if exists(adjacency_mat):\n",
    "            attn = attn + Lg * adjacency_mat\n",
    "\n",
    "        if exists(distance_mat):\n",
    "            distance_mat = dist_kernel_config[\"fn\"](distance_mat)\n",
    "            attn = attn + Ld * distance_mat\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "\n",
    "class MAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = x.mean(dim=-2)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MATMaskedPool(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim_in,\n",
    "        model_dim,\n",
    "        dim_out,\n",
    "        depth,\n",
    "        heads=8,\n",
    "        Lg=0.5,\n",
    "        Ld=0.5,\n",
    "        La=1,\n",
    "        dist_kernel_fn=\"exp\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_to_model = nn.Linear(dim_in, model_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList(\n",
    "                [\n",
    "                    Residual(\n",
    "                        PreNorm(\n",
    "                            model_dim,\n",
    "                            Attention(\n",
    "                                model_dim,\n",
    "                                heads=heads,\n",
    "                                Lg=Lg,\n",
    "                                Ld=Ld,\n",
    "                                La=La,\n",
    "                                dist_kernel_fn=dist_kernel_fn,\n",
    "                            ),\n",
    "                        )\n",
    "                    ),\n",
    "                    Residual(PreNorm(model_dim, FeedForwardV1(model_dim))),\n",
    "                ]\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm_out = nn.LayerNorm(model_dim)\n",
    "        self.pool = MeanPoolingWithMask()\n",
    "        self.ff_out = FeedForward(model_dim, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x = batch[\"event\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        adjacency_mat = batch[\"adjecent_matrix\"]\n",
    "        distance_mat = batch[\"distance_matrix\"]\n",
    "\n",
    "        x = self.embed_to_model(x)\n",
    "\n",
    "        for (attn, ff) in self.layers:\n",
    "            x = attn(\n",
    "                x, mask=mask, adjacency_mat=adjacency_mat, distance_mat=distance_mat\n",
    "            )\n",
    "            x = ff(x)\n",
    "\n",
    "        x = self.norm_out(x)\n",
    "        x = self.pool(x, mask=mask)\n",
    "        x = self.ff_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMAT(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MAT(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n",
    "\n",
    "class IceCubeModelEncoderMATMasked(nn.Module):\n",
    "    def __init__(self, dim=128, in_features=6):\n",
    "        super().__init__()\n",
    "        self.md = MATMaskedPool(\n",
    "            dim_in=6,\n",
    "            model_dim=128,\n",
    "            dim_out=2,\n",
    "            depth=6,\n",
    "            Lg=0.5,  # lambda (g)raph - weight for adjacency matrix\n",
    "            Ld=0.5,  # lambda (d)istance - weight for distance matrix\n",
    "            La=1,  # lambda (a)ttention - weight for usual self-attention\n",
    "            dist_kernel_fn=\"exp\",  # distance kernel fn - either 'exp' or 'softmax'\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.md(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IceCubeModelEncoderMATMasked()\n",
    "event = torch.randn(2, 100, 6)\n",
    "mask = torch.ones(2, 100).bool()\n",
    "adjecent_matrix = torch.empty(2, 100, 100).random_(2).float()\n",
    "distance_matrix = torch.randn(2, 100, 100)\n",
    "batch = dict(\n",
    "    event=event,\n",
    "    mask=mask,\n",
    "    adjecent_matrix=adjecent_matrix,\n",
    "    distance_matrix=distance_matrix,\n",
    ")\n",
    "out = model(batch)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu115'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, ReLU, SiLU, Sequential\n",
    "from torch_geometric.nn import MessagePassing, global_add_pool, global_mean_pool\n",
    "from torch_scatter import scatter\n",
    "\n",
    "\n",
    "class EGNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"E(n) Equivariant GNN Layer\n",
    "        Paper: E(n) Equivariant Graph Neural Networks, Satorras et al.\n",
    "        \n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim + 1, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\psi_x` for computing messages `\\overrightarrow{m}_ij`\n",
    "        self.mlp_pos = Sequential(\n",
    "            Linear(emb_dim, emb_dim), self.norm(emb_dim), self.activation, Linear(emb_dim, 1)\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, pos, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            pos: (n, 3) - initial node coordinates\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: [(n, d),(n,3)] - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h, pos=pos)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j, pos_i, pos_j):\n",
    "        # Compute messages\n",
    "        pos_diff = pos_i - pos_j\n",
    "        dists = torch.norm(pos_diff, dim=-1).unsqueeze(1)\n",
    "        msg = torch.cat([h_i, h_j, dists], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        # Scale magnitude of displacement vector\n",
    "        pos_diff = pos_diff * self.mlp_pos(msg)  # torch.clamp(updates, min=-100, max=100)\n",
    "        return msg, pos_diff\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        msgs, pos_diffs = inputs\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(msgs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        # Aggregate displacement vectors\n",
    "        pos_aggr = scatter(pos_diffs, index, dim=self.node_dim, reduce=\"mean\")\n",
    "        return msg_aggr, pos_aggr\n",
    "\n",
    "    def update(self, aggr_out, h, pos):\n",
    "        msg_aggr, pos_aggr = aggr_out\n",
    "        upd_out = self.mlp_upd(torch.cat([h, msg_aggr], dim=-1))\n",
    "        upd_pos = pos + pos_aggr\n",
    "        return upd_out, upd_pos\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\"\n",
    "\n",
    "\n",
    "class MPNNLayer(MessagePassing):\n",
    "    def __init__(self, emb_dim, activation=\"relu\", norm=\"layer\", aggr=\"add\"):\n",
    "        \"\"\"Vanilla Message Passing GNN layer\n",
    "        \n",
    "        Args:\n",
    "            emb_dim: (int) - hidden dimension `d`\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "        \"\"\"\n",
    "        # Set the aggregation function\n",
    "        super().__init__(aggr=aggr)\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.activation = {\"swish\": SiLU(), \"relu\": ReLU()}[activation]\n",
    "        self.norm = {\"layer\": torch.nn.LayerNorm, \"batch\": torch.nn.BatchNorm1d}[norm]\n",
    "\n",
    "        # MLP `\\psi_h` for computing messages `m_ij`\n",
    "        self.mlp_msg = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
    "        self.mlp_upd = Sequential(\n",
    "            Linear(2 * emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "            Linear(emb_dim, emb_dim),\n",
    "            self.norm(emb_dim),\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, h, edge_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (n, d) - initial node features\n",
    "            edge_index: (e, 2) - pairs of edges (i, j)\n",
    "        Returns:\n",
    "            out: (n, d) - updated node features\n",
    "        \"\"\"\n",
    "        out = self.propagate(edge_index, h=h)\n",
    "        return out\n",
    "\n",
    "    def message(self, h_i, h_j):\n",
    "        # Compute messages\n",
    "        msg = torch.cat([h_i, h_j], dim=-1)\n",
    "        msg = self.mlp_msg(msg)\n",
    "        return msg\n",
    "\n",
    "    def aggregate(self, inputs, index):\n",
    "        # Aggregate messages\n",
    "        msg_aggr = scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
    "        return msg_aggr\n",
    "\n",
    "    def update(self, aggr_out, h):\n",
    "        upd_out = self.mlp_upd(torch.cat([h, aggr_out], dim=-1))\n",
    "        return upd_out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNNModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=5,\n",
    "        emb_dim=128,\n",
    "        in_dim=1,\n",
    "        out_dim=1,\n",
    "        activation=\"relu\",\n",
    "        norm=\"layer\",\n",
    "        aggr=\"sum\",\n",
    "        pool=\"sum\",\n",
    "        residual=True\n",
    "    ):\n",
    "        \"\"\"E(n) Equivariant GNN model \n",
    "        \n",
    "        Args:\n",
    "            num_layers: (int) - number of message passing layers\n",
    "            emb_dim: (int) - hidden dimension\n",
    "            in_dim: (int) - initial node feature dimension\n",
    "            out_dim: (int) - output number of classes\n",
    "            activation: (str) - non-linearity within MLPs (swish/relu)\n",
    "            norm: (str) - normalisation layer (layer/batch)\n",
    "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
    "            pool: (str) - global pooling function (sum/mean)\n",
    "            residual: (bool) - whether to use residual connections\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding lookup for initial node features\n",
    "        self.emb_in = torch.nn.Embedding(in_dim, emb_dim)\n",
    "\n",
    "        # Stack of GNN layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(EGNNLayer(emb_dim, activation, norm, aggr))\n",
    "\n",
    "        # Global pooling/readout function\n",
    "        self.pool = {\"mean\": global_mean_pool, \"sum\": global_add_pool}[pool]\n",
    "\n",
    "        # Predictor MLP\n",
    "        self.pred = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_dim, emb_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_dim, out_dim)\n",
    "        )\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \n",
    "        h = self.emb_in(batch.atoms)  # (n,) -> (n, d)\n",
    "        pos = batch.pos  # (n, 3)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            # Message passing layer\n",
    "            h_update, pos_update = conv(h, pos, batch.edge_index)\n",
    "\n",
    "            # Update node features (n, d) -> (n, d)\n",
    "            h = h + h_update if self.residual else h_update \n",
    "\n",
    "            # Update node coordinates (no residual) (n, 3) -> (n, 3)\n",
    "            pos = pos_update\n",
    "\n",
    "        out = self.pool(h, batch.batch)  # (n, d) -> (batch_size, d)\n",
    "        return self.pred(out)  # (batch_size, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 128])\n"
     ]
    }
   ],
   "source": [
    "#feats = torch.randint(0, 21, (1, 200)) # (1, 1024)\n",
    "feats = torch.rand(1, 200, 8)         # (1, 1024, 3)\n",
    "coors = torch.randn(1, 200, 4)         # (1, 1024, 3)\n",
    "mask = torch.ones_like(torch.rand(1, 200)).bool()    # (1, 1024)\n",
    "\n",
    "feats_out, coors_out = net(feats, coors, mask = mask) # (1, 1024, 32), (1, 1024, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 128])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.token_emb(feats).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6397,  0.0150, -0.5736,  ...,  0.4262,  0.0347, -0.2446],\n",
       "         [-0.6280,  1.0807,  1.9626,  ..., -1.2121,  0.1134,  1.1187],\n",
       "         [ 0.3124,  0.5278,  2.3593,  ...,  0.5552,  0.8637,  0.1939],\n",
       "         ...,\n",
       "         [-1.0462,  0.7790,  0.9071,  ...,  2.0370, -1.1702, -0.5642],\n",
       "         [-0.0345,  0.5069, -0.1040,  ...,  1.0613, -0.1551, -0.6776],\n",
       "         [-0.5054, -0.1815,  1.8672,  ..., -0.1932,  0.8704, -0.0397]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
