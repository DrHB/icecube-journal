{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/opt/slh/icecube/')\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efa89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "from icecube.models import EncoderWithDirectionReconstructionV8\n",
    "from icecube.modelsgraph import DynEdgeV3\n",
    "from fastai_fix import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = 'dynEDGE'\n",
    "PATH = '../data/'\n",
    "\n",
    "NUM_WORKERS = 20\n",
    "SEED = 2023\n",
    "bs = 1024\n",
    "L = 196\n",
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "os.makedirs(OUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "from typing import Iterator, Iterable, Optional, Sequence, List, TypeVar, Generic, Sized, Union\n",
    "\n",
    "class RandomChunkSampler(torch.utils.data.Sampler[int]):\n",
    "    r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
    "    If with replacement, then user can specify :attr:`num_samples` to draw.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\n",
    "        num_samples (int): number of samples to draw, default=`len(dataset)`.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "    data_source: Sized\n",
    "    replacement: bool\n",
    "\n",
    "    def __init__(self, data_source: Sized, num_samples: Optional[int] = None,\n",
    "                 generator=None, chunk_size=200000, **kwargs) -> None:\n",
    "        self.data_source = data_source\n",
    "        self._num_samples = num_samples\n",
    "        self.generator = generator\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
    "            raise ValueError(\"num_samples should be a positive integer \"\n",
    "                             \"value, but got num_samples={}\".format(self.num_samples))\n",
    "\n",
    "    @property\n",
    "    def num_samples(self) -> int:\n",
    "        # dataset size might change at runtime\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        n = len(self.data_source)\n",
    "        if self.generator is None:\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(seed)\n",
    "        else:\n",
    "            generator = self.generator\n",
    "\n",
    "        chunk_list = torch.randperm(self.num_samples // self.chunk_size, generator=generator).tolist()\n",
    "        for i in range(self.num_samples // self.chunk_size):\n",
    "            chunk = chunk_list[i]\n",
    "            yield from (chunk*self.chunk_size + torch.randperm(self.chunk_size, generator=generator)).tolist()\n",
    "        #yield from ((self.num_samples // self.chunk_size)*self.chunk_size + \n",
    "        #    torch.randperm(self.num_samples%self.chunk_size, generator=generator)).tolist()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "    \n",
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            s = self.sampler.data_source[idx]\n",
    "            if isinstance(s,tuple): L = s[0][\"mask\"].sum()\n",
    "            else: L = s[\"mask\"].sum()\n",
    "            #if torch.rand(1).item() < 0.1: L = int(1.5*L)\n",
    "            L = L // 16 \n",
    "            if len(buckets[L]) == 0:  buckets[L] = []\n",
    "            buckets[L].append(idx)\n",
    "            \n",
    "            if len(buckets[L]) == self.batch_size:\n",
    "                batch = list(buckets[L])\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[L] = []\n",
    "                \n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "\n",
    "        #assert len(self) == yielded,\\\n",
    "        #  \"produced an inccorect number of batches. expected %i, but yielded %i\" %(len(self), yielded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os,gc\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def prepare_sensors(path=PATH):\n",
    "    sensors = pd.read_csv(os.path.join(path,'sensor_geometry.csv')).astype(\n",
    "        {\n",
    "            \"sensor_id\": np.int16,\n",
    "            \"x\": np.float32,\n",
    "            \"y\": np.float32,\n",
    "            \"z\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    sensors[\"string\"] = 0\n",
    "    sensors[\"qe\"] = 0#1\n",
    "\n",
    "    for i in range(len(sensors) // 60):\n",
    "        start, end = i * 60, (i * 60) + 60\n",
    "        sensors.loc[start:end, \"string\"] = i\n",
    "\n",
    "        # High Quantum Efficiency in the lower 50 DOMs - https://arxiv.org/pdf/2209.03042.pdf (Figure 1)\n",
    "        if i in range(78, 86):\n",
    "            start_veto, end_veto = i * 60, (i * 60) + 10\n",
    "            start_core, end_core = end_veto + 1, (i * 60) + 60\n",
    "            sensors.loc[start_core:end_core, \"qe\"] = 1# 1.35\n",
    "\n",
    "    # https://github.com/graphnet-team/graphnet/blob/b2bad25528652587ab0cdb7cf2335ee254cfa2db/src/graphnet/models/detector/icecube.py#L33-L41\n",
    "    # Assume that \"rde\" (relative dom efficiency) is equivalent to QE\n",
    "    sensors[\"x\"] /= 500\n",
    "    sensors[\"y\"] /= 500\n",
    "    sensors[\"z\"] /= 500\n",
    "    #sensors[\"qe\"] -= 1.25\n",
    "    #sensors[\"qe\"] /= 0.25\n",
    "\n",
    "    return sensors\n",
    "\n",
    "def ice_transparency(path=PATH, datum=1950):\n",
    "    # Data from page 31 of https://arxiv.org/pdf/1301.5361.pdf\n",
    "    # Datum is from footnote 8 of page 29\n",
    "    df = pd.read_csv(os.path.join(path,'ice_transparency.txt'), delim_whitespace=True)\n",
    "    df[\"z\"] = df[\"depth\"] - datum\n",
    "    df[\"z_norm\"] = df[\"z\"] / 500\n",
    "    df[[\"scattering_len_norm\", \"absorption_len_norm\"]] = RobustScaler().fit_transform(\n",
    "        df[[\"scattering_len\", \"absorption_len\"]])\n",
    "\n",
    "    # These are both roughly equivalent after scaling\n",
    "    f_scattering = interp1d(df[\"z_norm\"], df[\"scattering_len_norm\"])\n",
    "    f_absorption = interp1d(df[\"z_norm\"], df[\"absorption_len_norm\"])\n",
    "    return f_scattering, f_absorption\n",
    "\n",
    "class IceCubeDataset(Dataset):\n",
    "    def __init__(self, path=PATH, chunk_size=200000, L=256, buf_size=4, train=True, reduce_size=-1):\n",
    "        #path_geometry=PATH_GEOMETRY, /sensor_geometry.csv\n",
    "        self.path = os.path.join(path,'train')\n",
    "        self.files = [p for p in sorted(os.listdir(self.path)) if p!='batch_660.parquet'] #660 is shorter\n",
    "        val_fnames = ['batch_655.parquet','batch_656.parquet','batch_657.parquet','batch_658.parquet',\n",
    "                      'batch_659.parquet']\n",
    "        if not train: self.files = val_fnames\n",
    "        else: self.files = sorted(set(self.files) - set(val_fnames))\n",
    "        self.chunk_size = chunk_size\n",
    "        self.buf = OrderedDict()\n",
    "        self.L,self.buf_size = L,buf_size\n",
    "        sensors = prepare_sensors(path)\n",
    "        self.geometry = torch.from_numpy(sensors[['x','y','z']].values.astype(np.float32))\n",
    "        self.qe = sensors['qe'].values\n",
    "        self.ice_properties = ice_transparency(path)\n",
    "        \n",
    "        df = pd.read_parquet(os.path.join(path,'train_meta.parquet'))\n",
    "        df = df[['event_id','azimuth','zenith']]\n",
    "        df['azimuth'] = df['azimuth'].astype(np.float32)\n",
    "        df['zenith'] = df['zenith'].astype(np.float32)\n",
    "        df['event_id'] = df['event_id'].astype(np.int32)\n",
    "        df = df.set_index('event_id',drop=True)\n",
    "        self.target = df\n",
    "        gc.collect()\n",
    "        self.reduce_size = reduce_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)*self.chunk_size if self.reduce_size < 0 \\\n",
    "                else int(self.reduce_size*len(self.files))*self.chunk_size\n",
    "        \n",
    "    def __getitem__(self, idx0):\n",
    "        fname = self.files[idx0//self.chunk_size]\n",
    "        if fname not in self.buf:\n",
    "            df = pl.read_parquet(os.path.join(self.path,fname))\n",
    "            df = df.groupby(\"event_id\").agg([\n",
    "                pl.count(),\n",
    "                pl.col(\"sensor_id\").list(),\n",
    "                pl.col(\"time\").list(),\n",
    "                pl.col(\"charge\").list(),\n",
    "                pl.col(\"auxiliary\").list(),])\n",
    "            self.buf[fname] = df.sort('event_id')\n",
    "            if len(self.buf) > self.buf_size: del self.buf[list(self.buf.keys())[0]]\n",
    "        \n",
    "        idx = idx0%self.chunk_size\n",
    "        df = self.buf[fname]\n",
    "        sensor_id =  df[idx]['sensor_id'][0].item().to_numpy()\n",
    "        time =  df[idx]['time'][0].item().to_numpy()\n",
    "        charge = df[idx]['charge'][0].item().to_numpy()\n",
    "        auxiliary = df[idx]['auxiliary'][0].item().to_numpy()\n",
    "        event_idx = df[idx]['event_id'].item()\n",
    "            \n",
    "        #sensor_id = sensor_id[~auxiliary]\n",
    "        #time = time[~auxiliary]\n",
    "        #charge = charge[~auxiliary]\n",
    "        \n",
    "        time = (time - 1e4)/3e4\n",
    "        charge = np.log10(charge)/3.0 #np.log(charge)\n",
    "        \n",
    "        L = len(sensor_id)\n",
    "        if L < self.L:\n",
    "            sensor_id = np.pad(sensor_id,(0,max(0,self.L-L)))\n",
    "            time = np.pad(time,(0,max(0,self.L-L)))\n",
    "            charge = np.pad(charge,(0,max(0,self.L-L)))\n",
    "            auxiliary = np.pad(auxiliary,(0,max(0,self.L-L)))\n",
    "        else:\n",
    "            ids = torch.randperm(L).numpy()\n",
    "            auxiliary_n = np.where(~auxiliary)[0]\n",
    "            auxiliary_p = np.where(auxiliary)[0]\n",
    "            ids_n = ids[auxiliary_n][:min(self.L,len(auxiliary_n))]\n",
    "            ids_p = ids[auxiliary_p][:min(self.L-len(ids_n),len(auxiliary_p))]\n",
    "            ids = np.concatenate([ids_n,ids_p])\n",
    "            ids.sort()\n",
    "            L = len(ids)\n",
    "            \n",
    "            sensor_id = sensor_id[ids]\n",
    "            time = time[ids]\n",
    "            charge = charge[ids]\n",
    "            auxiliary = auxiliary[ids]\n",
    "            L = len(ids)\n",
    "            \n",
    "        attn_mask = torch.zeros(self.L, dtype=torch.bool)\n",
    "        attn_mask[:L] = True\n",
    "        sensor_id = torch.from_numpy(sensor_id).long()\n",
    "        pos = self.geometry[sensor_id]\n",
    "        pos[L:] = 0\n",
    "        qe = self.qe[sensor_id]\n",
    "        qe[L:] = 0\n",
    "        ice_properties = np.stack([self.ice_properties[0](pos[:L,2]),\n",
    "                                   self.ice_properties[1](pos[:L,2])],-1)\n",
    "        ice_properties = np.pad(ice_properties,((0,max(0,self.L-L)),(0,0)))\n",
    "        ice_properties = torch.from_numpy(ice_properties).float()\n",
    "        \n",
    "        target = self.target.loc[event_idx].values\n",
    "\n",
    "        return {'sensor_id': sensor_id, 'time': torch.from_numpy(time).float(),\n",
    "                'charge': torch.from_numpy(charge).float(), 'pos':pos, 'mask':attn_mask,\n",
    "                'idx':event_idx, 'auxiliary':torch.from_numpy(auxiliary).long(),\n",
    "                'qe':qe, 'ice_properties':ice_properties},\\\n",
    "               {'target': torch.from_numpy(target).float()}\n",
    "    \n",
    "    \n",
    "class IceCubeDataset_len(Dataset):\n",
    "    def __init__(self, path=PATH, chunk_size=200000, L=256, buf_size=2, train=True, reduce_size=-1):\n",
    "        #path_geometry=PATH_GEOMETRY, /sensor_geometry.csv\n",
    "        self.path = os.path.join(path,'train')\n",
    "        self.files = [p for p in sorted(os.listdir(self.path)) if p!='batch_660.parquet'] #660 is shorter\n",
    "        val_fnames = ['batch_655.parquet','batch_656.parquet','batch_657.parquet','batch_658.parquet',\n",
    "                      'batch_659.parquet']\n",
    "        if not train: self.files = val_fnames\n",
    "        else: self.files = sorted(set(self.files) - set(val_fnames))\n",
    "        self.chunk_size = chunk_size\n",
    "        self.buf = OrderedDict()\n",
    "        self.L,self.buf_size = L,buf_size\n",
    "        sensors = prepare_sensors(path)\n",
    "        self.geometry = torch.from_numpy(sensors[['x','y','z']].values.astype(np.float32))\n",
    "        self.qe = sensors['qe'].values\n",
    "        self.ice_properties = ice_transparency(path)\n",
    "        \n",
    "        gc.collect()\n",
    "        self.reduce_size = reduce_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)*self.chunk_size if self.reduce_size < 0 \\\n",
    "                else int(self.reduce_size*len(self.files))*self.chunk_size\n",
    "        \n",
    "    def __getitem__(self, idx0):\n",
    "        fname = self.files[idx0//self.chunk_size]\n",
    "        if fname not in self.buf:\n",
    "            df = pl.read_parquet(os.path.join(self.path,fname))\n",
    "            df = df.groupby(\"event_id\").agg([\n",
    "                pl.count(),\n",
    "                pl.col(\"sensor_id\").list(),\n",
    "                pl.col(\"time\").list(),\n",
    "                pl.col(\"charge\").list(),\n",
    "                pl.col(\"auxiliary\").list(),])\n",
    "            self.buf[fname] = df.sort('event_id')\n",
    "            if len(self.buf) > self.buf_size: del self.buf[list(self.buf.keys())[0]]\n",
    "        \n",
    "        idx = idx0%self.chunk_size\n",
    "        df = self.buf[fname]\n",
    "        sensor_id =  df[idx]['sensor_id'][0].item().to_numpy()\n",
    "        mask = torch.ones(min(len(sensor_id),self.L), dtype=torch.long)\n",
    "        return {'mask':mask},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a08fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from loss_functions import VonMisesFisher3DLoss\n",
    "\n",
    "def loss(pred,y):\n",
    "    #print(pred.max())\n",
    "    pred = F.normalize(pred.float(),dim=-1)\n",
    "    \n",
    "    sa2 = torch.sin(y['target'][:,0])\n",
    "    ca2 = torch.cos(y['target'][:,0])\n",
    "    sz2 = torch.sin(y['target'][:,1])\n",
    "    cz2 = torch.cos(y['target'][:,1])\n",
    "    \n",
    "    scalar_prod = (pred[:,0]*sa2*sz2 + pred[:,1]*ca2*sz2 + pred[:,2]*cz2).clip(-1+1e-8,1-1e-8)\n",
    "    return torch.acos(scalar_prod).abs().mean(-1)   \n",
    "\n",
    "def loss_vms(pred,y):\n",
    "    sa2 = torch.sin(y['target'][:,0])\n",
    "    ca2 = torch.cos(y['target'][:,0])\n",
    "    sz2 = torch.sin(y['target'][:,1])\n",
    "    cz2 = torch.cos(y['target'][:,1])\n",
    "    t = torch.stack([sa2*sz2,ca2*sz2,cz2],-1)\n",
    "    \n",
    "    p = pred.float()\n",
    "    l = torch.norm(pred.float(),dim=-1).unsqueeze(-1)\n",
    "    p = torch.cat([pred.float()/l,l],-1)\n",
    "    \n",
    "    loss = VonMisesFisher3DLoss()(p,t)\n",
    "    return loss\n",
    "\n",
    "def get_val(pred):\n",
    "    pred = F.normalize(pred,dim=-1)\n",
    "    zen = torch.acos(pred[:,2].clip(-1,1))\n",
    "    f = F.normalize(pred[:,:2],dim=-1)\n",
    "    az = torch.asin(f[:,0].clip(-1,1))\n",
    "    az = torch.where(f[:,1] > 0, az, math.pi - az)\n",
    "    az = torch.where(az > 0, az, az + 2.0*math.pi)\n",
    "    return torch.stack([az,zen],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a400ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to(x, device='cuda'):\n",
    "    return {k:x[k].to(device) for k in x}\n",
    "\n",
    "def to_device(x, device='cuda'):\n",
    "    return tuple(dict_to(e,device) for e in x)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    def __init__(self, dataloader, device='cuda'):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(dict_to(x, self.device) for x in batch)\n",
    "            \n",
    "def WrapperAdamW(param_groups,**kwargs):\n",
    "    return OptimWrapper(param_groups,torch.optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f40b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'DynEdgeV3'\n",
    "\n",
    "ds_train = IceCubeDataset(train=True, reduce_size=0.125, L=L)\n",
    "ds_train_len = IceCubeDataset_len(train=True, reduce_size=0.125, L=L)\n",
    "len_sampler_train = LenMatchBatchSampler(RandomChunkSampler(ds_train_len),batch_size=bs, drop_last=True)\n",
    "dl_train = DeviceDataLoader(DataLoader(ds_train, batch_sampler=len_sampler_train, num_workers=4, \n",
    "                        persistent_workers=True))\n",
    "#dl_train = DeviceDataLoader(DataLoader(ds_train, batch_size=bs, sampler=RandomChunkSampler(ds_train),\n",
    "#                            num_workers=8, persistent_workers=True, drop_last=True))\n",
    "#there is a bug in process pool creation, so persistent_workers and num_workers=0 are important\n",
    "ds_val = IceCubeDataset(train=False, L=L)\n",
    "ds_val_len = IceCubeDataset_len(train=False, L=L)\n",
    "len_sampler_val = LenMatchBatchSampler(RandomChunkSampler(ds_val_len),batch_size=bs, drop_last=False)\n",
    "dl_val = DeviceDataLoader(DataLoader(ds_val, batch_sampler=len_sampler_val, num_workers=0))\n",
    "#dl_val= DeviceDataLoader(DataLoader(ds_val, batch_size=bs, sampler=RandomChunkSampler(ds_val),\n",
    "#                            num_workers=0, drop_last=False))\n",
    "\n",
    "data = DataLoaders(dl_train,dl_val)\n",
    "model = DynEdgeV3()\n",
    "model = model.cuda()\n",
    "learn = Learner(data, model, path = OUT, loss_func=loss_vms,cbs=[GradientClip(3.0),\n",
    "            SaveModelCallback(monitor='loss',comp=np.less,every_epoch=True),\n",
    "            GradientAccumulation(n_acc=4)],\n",
    "            metrics=[loss], opt_func=partial(WrapperAdamW,eps=1e-7)).to_fp16()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff8720-072f-45d0-933f-f4ce9fdc36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.model.load_state_dict(torch.load('/opt/slh/icecube/Iafoss/init/init5small_0.pth'))\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ace60-f05d-46df-84f8-b9a41aca6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.load('opt/slh/icecube/Iafoss/models/model.pth')\n",
    "learn.fit_one_cycle(16, lr_max=1e-3, wd=0.05, pct_start=0.01)\n",
    "#torch.save(learn.model.state_dict(),os.path.join(OUT,f'{fname}_0.pth'))\n",
    "#torch.save(learn.model.module.state_dict(),os.path.join(OUT,f'{fname}_0.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c12eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b15cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f78eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
