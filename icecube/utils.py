# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_utils.ipynb.

# %% auto 0
__all__ = ['get_size', 'reduce_mem_usage', 'save_folder', 'SaveModel', 'SaveModelMetric', 'SaveModelEpoch', 'fit',
           'compare_events', 'good_luck']

# %% ../nbs/00_utils.ipynb 1
import numpy as np
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import os
from joblib import Parallel, delayed
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from fastprogress.fastprogress import master_bar, progress_bar

# %% ../nbs/00_utils.ipynb 2
def get_size(df):
    return round(df.memory_usage(deep=True).sum() / 1024 ** 3, 2)


def reduce_mem_usage(df):
    start_mem = get_size(df)
    print(f"Memory usage of dataframe is {start_mem} GB")
    for col in df.columns:
        col_type = df[col].dtype

        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == "int":
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif (
                    c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max
                ):
                    df[col] = df[col].astype(np.int16)
                elif (
                    c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max
                ):
                    df[col] = df[col].astype(np.int32)
                elif (
                    c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max
                ):
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif (
                    c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max
                ):
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype("category")

    end_mem = get_size(df)
    print(f"Memory usage after optimization is: {end_mem} GB")
    print(f"Decreased by {100 * (start_mem - end_mem) / start_mem}%")
    return df




# %% ../nbs/00_utils.ipynb 4
def save_folder(BATCH: int, CFG, n_jobs: int = 48):
    """
    Save the events in a folder


    """
    print("...")
    print("Loading data...")
    train_meta = pd.read_parquet(CFG.PATH_META).query("batch_id == @BATCH")
    geometry = pd.read_csv(CFG.PATH_GEOMETRY)
    train = pd.read_parquet(CFG.PATH_DATASET / "train" / f"batch_{BATCH}.parquet")

    # current save path
    CURRENT_SAVE_PATH = CFG.SAVE_PATH / f"batch_{BATCH}"
    os.makedirs(CURRENT_SAVE_PATH, exist_ok=True)

    # function to get the event from the dataframe
    def get_event(event_id: int):
        one_event = train_meta.query("event_id == @event_id")
        event = train.loc[event_id]
        event = event.merge(geometry, on="sensor_id")
        target = one_event[["azimuth", "zenith"]]
        return {"event": event.to_records(), "target": target.to_records()}

    # function that saves event as .pth file
    def save_event(event_id: int, save_path: Path):
        event = get_event(event_id)
        torch.save(event, save_path / f"{event_id}.pth")

    # function that save in parallel all the using joblib
    def save_all_events(event_ids: int, save_path: Path, n_jobs: int = 8):
        Parallel(n_jobs=n_jobs)(
            delayed(save_event)(event_id, save_path) for event_id in tqdm(event_ids)
        )

    print("Saving data for batch", BATCH)
    save_all_events(train_meta.event_id.unique(), CURRENT_SAVE_PATH, n_jobs=n_jobs)


# %% ../nbs/00_utils.ipynb 5
class SaveModel:
    def __init__(self, folder, exp_name, best=np.inf):
        self.best = best
        self.folder = Path(folder) / f"{exp_name}.pth"

    def __call__(self, score, model, epoch):
        if score < self.best:
            self.best = score
            print(f"Better model found at epoch {epoch} with value: {self.best}.")
            torch.save(model.state_dict(), self.folder)


class SaveModelMetric:
    def __init__(self, folder, exp_name, best=-np.inf):
        self.best = best
        self.folder = Path(folder) / f"{exp_name}.pth"

    def __call__(self, score, model, epoch):
        if score > self.best:
            self.best = score
            print(f"Better model found at epoch {epoch} with value: {self.best}.")
            torch.save(model.state_dict(), self.folder)


class SaveModelEpoch:
    def __init__(self, folder, exp_name, best=-np.inf):
        self.best = best
        self.folder = Path(folder)
        self.exp_name = exp_name

    def __call__(self, score, model, epoch):
        self.best = score
        print(f"Better model found at epoch {epoch} with value: {self.best}.")
        torch.save(model.state_dict(), f"{self.folder/self.exp_name}_{epoch}.pth")


def fit(
    epochs,
    model,
    train_dl,
    valid_dl,
    loss_fn,
    opt,
    metric,
    folder="models",
    exp_name="exp_00",
    device=None,
    sched=None,
    save_md=SaveModelEpoch,
):
    if device is None:
        device = (
            torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        )

    os.makedirs(folder, exist_ok=True)

    mb = master_bar(range(epochs))
    mb.write(["epoch", "train_loss", "valid_loss", "val_metric"], table=True)
    model.to(device)  # we have to put our model on gpu
    scaler = torch.cuda.amp.GradScaler()  # this for half precision training
    save_md = save_md(folder, exp_name)

    for i in mb:  # iterating  epoch
        trn_loss, val_loss = 0.0, 0.0
        trn_n, val_n = len(train_dl.dataset), len(valid_dl.dataset)
        model.train()  # set model for training
        for batch in progress_bar(train_dl, parent=mb):
            # putting batches to device
            xb, mask, yb = batch["event"], batch["mask"], batch["label"]
            xb, mask, yb = xb.to(device), mask.to(device), yb.to(device)
            with torch.cuda.amp.autocast():  # half precision
                out = model(xb, mask=mask)  # forward pass
                loss = loss_fn(out, yb)  # calulation loss

            trn_loss += loss.item()

            scaler.scale(loss).backward()  # backward
            scaler.step(opt)  # optimzers step
            scaler.update()  # for half precision
            opt.zero_grad()  # zeroing optimizer
            if sched is not None:
                sched.step()  # scuedular step

        trn_loss /= mb.child.total

        # putting model in eval mode
        model.eval()
        gt = []
        pred = []
        # after epooch is done we can run a validation dataloder and see how are doing
        with torch.no_grad():
            for batch in progress_bar(valid_dl, parent=mb):
                xb, mask, yb = batch
                xb, mask, yb = batch["event"], batch["mask"], batch["label"]
                xb, mask, yb = xb.to(device), mask.to(device), yb.to(device)
                with torch.cuda.amp.autocast():  # half precision
                    out = model(xb, mask=mask)  # forward pass
                    loss = loss_fn(out, yb)  # calulation loss
                val_loss += loss.item()

                gt.append(yb.detach())
                pred.append(out.detach())
        # calculating metric
        metric_ = metric(torch.cat(pred), torch.cat(gt))
        # saving model if necessary
        save_md(metric_, model, i)
        val_loss /= mb.child.total

        mb.write(
            [
                i,
                f"{trn_loss:.6f}",
                f"{val_loss:.6f}",
                f"{metric_:.6f}",
            ],
            table=True,
        )

        pd.DataFrame(
            {
                "epoch": [i],
                "train_loss": [trn_loss],
                "valid_loss": [val_loss],
                "metric": [metric_],
            }
        ).to_csv(f"{Path(folder)/exp_name}_{i}.csv", index=False)
    print("Training done")


# %% ../nbs/00_utils.ipynb 7
#function that compares events from parquet and pth files
def compare_events(event_id: int, CFG, BATCH: int):
    train_meta = pd.read_parquet(CFG.PATH_META)
    geometry = pd.read_csv(CFG.PATH_GEOMETRY)
    train = pd.read_parquet(CFG.PATH_DATASET/'train'/f'batch_{BATCH}.parquet')
    one_event = train_meta.query("event_id == @event_id")
    event = train.loc[event_id]
    event = event.merge(geometry, on="sensor_id")
    event_pth = pd.DataFrame.from_records(torch.load(CFG.SAVE_PATH/f'batch_{BATCH}' / f"{event_id}.pth")['event']).iloc[:, 1:]
    return np.all(event == event_pth)

# %% ../nbs/00_utils.ipynb 12
def good_luck():
    return True
